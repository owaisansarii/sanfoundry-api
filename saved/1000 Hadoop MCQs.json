[
    {
        "id": 1,
        "Question": "IBM and ________ have announced a major initiative to use Hadoop to support university courses in distributed computer programming.",
        "Options": [
            "a) Google Latitude",
            "b) Android (operating system)",
            "c) Google Variations",
            "d) Google"
        ],
        "Answer": "Answer: d\nExplanation: Google and IBM Announce University Initiative to Address Internet-Scale."
    },
    {
        "id": 2,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hadoop is an ideal environment for extracting and transforming small volumes of data",
            "b) Hadoop stores data in HDFS and supports data compression/decompression",
            "c) The Giraph framework is less  useful than a MapReduce job to solve graph and machine learning",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Data compression can be achieved using compression algorithms like bzip2, gzip, LZO, etc. Different algorithms can be used in different scenarios based on their capabilities."
    },
    {
        "id": 3,
        "Question": "What license is Hadoop distributed under?",
        "Options": [
            "a) Apache License 2.0",
            "b) Mozilla Public License",
            "c) Shareware",
            "d) Commercial"
        ],
        "Answer": "Answer: a\nExplanation: Hadoop is Open Source, released under Apache 2 license."
    },
    {
        "id": 4,
        "Question": "Sun also has the Hadoop Live CD ________ project, which allows running a fully functional Hadoop cluster using a live CD.",
        "Options": [
            "a) OpenOffice.org",
            "b) OpenSolaris",
            "c) GNU",
            "d) Linux"
        ],
        "Answer": "Answer: b\nExplanation: The OpenSolaris Hadoop LiveCD project built a bootable CD-ROM image. "
    },
    {
        "id": 5,
        "Question": "Which of the following genres does Hadoop produce?",
        "Options": [
            "a) Distributed file system",
            "b) JAX-RS",
            "c) Java Message Service",
            "d) Relational Database Management System"
        ],
        "Answer": "Answer: a\nExplanation: The Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to the user."
    },
    {
        "id": 6,
        "Question": "What was Hadoop written in?",
        "Options": [
            "a) Java (software platform)",
            "b) Perl",
            "c) Java (programming language)",
            "d) Lua (programming language)"
        ],
        "Answer": "Answer: c\nExplanation: The Hadoop framework itself is mostly written in the Java programming language, with some native code in C and command-line utilities written as shell scripts."
    },
    {
        "id": 7,
        "Question": "Which of the following platforms does Hadoop run on?",
        "Options": [
            "a) Bare metal",
            "b) Debian",
            "c) Cross-platform",
            "d) Unix-like"
        ],
        "Answer": "Answer: c\nExplanation: Hadoop has support for cross-platform operating system."
    },
    {
        "id": 8,
        "Question": "Hadoop achieves reliability by replicating the data across multiple hosts and hence does not require ________ storage on hosts.",
        "Options": [
            "a) RAID",
            "b) Standard RAID levels",
            "c) ZFS",
            "d) Operating system"
        ],
        "Answer": "Answer: a\nExplanation: With the default replication value, 3, data is stored on three nodes: two on the same rack, and one on a different rack."
    },
    {
        "id": 9,
        "Question": "Above the file systems comes the ________ engine, which consists of one Job Tracker, to which client applications submit MapReduce jobs.",
        "Options": [
            "a) MapReduce",
            "b) Google",
            "c) Functional programming",
            "d) Facebook"
        ],
        "Answer": "Answer: a\nExplanation: MapReduce engine uses to distribute work around a cluster."
    },
    {
        "id": 10,
        "Question": "The Hadoop list includes the HBase database, the Apache Mahout ________ system, and matrix operations.",
        "Options": [
            "a) Machine learning",
            "b) Pattern recognition",
            "c) Statistical classification",
            "d) Artificial intelligence"
        ],
        "Answer": "Answer: a\nExplanation: The Apache Mahout project’s goal is to build a scalable machine learning tool."
    },
    {
        "id": 11,
        "Question": "As companies move past the experimental phase with Hadoop, many cite the need for additional capabilities, including _______________",
        "Options": [
            "a) Improved data storage and information retrieval",
            "b) Improved extract, transform and load features for data integration",
            "c) Improved data warehousing functionality",
            "d) Improved security, workload management, and SQL support"
        ],
        "Answer": "Answer: d\nExplanation: Adding security to Hadoop is challenging because all the interactions do not follow the classic client-server pattern."
    },
    {
        "id": 12,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hadoop do need specialized hardware to process the data",
            "b) Hadoop 2.0 allows live stream processing of real-time data",
            "c) In the Hadoop programming framework output files are divided into lines or records",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Hadoop batch processes data distributed over a number of computers ranging in 100s and 1000s."
    },
    {
        "id": 13,
        "Question": "According to analysts, for what can traditional IT systems provide a foundation when they’re integrated with big data technologies like Hadoop?",
        "Options": [
            "a) Big data management and data mining",
            "b) Data warehousing and business intelligence",
            "c) Management of Hadoop clusters",
            "d) Collecting and storing unstructured data"
        ],
        "Answer": "Answer: a\nExplanation: Data warehousing integrated with Hadoop would give a better understanding of data."
    },
    {
        "id": 14,
        "Question": "Hadoop is a framework that works with a variety of related tools. Common cohorts include ____________",
        "Options": [
            "a) MapReduce, Hive and HBase",
            "b) MapReduce, MySQL and Google Apps",
            "c) MapReduce, Hummer and Iguana",
            "d) MapReduce, Heron and Trumpet"
        ],
        "Answer": "Answer: a\nExplanation: To use Hive with HBase you’ll typically want to launch two clusters, one to run HBase and the other to run Hive."
    },
    {
        "id": 15,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Hardtop processing capabilities are huge and its real advantage lies in the ability to process terabytes & petabytes of data",
            "b) Hadoop uses a programming model called “MapReduce”, all the programs should conform to this model in order to work on the Hadoop platform",
            "c) The programming model, MapReduce, used by Hadoop is difficult to write and test",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The programming model, MapReduce, used by Hadoop is simple to write and test."
    },
    {
        "id": 16,
        "Question": "What was Hadoop named after?",
        "Options": [
            "a) Creator Doug Cutting’s favorite circus act",
            "b) Cutting’s high school rock band",
            "c) The toy elephant of Cutting’s son",
            "d) A sound Cutting’s laptop made during Hadoop development"
        ],
        "Answer": "Answer: c\nExplanation: Doug Cutting, Hadoop creator, named the framework after his child’s stuffed toy elephant."
    },
    {
        "id": 17,
        "Question": "All of the following accurately describe Hadoop, EXCEPT ____________",
        "Options": [
            "a) Open-source",
            "b) Real-time",
            "c) Java-based",
            "d) Distributed computing approach"
        ],
        "Answer": "Answer: b\nExplanation: Apache Hadoop is an open-source software framework for distributed storage and distributed processing of Big Data on clusters of commodity hardware."
    },
    {
        "id": 18,
        "Question": "__________ can best be described as a programming model used to develop Hadoop-based applications that can process massive amounts of data.",
        "Options": [
            "a) MapReduce",
            "b) Mahout",
            "c) Oozie",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: MapReduce is a programming model and an associated implementation for processing and generating large data sets with a parallel, distributed algorithm."
    },
    {
        "id": 19,
        "Question": "__________ has the world’s largest Hadoop cluster.",
        "Options": [
            "a) Apple",
            "b) Datamatics",
            "c) Facebook",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Facebook has many Hadoop clusters, the largest among them is the one that is used for Data warehousing."
    },
    {
        "id": 20,
        "Question": "Facebook Tackles Big Data With _______ based on Hadoop.",
        "Options": [
            "a) ‘Project Prism’",
            "b) ‘Prism’",
            "c) ‘Project Big’",
            "d) ‘Project Data’"
        ],
        "Answer": "Answer: a\nExplanation: Prism automatically replicates and moves data wherever it’s needed across a vast network of computing facilities."
    },
    {
        "id": 21,
        "Question": "________ is a platform for constructing data flows for extract, transform, and load (ETL) processing and analysis of large datasets.",
        "Options": [
            "a) Pig Latin",
            "b) Oozie",
            "c) Pig",
            "d) Hive"
        ],
        "Answer": "Answer: c\nExplanation: Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs."
    },
    {
        "id": 22,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hive is not a relational database, but a query engine that supports the parts of SQL specific to querying data",
            "b) Hive is  a relational database with SQL support",
            "c) Pig is a relational database with SQL support",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Hive is a SQL-based data warehouse system for Hadoop that facilitates data summarization, ad hoc queries, and the analysis of large datasets stored in Hadoop-compatible file systems."
    },
    {
        "id": 23,
        "Question": "_________ hides the limitations of Java behind a powerful and concise Clojure API for Cascading.",
        "Options": [
            "a) Scalding",
            "b) HCatalog",
            "c) Cascalog",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Cascalog also adds Logic Programming concepts inspired by Datalog. Hence the name “Cascalog” is a contraction of Cascading and Datalog. "
    },
    {
        "id": 24,
        "Question": "Hive also support custom extensions written in ____________",
        "Options": [
            "a) C#",
            "b) Java",
            "c) C",
            "d) C++"
        ],
        "Answer": "Answer: b\nExplanation: Hive also supports custom extensions written in Java, including user-defined functions (UDFs) and serializer-deserializers for reading and optionally writing custom formats."
    },
    {
        "id": 25,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Elastic MapReduce (EMR) is Facebook’s packaged Hadoop offering",
            "b) Amazon Web Service Elastic MapReduce (EMR) is Amazon’s packaged Hadoop offering",
            "c) Scalding is a Scala API on top of Cascading that removes most Java boilerplate",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Rather than building Hadoop deployments manually on EC2 (Elastic Compute Cloud) clusters, users can spin up fully configured Hadoop installations using simple invocation commands, either through the AWS Web Console or through command-line tools."
    },
    {
        "id": 26,
        "Question": "________ is the most popular high-level Java API in Hadoop Ecosystem",
        "Options": [
            "a) Scalding",
            "b) HCatalog",
            "c) Cascalog",
            "d) Cascading"
        ],
        "Answer": "Answer: d\nExplanation: Cascading hides many of the complexities of MapReduce programming behind more intuitive pipes and data flow abstractions."
    },
    {
        "id": 27,
        "Question": "___________ is general-purpose computing model and runtime system for distributed data analytics.",
        "Options": [
            "a) Mapreduce",
            "b) Drill",
            "c) Oozie",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Mapreduce provides a flexible and scalable foundation for analytics, from traditional reporting to leading-edge machine learning algorithms."
    },
    {
        "id": 28,
        "Question": "The Pig Latin scripting language is not only a higher-level data flow language but also has operators similar to ____________",
        "Options": [
            "a) SQL",
            "b) JSON",
            "c) XML",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Pig Latin, in essence, is designed to fill the gap between the declarative style of SQL and the low-level procedural style of MapReduce."
    },
    {
        "id": 29,
        "Question": "_______ jobs are optimized for scalability but not latency.",
        "Options": [
            "a) Mapreduce",
            "b) Drill",
            "c) Oozie",
            "d) Hive"
        ],
        "Answer": "Answer: d\nExplanation: Hive Queries are translated to MapReduce jobs to exploit the scalability of MapReduce."
    },
    {
        "id": 30,
        "Question": "______ is a framework for performing remote procedure calls and data serialization.",
        "Options": [
            "a) Drill",
            "b) BigTop",
            "c) Avro",
            "d) Chukwa"
        ],
        "Answer": "Answer: c\nExplanation: In the context of Hadoop, Avro can be used to pass data from one program or language to another."
    },
    {
        "id": 31,
        "Question": "A ________ node acts as the Slave and is responsible for executing a Task assigned to it by the JobTracker.",
        "Options": [
            "a) MapReduce",
            "b) Mapper",
            "c) TaskTracker",
            "d) JobTracker"
        ],
        "Answer": "Answer: c\nExplanation: TaskTracker receives the information necessary for the execution of a Task from JobTracker, Executes the Task, and Sends the Results back to JobTracker."
    },
    {
        "id": 32,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) MapReduce tries to place the data and the compute as close as possible",
            "b) Map Task in MapReduce is performed using the Mapper() function",
            "c) Reduce Task in MapReduce is performed using the Map() function",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: This feature of MapReduce is “Data Locality”."
    },
    {
        "id": 33,
        "Question": "___________ part of the MapReduce is responsible for processing one or more chunks of data and producing the output results.",
        "Options": [
            "a) Maptask",
            "b) Mapper",
            "c) Task execution",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Map Task in MapReduce is performed using the Map() function."
    },
    {
        "id": 34,
        "Question": "_________ function is responsible for consolidating the results produced by each of the Map() functions/tasks.",
        "Options": [
            "a) Reduce",
            "b) Map",
            "c) Reducer",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Reduce function collates the work and resolves the results."
    },
    {
        "id": 35,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner",
            "b) The MapReduce framework operates exclusively on <key, value> pairs",
            "c) Applications typically implement the Mapper and Reducer interfaces to provide the map and reduce methods",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: The MapReduce framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks."
    },
    {
        "id": 36,
        "Question": "Although the Hadoop framework is implemented in Java, MapReduce applications need not be written in ____________",
        "Options": [
            "a) Java",
            "b) C",
            "c) C#",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Hadoop Pipes is a SWIG- compatible C++ API to implement MapReduce applications (non JNITM based)."
    },
    {
        "id": 37,
        "Question": "________ is a utility which allows users to create and run jobs with any executables as the mapper and/or the reducer.",
        "Options": [
            "a) Hadoop Strdata",
            "b) Hadoop Streaming",
            "c) Hadoop Stream",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Hadoop streaming is one of the most important utilities in the Apache Hadoop distribution."
    },
    {
        "id": 38,
        "Question": "__________ maps input key/value pairs to a set of intermediate key/value pairs.",
        "Options": [
            "a) Mapper",
            "b) Reducer",
            "c) Both Mapper and Reducer",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Maps are the individual tasks that transform input records into intermediate records."
    },
    {
        "id": 39,
        "Question": "The number of maps is usually driven by the total size of ____________",
        "Options": [
            "a) inputs",
            "b) outputs",
            "c) tasks",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Total size of inputs means the total number of blocks of the input files."
    },
    {
        "id": 40,
        "Question": "_________ is the default Partitioner for partitioning key space.",
        "Options": [
            "a) HashPar",
            "b) Partitioner",
            "c) HashPartitioner",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation:  The default partitioner in Hadoop is the HashPartitioner which has a method called getPartition to partition."
    },
    {
        "id": 41,
        "Question": "Running a ___________ program involves running mapping tasks on many or all of the nodes in our cluster.",
        "Options": [
            "a) MapReduce",
            "b) Map",
            "c) Reducer",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: In some applications, component tasks need to create and/or write to side-files, which differ from the actual job-output files."
    },
    {
        "id": 42,
        "Question": "Mapper implementations are passed the JobConf for the job via the ________ method.",
        "Options": [
            "a) JobConfigure.configure",
            "b) JobConfigurable.configure",
            "c) JobConfigurable.configurable",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: JobConfigurable.configure method is overridden to initialize themselves."
    },
    {
        "id": 43,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Applications can use the Reporter to report progress",
            "b) The Hadoop MapReduce framework spawns one map task for each InputSplit generated by the InputFormat for the job",
            "c) The intermediate, sorted outputs are always stored in a simple (key-len, key, value-len, value) format",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Reporters can be used to set application-level status messages and update Counters."
    },
    {
        "id": 44,
        "Question": "Input to the _______ is the sorted output of the mappers.",
        "Options": [
            "a) Reducer",
            "b) Mapper",
            "c) Shuffle",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: In the Shuffle phase the framework fetches the relevant partition of the output of all the mappers, via HTTP."
    },
    {
        "id": 45,
        "Question": "The right number of reduces seems to be ____________",
        "Options": [
            "a) 0.90",
            "b) 0.80",
            "c) 0.36",
            "d) 0.95"
        ],
        "Answer": "Answer: d\nExplanation: The right number of reduces seems to be 0.95 or 1.75."
    },
    {
        "id": 46,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Reducer has 2 primary phases",
            "b) Increasing the number of reduces increases the framework overhead, but increases load balancing and lowers the cost of failures",
            "c) It is legal to set the number of reduce-tasks to zero if no reduction is desired",
            "d) The framework groups Reducer inputs by keys (since different mappers may have output the same key) in the sort stage"
        ],
        "Answer": "Answer: a\nExplanation: Reducer has 3 primary phases: shuffle, sort and reduce."
    },
    {
        "id": 47,
        "Question": "The output of the _______ is not sorted in the Mapreduce framework for Hadoop.",
        "Options": [
            "a) Mapper",
            "b) Cascader",
            "c) Scalding",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: The output of the reduce task is typically written to the FileSystem. The output of the Reducer is not sorted."
    },
    {
        "id": 48,
        "Question": "Which of the following phases occur simultaneously?",
        "Options": [
            "a) Shuffle and Sort",
            "b) Reduce and Sort",
            "c) Shuffle and Map",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The shuffle and sort phases occur simultaneously; while map-outputs are being fetched they are merged."
    },
    {
        "id": 49,
        "Question": "Mapper and Reducer implementations can use the ________ to report progress or just indicate that they are alive.",
        "Options": [
            "a) Partitioner",
            "b) OutputCollector",
            "c) Reporter",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Reporter is a facility for MapReduce applications to report progress, set application-level status messages and update Counters."
    },
    {
        "id": 50,
        "Question": "__________ is a generalization of the facility provided by the MapReduce framework to collect data output by the Mapper or the Reducer.",
        "Options": [
            "a) Partitioner",
            "b) OutputCollector",
            "c) Reporter",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Hadoop MapReduce comes bundled with a library of generally useful mappers, reducers, and partitioners."
    },
    {
        "id": 51,
        "Question": "_________ is the primary interface for a user to describe a MapReduce job to the Hadoop framework for execution.",
        "Options": [
            "a) Map Parameters",
            "b) JobConf",
            "c) MemoryConf",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: JobConf represents a MapReduce job configuration."
    },
    {
        "id": 52,
        "Question": "________ systems are scale-out file-based (HDD) systems moving to more uses of memory in the nodes.",
        "Options": [
            "a) NoSQL",
            "b) NewSQL",
            "c) SQL",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: NoSQL systems make the most sense whenever the application is based on data with varying data types and the data can be stored in key-value notation."
    },
    {
        "id": 53,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hadoop is ideal for the analytical, post-operational, data-warehouse-ish type of workload",
            "b) HDFS runs on a small cluster of commodity-class nodes",
            "c) NEWSQL is frequently the collection point for big data",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Hadoop together with a relational data warehouse, they can form very effective data warehouse infrastructure."
    },
    {
        "id": 54,
        "Question": "Hadoop data is not sequenced and is in 64MB to 256MB block sizes of delimited record values with schema applied on read based on ____________",
        "Options": [
            "a) HCatalog",
            "b) Hive",
            "c) Hbase",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Other means of tagging the values also can be used."
    },
    {
        "id": 55,
        "Question": "__________ are highly resilient and eliminate the single-point-of-failure risk with traditional Hadoop deployments.",
        "Options": [
            "a) EMR",
            "b) Isilon solutions",
            "c) AWS",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: enterprise data protection and security options including file system auditing and data-at-rest encryption to address compliance requirements are also provided by Isilon solution."
    },
    {
        "id": 56,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) EMC Isilon Scale-out Storage Solutions for Hadoop combine a powerful yet simple and highly efficient storage platform",
            "b) Isilon native HDFS integration means you can avoid the need to invest in a separate Hadoop infrastructure",
            "c) NoSQL systems do provide high latency access and accommodate less concurrent users",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: NoSQL systems do provide low latency access and accommodate many concurrent users."
    },
    {
        "id": 57,
        "Question": "HDFS and NoSQL file systems focus almost exclusively on adding nodes to ____________",
        "Options": [
            "a) Scale out",
            "b) Scale up",
            "c) Both Scale out and up",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: HDFS and NoSQL file systems focus almost exclusively on adding nodes to increase performance (scale-out) but even they require node configuration with elements of scale up."
    },
    {
        "id": 58,
        "Question": "Which is the most popular NoSQL database for scalable big data store with Hadoop?",
        "Options": [
            "a) Hbase",
            "b) MongoDB",
            "c) Cassandra",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: HBase is the Hadoop database: a distributed, scalable Big Data store that lets you host very large tables — billions of rows multiplied by millions of columns — on clusters built with commodity hardware."
    },
    {
        "id": 59,
        "Question": "The ___________ can also be used to distribute both jars and native libraries for use in the map and/or reduce tasks.",
        "Options": [
            "a) DataCache",
            "b) DistributedData",
            "c) DistributedCache",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The child-jvm always has its current working directory added to the java.library.path and LD_LIBRARY_PATH."
    },
    {
        "id": 60,
        "Question": "HBase provides ___________ like capabilities on top of Hadoop and HDFS.",
        "Options": [
            "a) TopTable",
            "b) BigTop",
            "c) Bigtable",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation:  Google Bigtable leverages the distributed data storage provided by the Google File System."
    },
    {
        "id": 61,
        "Question": "__________ refers to incremental costs with no major impact on solution design, performance and complexity.",
        "Options": [
            "a) Scale-out",
            "b) Scale-down",
            "c) Scale-up",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Adding more CPU/RAM/Disk capacity to Hadoop DataNode that is already part of a cluster does not require additional network switches."
    },
    {
        "id": 62,
        "Question": "Streaming supports streaming command options as well as _________ command options.",
        "Options": [
            "a) generic",
            "b) tool",
            "c) library",
            "d) task"
        ],
        "Answer": "Answer: a\nExplanation: Place the generic options before the streaming options, otherwise the command will fail."
    },
    {
        "id": 63,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) You can specify any executable as the mapper and/or the reducer",
            "b) You cannot supply a Java class as the mapper and/or the reducer",
            "c) The class you supply for the output format should return key/value pairs of Text class",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: If you do not specify an input format class, the TextInputFormat is used as the default."
    },
    {
        "id": 64,
        "Question": "Which of the following Hadoop streaming command option parameter is required?",
        "Options": [
            "a) output directoryname",
            "b) mapper executable",
            "c) input directoryname",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Required parameters are used for Input and Output location for the mapper."
    },
    {
        "id": 65,
        "Question": "To set an environment variable in a streaming command use ____________",
        "Options": [
            "a) -cmden EXAMPLE_DIR=/home/example/dictionaries/",
            "b) -cmdev EXAMPLE_DIR=/home/example/dictionaries/",
            "c) -cmdenv EXAMPLE_DIR=/home/example/dictionaries/",
            "d) -cmenv EXAMPLE_DIR=/home/example/dictionaries/"
        ],
        "Answer": "Answer: c\nExplanation: Environment Variable is set using cmdenv command."
    },
    {
        "id": 66,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Hadoop has a library package called Aggregate",
            "b) Aggregate allows you to define a mapper plugin class that is expected to generate “aggregatable items” for each input key/value pair of the mappers",
            "c) To use Aggregate, simply specify “-mapper aggregate”",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: To use Aggregate, simply specify “-reducer aggregate”:"
    },
    {
        "id": 67,
        "Question": "The ________ option allows you to copy jars locally to the current working directory of tasks and automatically unjar the files.",
        "Options": [
            "a) archives",
            "b) files",
            "c) task",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Archives options is also a generic option."
    },
    {
        "id": 68,
        "Question": "______________ class allows the Map/Reduce framework to partition the map outputs based on certain key fields, not the whole keys.",
        "Options": [
            "a) KeyFieldPartitioner",
            "b) KeyFieldBasedPartitioner",
            "c) KeyFieldBased",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The primary key is used for partitioning, and the combination of the primary and secondary keys is used for sorting."
    },
    {
        "id": 69,
        "Question": "Which of the following class provides a subset of features provided by the Unix/GNU Sort?",
        "Options": [
            "a) KeyFieldBased",
            "b) KeyFieldComparator",
            "c) KeyFieldBasedComparator",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Hadoop has a library class, KeyFieldBasedComparator, that is useful for many applications."
    },
    {
        "id": 70,
        "Question": "Which of the following class is provided by the Aggregate package?",
        "Options": [
            "a) Map",
            "b) Reducer",
            "c) Reduce",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Aggregate provides a special reducer class and a special combiner class, and a list of simple aggregators that perform aggregations such as “sum”, “max”, “min” and so on over a sequence of values. "
    },
    {
        "id": 71,
        "Question": "Hadoop has a library class, org.apache.hadoop.mapred.lib.FieldSelectionMapReduce, that effectively allows you to process text data like the unix ______ utility.",
        "Options": [
            "a) Copy",
            "b) Cut",
            "c) Paste",
            "d) Move"
        ],
        "Answer": "Answer: b\nExplanation: The map function defined in the class treats each input key/value pair as a list of fields."
    },
    {
        "id": 72,
        "Question": "A ________ serves as the master and there is only one NameNode per cluster.",
        "Options": [
            "a) Data Node",
            "b) NameNode",
            "c) Data block",
            "d) Replication"
        ],
        "Answer": "Answer: b\nExplanation: All the metadata related to HDFS including the information about data nodes, files stored on HDFS, and Replication, etc. are stored and maintained on the NameNode."
    },
    {
        "id": 73,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) DataNode is the slave/worker node and holds the user data in the form of Data Blocks",
            "b) Each incoming file is broken into 32 MB by default",
            "c) Data blocks are replicated across different nodes in the cluster to ensure a low degree of fault tolerance",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: There can be any number of DataNodes in a Hadoop Cluster."
    },
    {
        "id": 74,
        "Question": "HDFS works in a  __________ fashion.",
        "Options": [
            "a) master-worker",
            "b) master-slave",
            "c) worker/slave",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: NameNode servers as the master and each DataNode servers as a worker/slave"
    },
    {
        "id": 75,
        "Question": "________ NameNode is used when the Primary NameNode goes down.",
        "Options": [
            "a) Rack",
            "b) Data",
            "c) Secondary",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Secondary namenode is used for all time availability and reliability."
    },
    {
        "id": 76,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Replication Factor can be configured at a cluster level (Default is set to 3) and also at a file level",
            "b) Block Report from each DataNode contains a list of all the blocks that are stored on that DataNode",
            "c) User data is stored on the local file system of DataNodes",
            "d) DataNode is aware of the files to which the blocks stored on it belong to"
        ],
        "Answer": "Answer: d\nExplanation:  NameNode is aware of the files to which the blocks stored on it belong to."
    },
    {
        "id": 77,
        "Question": "Which of the following scenario may not be a good fit for HDFS?",
        "Options": [
            "a) HDFS is not suitable for scenarios requiring multiple/simultaneous writes to the same file",
            "b) HDFS is suitable for storing data related to applications requiring low latency data access",
            "c) HDFS is suitable for storing data related to applications requiring low latency data access",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: HDFS can be used for storing archive data since it is cheaper as HDFS allows storing the data on low cost commodity hardware while ensuring a high degree of fault-tolerance."
    },
    {
        "id": 78,
        "Question": "The need for data replication can arise in various scenarios like ____________",
        "Options": [
            "a) Replication Factor is changed",
            "b) DataNode goes down",
            "c) Data Blocks get corrupted",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Data is replicated across different DataNodes to ensure a high degree of fault-tolerance."
    },
    {
        "id": 79,
        "Question": "________ is the slave/worker node and holds the user data in the form of Data Blocks.",
        "Options": [
            "a) DataNode",
            "b) NameNode",
            "c) Data block",
            "d) Replication"
        ],
        "Answer": "Answer: a\nExplanation:  A DataNode stores data in the [HadoopFileSystem]. A functional filesystem has more than one DataNode, with data replicated across them."
    },
    {
        "id": 80,
        "Question": "HDFS provides a command line interface called __________ used to interact with HDFS.",
        "Options": [
            "a) “HDFS Shell”",
            "b) “FS Shell”",
            "c) “DFS Shell”",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The File System (FS) shell includes various shell-like commands that directly interact with the Hadoop Distributed File System (HDFS)."
    },
    {
        "id": 81,
        "Question": "HDFS is implemented in _____________ programming language.",
        "Options": [
            "a) C++",
            "b) Java",
            "c) Scala",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: HDFS is implemented in Java and any computer which can run Java can host a NameNode/DataNode on it."
    },
    {
        "id": 82,
        "Question": "For YARN, the ___________ Manager UI provides host and port information.",
        "Options": [
            "a) Data Node",
            "b) NameNode",
            "c) Resource",
            "d) Replication"
        ],
        "Answer": "Answer: c\nExplanation: All the metadata related to HDFS including the information about data nodes, files stored on HDFS, and Replication, etc. are stored and maintained on the NameNode."
    },
    {
        "id": 83,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The Hadoop framework publishes the job flow status to an internally running web server on the master nodes of the Hadoop cluster",
            "b) Each incoming file is broken into 32 MB by default",
            "c) Data blocks are replicated across different nodes in the cluster to ensure a low degree of fault tolerance",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The web interface for the Hadoop Distributed File System (HDFS) shows information about the NameNode itself."
    },
    {
        "id": 84,
        "Question": "For ________ the HBase Master UI provides information about the HBase Master uptime.",
        "Options": [
            "a) HBase",
            "b) Oozie",
            "c) Kafka",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: HBase Master UI provides information about the num­ber of live, dead and transitional servers, logs, ZooKeeper information, debug dumps, and thread stacks."
    },
    {
        "id": 85,
        "Question": "During start up, the ___________ loads the file system state from the fsimage and the edits log file.",
        "Options": [
            "a) DataNode",
            "b) NameNode",
            "c) ActionNode",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: HDFS is implemented on any computer which can run Java can host a NameNode/DataNode on it."
    },
    {
        "id": 86,
        "Question": "In order to read any file in HDFS, instance of __________ is required.",
        "Options": [
            "a) filesystem",
            "b) datastream",
            "c) outstream",
            "d) inputstream"
        ],
        "Answer": "Answer: a\nExplanation: InputDataStream is used to read data from file."
    },
    {
        "id": 87,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The framework groups Reducer inputs by keys",
            "b) The shuffle and sort phases occur simultaneously i.e. while outputs are being fetched they are merged",
            "c) Since JobConf.setOutputKeyComparatorClass(Class) can be used to control how intermediate keys are grouped, these can be used in conjunction to simulate secondary sort on values",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: If equivalence rules for keys while grouping the intermediates are different from those for grouping keys before reduction, then one may specify a Comparator. "
    },
    {
        "id": 88,
        "Question": "______________ is method to copy byte from input stream to any other stream in Hadoop.",
        "Options": [
            "a) IOUtils",
            "b) Utils",
            "c) IUtils",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: IOUtils class is static method in Java interface."
    },
    {
        "id": 89,
        "Question": "_____________ is used to read data from bytes buffers.",
        "Options": [
            "a) write()",
            "b) read()",
            "c) readwrite()",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: readfully method can also be used instead of read method."
    },
    {
        "id": 90,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The framework calls reduce method for each <key, (list of values)> pair in the grouped inputs",
            "b) The output of the Reducer is re-sorted",
            "c) reduce method reduces values for a given key",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The output of the Reducer is not re-sorted."
    },
    {
        "id": 91,
        "Question": "Interface  ____________ reduces a set of intermediate values which share a key to a smaller set of values.",
        "Options": [
            "a) Mapper",
            "b) Reducer",
            "c) Writable",
            "d) Readable"
        ],
        "Answer": "Answer: b\nExplanation: Reducer implementations can access the JobConf for the job."
    },
    {
        "id": 92,
        "Question": "Reducer is input the grouped output of a ____________",
        "Options": [
            "a) Mapper",
            "b) Reducer",
            "c) Writable",
            "d) Readable"
        ],
        "Answer": "Answer: a\nExplanation: In the phase the framework, for each Reducer, fetches the relevant partition of the output of all the Mappers, via HTTP. "
    },
    {
        "id": 93,
        "Question": "The output of the reduce task is typically written to the FileSystem via ____________",
        "Options": [
            "a) OutputCollector",
            "b) InputCollector",
            "c) OutputCollect",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: In reduce phase the reduce(Object, Iterator, OutputCollector, Reporter) method is called for each  pair in the grouped inputs."
    },
    {
        "id": 94,
        "Question": "Applications can use the _________ provided to report progress or just indicate that they are alive.",
        "Options": [
            "a) Collector",
            "b) Reporter",
            "c) Dashboard",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: In scenarios where the application takes a significant amount of time to process individual key/value pairs, this is crucial since the framework might assume that the task has timed-out and kill that task. "
    },
    {
        "id": 95,
        "Question": "Which of the following parameter is to collect keys and combined values?",
        "Options": [
            "a) key",
            "b) values",
            "c) reporter",
            "d) output"
        ],
        "Answer": "Answer: d\nExplanation: The reporter parameter is for a facility to report progress."
    },
    {
        "id": 96,
        "Question": "________ is a programming model designed for processing large volumes of data in parallel by dividing the work into a set of independent tasks.",
        "Options": [
            "a) Hive",
            "b) MapReduce",
            "c) Pig",
            "d) Lucene"
        ],
        "Answer": "Answer: b\nExplanation: MapReduce is the heart of hadoop."
    },
    {
        "id": 97,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Data locality means movement of the algorithm to the data instead of data to algorithm",
            "b) When the processing is done on the data algorithm is moved across the Action Nodes rather than data to the algorithm",
            "c) Moving Computation is expensive than Moving Data",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Data flow framework possesses the feature of data locality."
    },
    {
        "id": 98,
        "Question": "The daemons associated with the MapReduce phase are ________ and task-trackers.",
        "Options": [
            "a) job-tracker",
            "b) map-tracker",
            "c) reduce-tracker",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Map-Reduce jobs are submitted on job-tracker."
    },
    {
        "id": 99,
        "Question": "The JobTracker pushes work out to available _______ nodes in the cluster, striving to keep the work as close to the data as possible.",
        "Options": [
            "a) DataNodes",
            "b) TaskTracker",
            "c) ActionNodes",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: A heartbeat is sent from the TaskTracker to the JobTracker every few minutes to check its status whether the node is dead or alive."
    },
    {
        "id": 100,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The map function in Hadoop MapReduce have the following general form:map:(K1, V1) -> list(K2, V2)",
            "b) The reduce function in Hadoop MapReduce have the following general form: reduce: (K2, list(V2)) -> list(K3, V3)",
            "c) MapReduce has a complex model of data processing: inputs and outputs for the map and reduce functions are key-value pairs",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: MapReduce is relatively simple model to implement in Hadoop."
    },
    {
        "id": 101,
        "Question": "InputFormat class calls the ________ function and computes splits for each file and then sends them to the jobtracker.",
        "Options": [
            "a) puts",
            "b) gets",
            "c) getSplits",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: InputFormat uses their storage locations to schedule map tasks to process them on the tasktrackers."
    },
    {
        "id": 102,
        "Question": " On a tasktracker, the map task passes the split to the createRecordReader() method on InputFormat to obtain a _________ for that split.",
        "Options": [
            "a) InputReader",
            "b) RecordReader",
            "c) OutputReader",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The RecordReader loads data from its source and converts into key-value pairs suitable for reading by mapper."
    },
    {
        "id": 103,
        "Question": "The default InputFormat is __________ which treats each value of input a new value and the associated key is byte offset.",
        "Options": [
            "a) TextFormat",
            "b) TextInputFormat",
            "c) InputFormat",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: A RecordReader is little more than an iterator over records, and the map task uses one to generate record key-value pairs."
    },
    {
        "id": 104,
        "Question": "__________ controls the partitioning of the keys of the intermediate map-outputs.",
        "Options": [
            "a) Collector",
            "b) Partitioner",
            "c) InputFormat",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The output of the mapper is sent to the partitioner."
    },
    {
        "id": 105,
        "Question": "Output of the mapper is first written on the local disk for sorting and _________ process.",
        "Options": [
            "a) shuffling",
            "b) secondary sorting",
            "c) forking",
            "d) reducing"
        ],
        "Answer": "Answer: a\nExplanation: All values corresponding to the same key will go the same reducer."
    },
    {
        "id": 106,
        "Question": "_________ is the name of the archive you would like to create.",
        "Options": [
            "a) archive",
            "b) archiveName",
            "c) name",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The name should have a *.har extension."
    },
    {
        "id": 107,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) A Hadoop archive maps to a file system directory",
            "b) Hadoop archives are special format archives",
            "c) A Hadoop archive always has a *.har extension",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: A Hadoop archive directory contains metadata (in the form of _index and _masterindex) and data (part-*) files."
    },
    {
        "id": 108,
        "Question": "Using Hadoop Archives in __________ is as easy as specifying a different input filesystem than the default file system.",
        "Options": [
            "a) Hive",
            "b) Pig",
            "c) MapReduce",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Hadoop Archives is exposed as a file system MapReduce will be able to use all the logical input files in Hadoop Archives as input."
    },
    {
        "id": 109,
        "Question": "The __________ guarantees that excess resources taken from a queue will be restored to it within N minutes of its need for them.",
        "Options": [
            "a) capacitor",
            "b) scheduler",
            "c) datanode",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Free resources can be allocated to any queue beyond its guaranteed capacity."
    },
    {
        "id": 110,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The Hadoop archive exposes itself as a file system layer",
            "b) Hadoop archives are immutable",
            "c) Archive rename, deletes and creates return an error",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: All the fs shell commands in the archives work but with a different URI."
    },
    {
        "id": 111,
        "Question": "_________ is a pluggable Map/Reduce scheduler for Hadoop which provides a way to share large clusters.",
        "Options": [
            "a) Flow Scheduler",
            "b) Data Scheduler",
            "c) Capacity Scheduler",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The Capacity Scheduler supports multiple queues, where a job is submitted to a queue."
    },
    {
        "id": 112,
        "Question": "Which of the following parameter describes destination directory which would contain the archive?",
        "Options": [
            "a) -archiveName <name>",
            "b) <source>",
            "c) <destination>",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation:  -archiveName <name> is the name of the archive to be created."
    },
    {
        "id": 113,
        "Question": "_________ identifies filesystem path names which work as usual with regular expressions.",
        "Options": [
            "a) -archiveName <name>",
            "b) <source>",
            "c) <destination>",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation:  identifies destination directory which would contain the archive."
    },
    {
        "id": 114,
        "Question": " __________ is the parent argument used to specify the relative path to which the files should be archived to",
        "Options": [
            "a) -archiveName <name>",
            "b) -p <parent_path>",
            "c) <destination>",
            "d) <source>"
        ],
        "Answer": "Answer: b\nExplanation: The hadoop archive command creates a Hadoop archive, a file that contains other files."
    },
    {
        "id": 115,
        "Question": "Hadoop I/O Hadoop comes with a set of ________ for data I/O.",
        "Options": [
            "a) methods",
            "b) commands",
            "c) classes",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Hadoop I/O consist of primitives for serialization and deserialization."
    },
    {
        "id": 116,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The sequence file also can contain a “secondary” key-value list that can be used as file Metadata",
            "b) SequenceFile formats share a header that contains some information which allows the reader to recognize is format",
            "c) There’re Key and Value Class Name’s that allow the reader to instantiate those classes, via reflection, for reading",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: In contrast with other persistent key-value data structures like B-Trees, you can’t seek to specified key editing, adding or removing it."
    },
    {
        "id": 117,
        "Question": "Apache Hadoop ___________ provides a persistent data structure for binary key-value pairs.",
        "Options": [
            "a) GetFile",
            "b) SequenceFile",
            "c) Putfile",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: SequenceFile is append-only."
    },
    {
        "id": 118,
        "Question": "How many formats of SequenceFile are present in Hadoop I/O?",
        "Options": [
            "a) 2",
            "b) 3",
            "c) 4",
            "d) 5"
        ],
        "Answer": "Answer: b\nExplanation: SequenceFile has 3 available formats: An “Uncompressed” format, a “Record Compressed” format and a “Block-Compressed”."
    },
    {
        "id": 119,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The data file contains all the key, value records but key N + 1 must be greater than or equal to the key N",
            "b) Sequence file is a kind of hadoop file based data structure",
            "c) Map file type is splittable as it contains a sync point after several records",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Map file is again a kind of hadoop file based data structure and it differs from a sequence file in a matter of the order."
    },
    {
        "id": 120,
        "Question": "Which of the following format is more compression-aggressive?",
        "Options": [
            "a) Partition Compressed",
            "b) Record Compressed",
            "c) Block-Compressed",
            "d) Uncompressed"
        ],
        "Answer": "Answer: c\nExplanation: SequenceFile key-value list can be just a Text/Text pair, and is written to the file during the initialization that happens in the SequenceFile."
    },
    {
        "id": 121,
        "Question": "The __________ is a directory that contains two SequenceFile.",
        "Options": [
            "a) ReduceFile",
            "b) MapperFile",
            "c) MapFile",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Sequence files are data file (“/data”) and the index file (“/index”)."
    },
    {
        "id": 122,
        "Question": "The ______ file is populated with the key and a LongWritable that contains the starting byte position of the record.",
        "Options": [
            "a) Array",
            "b) Index",
            "c) Immutable",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Index doesn’t contains all the keys but just a fraction of the keys."
    },
    {
        "id": 123,
        "Question": "The _________ as just the value field append(value) and the key is a LongWritable that contains the record number, count + 1.",
        "Options": [
            "a) SetFile",
            "b) ArrayFile",
            "c) BloomMapFile",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The SetFile instead of append(key, value) as just the key field append(key) and the value is always the NullWritable instance."
    },
    {
        "id": 124,
        "Question": "____________  data file takes is based on avro serialization framework which was primarily created for hadoop.",
        "Options": [
            "a) Oozie",
            "b) Avro",
            "c) cTakes",
            "d) Lucene"
        ],
        "Answer": "Answer: b\nExplanation: Avro is a splittable data format with a metadata section at the beginning and then a sequence of avro serialized objects."
    },
    {
        "id": 125,
        "Question": "The _________ codec from Google provides modest compression ratios.",
        "Options": [
            "a) Snapcheck",
            "b) Snappy",
            "c) FileCompress",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Snappy has fast compression and decompression speeds."
    },
    {
        "id": 126,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Snappy is licensed under the GNU Public License (GPL)",
            "b) BgCIK needs to create an index when it compresses a file",
            "c) The Snappy codec is integrated into Hadoop Common, a set of common utilities that supports other Hadoop subprojects",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: You can use Snappy as an add-on for more recent versions of Hadoop that do not yet provide Snappy codec support."
    },
    {
        "id": 127,
        "Question": "Which of the following compression is similar to Snappy compression?",
        "Options": [
            "a) LZO",
            "b) Bzip2",
            "c) Gzip",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: LZO is only really desirable if you need to compress text files."
    },
    {
        "id": 128,
        "Question": "Which of the following supports splittable compression?",
        "Options": [
            "a) LZO",
            "b) Bzip2",
            "c) Gzip",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: LZO enables the parallel processing of compressed text file splits by your MapReduce jobs."
    },
    {
        "id": 129,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) From a usability standpoint, LZO and Gzip are similar",
            "b) Bzip2 generates a better compression ratio than does Gzip, but it’s much slower",
            "c) Gzip is a compression utility that was adopted by the GNU project",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: From a usability standpoint, Bzip2 and Gzip are similar."
    },
    {
        "id": 130,
        "Question": "Which of the following is the slowest compression technique?",
        "Options": [
            "a) LZO",
            "b) Bzip2",
            "c) Gzip",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Of all the available compression codecs in Hadoop, Bzip2 is by far the slowest."
    },
    {
        "id": 131,
        "Question": "Gzip (short for GNU zip) generates compressed files that have a _________ extension.",
        "Options": [
            "a) .gzip",
            "b) .gz",
            "c) .gzp",
            "d) .g"
        ],
        "Answer": "Answer: b\nExplanation: You can use the gunzip command to decompress files that were created by a number of compression utilities, including Gzip."
    },
    {
        "id": 132,
        "Question": "Which of the following is based on the DEFLATE algorithm?",
        "Options": [
            "a) LZO",
            "b) Bzip2",
            "c) Gzip",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: gzip is based on the DEFLATE algorithm, which is a combination of LZ77 and Huffman Coding."
    },
    {
        "id": 133,
        "Question": "__________  typically compresses files to within 10% to 15% of the best available techniques.",
        "Options": [
            "a) LZO",
            "b) Bzip2",
            "c) Gzip",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: bzip2 is a freely available, patent free (see below), high-quality data compressor."
    },
    {
        "id": 134,
        "Question": "The LZO compression format is composed of approximately __________ blocks of compressed data.",
        "Options": [
            "a) 128k",
            "b) 256k",
            "c) 24k",
            "d) 36k"
        ],
        "Answer": "Answer: b\nExplanation: LZO was designed with speed in mind: it decompresses about twice as fast as gzip, meaning it’s fast enough to keep up with hard drive read speeds."
    },
    {
        "id": 135,
        "Question": "The HDFS client software implements __________ checking on the contents of HDFS files.",
        "Options": [
            "a) metastore",
            "b) parity",
            "c) checksum",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: When a client creates an HDFS file, it computes a checksum of each block of the file and stores these checksums in a separate hidden file in the same HDFS namespace."
    },
    {
        "id": 136,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The HDFS architecture is compatible with data rebalancing schemes",
            "b) Datablocks support storing a copy of data at a particular instant of time",
            "c) HDFS currently support snapshots",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: A scheme might automatically move data from one DataNode to another if the free space on a DataNode falls below a certain threshold."
    },
    {
        "id": 137,
        "Question": "The ___________ machine is a single point of failure for an HDFS cluster.",
        "Options": [
            "a) DataNode",
            "b) NameNode",
            "c) ActionNode",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: If the NameNode machine fails, manual intervention is necessary. Currently, automatic restart and failover of the NameNode software to another machine is not supported."
    },
    {
        "id": 138,
        "Question": "The ____________ and the EditLog are central data structures of HDFS.",
        "Options": [
            "a) DsImage",
            "b) FsImage",
            "c) FsImages",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: A corruption of these files can cause the HDFS instance to be non-functional."
    },
    {
        "id": 139,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) HDFS is designed to support small files only",
            "b) Any update to either the FsImage or EditLog causes each of the FsImages and EditLogs to get updated synchronously",
            "c) NameNode can be configured to support maintaining multiple copies of the FsImage and EditLog",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: HDFS is designed to support very large files."
    },
    {
        "id": 140,
        "Question": "__________ support storing a copy of data at a particular instant of time.",
        "Options": [
            "a) Data Image",
            "b) Datanots",
            "c) Snapshots",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: One usage of the snapshot feature may be to roll back a corrupted HDFS instance to a previously known good point in time."
    },
    {
        "id": 141,
        "Question": "Automatic restart and ____________ of the NameNode software to another machine is not supported.",
        "Options": [
            "a) failover",
            "b) end",
            "c) scalability",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: If the NameNode machine fails, manual intervention is necessary."
    },
    {
        "id": 142,
        "Question": "HDFS, by default, replicates each data block _____ times on different nodes and on at least ____ racks.",
        "Options": [
            "a) 3, 2",
            "b) 1, 2",
            "c) 2, 3",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: HDFS has a simple yet robust architecture that was explicitly designed for data reliability in the face of faults and failures in disks, nodes and networks."
    },
    {
        "id": 143,
        "Question": "_________ stores its metadata on multiple disks that typically include a non-local file server.",
        "Options": [
            "a) DataNode",
            "b) NameNode",
            "c) ActionNode",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: HDFS tolerates failures of storage servers (called DataNodes) and its disks."
    },
    {
        "id": 144,
        "Question": "The HDFS file system is temporarily unavailable whenever the HDFS ________ is down.",
        "Options": [
            "a) DataNode",
            "b) NameNode",
            "c) ActionNode",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: When the HDFS NameNode is restarted it recovers its metadata."
    },
    {
        "id": 145,
        "Question": "Apache _______ is a serialization framework that produces data in a compact binary format.",
        "Options": [
            "a) Oozie",
            "b) Impala",
            "c) kafka",
            "d) Avro"
        ],
        "Answer": "Answer: d\nExplanation: Apache Avro doesn’t require proxy objects or code generation."
    },
    {
        "id": 146,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Apache Avro is a framework that allows you to serialize data in a format that has a schema built in",
            "b) The serialized data is in a compact binary format that doesn’t require proxy objects or code generation",
            "c) Including schemas with the Avro messages allows any application to deserialize the data",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Instead of using generated proxy libraries and strong typing, Avro relies heavily on the schemas that are sent along with the serialized data."
    },
    {
        "id": 147,
        "Question": "Avro schemas describe the format of the message and are defined using ______________",
        "Options": [
            "a) JSON",
            "b) XML",
            "c) JS",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The JSON schema content is put into a file."
    },
    {
        "id": 148,
        "Question": "The ____________ is an iterator which reads through the file and returns objects using the next() method.",
        "Options": [
            "a) DatReader",
            "b) DatumReader",
            "c) DatumRead",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: DatumReader reads the content through the DataFileReader implementation."
    },
    {
        "id": 149,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Java code is used to deserialize the contents of the file into objects",
            "b) Avro allows you to use complex data structures within Hadoop MapReduce jobs",
            "c) The m2e plugin automatically downloads the newly added JAR files and their dependencies",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: A unit test is useful because you can make assertions to verify that the values of the deserialized object are the same as the original values."
    },
    {
        "id": 150,
        "Question": "The ____________ class extends and implements several Hadoop-supplied interfaces.",
        "Options": [
            "a) AvroReducer",
            "b) Mapper",
            "c) AvroMapper",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: AvroMapper is used to provide the ability to collect or map data."
    },
    {
        "id": 151,
        "Question": "____________ class accepts the values that the ModelCountMapper object has collected.",
        "Options": [
            "a) AvroReducer",
            "b) Mapper",
            "c) AvroMapper",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: AvroReducer summarizes them by looping through the values."
    },
    {
        "id": 152,
        "Question": "The ________ method in the ModelCountReducer class “reduces” the values the mapper collects into a derived value.",
        "Options": [
            "a) count",
            "b) add",
            "c) reduce",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: In some cases, it can be a simple sum of the values."
    },
    {
        "id": 153,
        "Question": "Which of the following works well with Avro?",
        "Options": [
            "a) Lucene",
            "b) kafka",
            "c) MapReduce",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: You can use Avro and MapReduce together to process many items serialized with Avro’s small binary format."
    },
    {
        "id": 154,
        "Question": "__________ tools is used to generate proxy objects in Java to easily work with the objects.",
        "Options": [
            "a) Lucene",
            "b) kafka",
            "c) MapReduce",
            "d) Avro"
        ],
        "Answer": "Answer: d\nExplanation: Avro serialization includes the schema with it — in JSON format — which allows you to have different versions of objects."
    },
    {
        "id": 155,
        "Question": "Avro schemas are defined with _____",
        "Options": [
            "a) JSON",
            "b) XML",
            "c) JAVA",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: JSON implementation facilitates implementation in languages that already have JSON libraries."
    },
    {
        "id": 156,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Avro provides functionality similar to systems such as Thrift",
            "b) When Avro is used in RPC, the client and server exchange data in the connection handshake",
            "c) Apache Avro, Avro, Apache, and the Avro and Apache logos are trademarks of The Java Foundation",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Avro differs from these systems in the fundamental aspects like untagged data."
    },
    {
        "id": 157,
        "Question": "__________ facilitates construction of generic data-processing systems and languages.",
        "Options": [
            "a) Untagged data",
            "b) Dynamic typing",
            "c) No manually-assigned field IDs",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Avro does not require that code be generated."
    },
    {
        "id": 158,
        "Question": "With ______ we can store data and read it easily with various programming languages.",
        "Options": [
            "a) Thrift",
            "b) Protocol Buffers",
            "c) Avro",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Avro is optimized to minimize the disk space needed by our data and it is flexible."
    },
    {
        "id": 159,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Apache Avro™ is a data serialization system",
            "b) Avro provides simple integration with dynamic languages",
            "c) Avro provides rich data structures",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation:  Code generation is not required to read or write data files nor to use or implement RPC protocols in Avro."
    },
    {
        "id": 160,
        "Question": "________ are a way of encoding structured data in an efficient yet extensible format.",
        "Options": [
            "a) Thrift",
            "b) Protocol Buffers",
            "c) Avro",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Google uses Protocol Buffers for almost all of its internal RPC protocols and file formats."
    },
    {
        "id": 161,
        "Question": "Thrift resolves possible conflicts through _________ of the field.",
        "Options": [
            "a) Name",
            "b) Static number",
            "c) UID",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Avro resolves possible conflicts through the name of the field. "
    },
    {
        "id": 162,
        "Question": "Avro is said to be the future _______ layer of Hadoop.",
        "Options": [
            "a) RMC",
            "b) RPC",
            "c) RDC",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: When Avro is used in RPC, the client and server exchange schemas in the connection handshake."
    },
    {
        "id": 163,
        "Question": "When using reflection to automatically build our schemas without code generation, we need to configure Avro using?",
        "Options": [
            "a) AvroJob.Reflect(jConf);",
            "b) AvroJob.setReflect(jConf);",
            "c) Job.setReflect(jConf);",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: For strongly typed languages like Java, it also provides a generation code layer, including RPC services code generation."
    },
    {
        "id": 164,
        "Question": "We can declare the schema of our data either in a ______ file.",
        "Options": [
            "a) JSON",
            "b) XML",
            "c) SQL",
            "d) R"
        ],
        "Answer": "Answer: c\nExplanation: Schema can be declared using an IDL or simply through Java beans by using reflection-based schema building."
    },
    {
        "id": 165,
        "Question": "Which of the following is a primitive data type in Avro?",
        "Options": [
            "a) null",
            "b) boolean",
            "c) float",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Primitive type names are also defined type names."
    },
    {
        "id": 166,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Records use the type name “record” and support three attributes",
            "b) Enum are represented using JSON arrays",
            "c) Avro data is always serialized with its schema",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: A record is encoded by encoding the values of its fields in the order that they are declared."
    },
    {
        "id": 167,
        "Question": "Avro supports ______ kinds of complex types.",
        "Options": [
            "a) 3",
            "b) 4",
            "c) 6",
            "d) 7"
        ],
        "Answer": "Answer: d\nExplanation: Avro supports six kinds of complex types: records, enums, arrays, maps, unions and fixed."
    },
    {
        "id": 168,
        "Question": "________ are encoded as a series of blocks.",
        "Options": [
            "a) Arrays",
            "b) Enum",
            "c) Unions",
            "d) Maps"
        ],
        "Answer": "Answer: a\nExplanation: Each block of the array consists of a long count value, followed by that many array items. A block with count zero indicates the end of the array. Each item is encoded per the array’s item schema."
    },
    {
        "id": 169,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Record, enums and fixed are named types",
            "b) Unions may immediately contain other unions",
            "c) A namespace is a dot-separated sequence of such names",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Unions may not immediately contain other unions."
    },
    {
        "id": 170,
        "Question": "________ instances are encoded using the number of bytes declared in the schema.",
        "Options": [
            "a) Fixed",
            "b) Enum",
            "c) Unions",
            "d) Maps"
        ],
        "Answer": "Answer: a\nExplanation: Except for unions, the JSON encoding is the same as is used to encode field default values."
    },
    {
        "id": 171,
        "Question": "________ permits data written by one system to be efficiently sorted by another system.",
        "Options": [
            "a) Complex Data type",
            "b) Order",
            "c) Sort Order",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Avro binary-encoded data can be efficiently ordered without deserializing it to objects."
    },
    {
        "id": 172,
        "Question": "_____________ are used between blocks to permit efficient splitting of files for MapReduce processing.",
        "Options": [
            "a) Codec",
            "b) Data Marker",
            "c) Synchronization markers",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Avro includes a simple object container file format."
    },
    {
        "id": 173,
        "Question": "The __________ codec uses Google’s Snappy compression library.",
        "Options": [
            "a) null",
            "b) snappy",
            "c) deflate",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Snappy is a compression library developed at Google, and, like many technologies that come from Google, Snappy was designed to be fast."
    },
    {
        "id": 174,
        "Question": "Avro messages are framed as a list of _________",
        "Options": [
            "a) buffers",
            "b) frames",
            "c) rows",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Framing is a layer between messages and transport. It exists to optimize certain operations."
    },
    {
        "id": 175,
        "Question": "_______ can change the maximum number of cells of a column family.",
        "Options": [
            "a) set",
            "b) reset",
            "c) alter",
            "d) select"
        ],
        "Answer": "Answer: c\nExplanation: Alter is the command used to make changes to an existing table."
    },
    {
        "id": 176,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) You can add a column family to a table using the method addColumn()",
            "b) Using alter, you can also create a column family",
            "c) Using disable-all, you can truncate a column family",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Columns can also be added through HbaseAdmin."
    },
    {
        "id": 177,
        "Question": "Which of the following is not a table scope operator?",
        "Options": [
            "a) MEMSTORE_FLUSH",
            "b) MEMSTORE_FLUSHSIZE",
            "c) MAX_FILESIZE",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Using alter, you can set and remove table scope operators such as MAX_FILESIZE, READONLY, MEMSTORE_FLUSHSIZE, DEFERRED_LOG_FLUSH, etc."
    },
    {
        "id": 178,
        "Question": "You can delete a column family from a table using the method _________ of HBAseAdmin class.",
        "Options": [
            "a) delColumn()",
            "b) removeColumn()",
            "c) deleteColumn()",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Alter command also can be used to delete a column family."
    },
    {
        "id": 179,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) To read data from an HBase table, use the get() method of the HTable class",
            "b) You can retrieve data from the HBase table using the get() method of the HTable class",
            "c) While retrieving data, you can get a single row by id, or get a set of rows by a set of row ids, or scan an entire table or a subset of rows",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: You can retrieve an HBase table data using the add method variants in Get class."
    },
    {
        "id": 180,
        "Question": "__________ class adds HBase configuration files to its object.",
        "Options": [
            "a) Configuration",
            "b) Collector",
            "c) Component",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: You can create a configuration object using the create() method of the HbaseConfiguration class."
    },
    {
        "id": 181,
        "Question": "The ________ class provides the getValue() method to read the values from its instance.",
        "Options": [
            "a) Get",
            "b) Result",
            "c) Put",
            "d) Value"
        ],
        "Answer": "Answer: b\nExplanation: Get the result by passing your Get class instance to the get method of the HTable class. This method returns the Result class object, which holds the requested result."
    },
    {
        "id": 182,
        "Question": "________ communicate with the client and handle data-related operations.",
        "Options": [
            "a) Master Server",
            "b) Region Server",
            "c) Htable",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Region Server handle read and write requests for all the regions under it."
    },
    {
        "id": 183,
        "Question": "_________ is the main configuration file of HBase.",
        "Options": [
            "a) hbase.xml",
            "b) hbase-site.xml",
            "c) hbase-site-conf.xml",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Set the data directory to an appropriate location by opening the HBase home folder in /usr/local/HBase."
    },
    {
        "id": 184,
        "Question": "HBase uses the _______ File System to store its data.",
        "Options": [
            "a) Hive",
            "b) Imphala",
            "c) Hadoop",
            "d) Scala"
        ],
        "Answer": "Answer: c\nExplanation: The data storage will be in the form of regions (tables). These regions will be split up and stored in region servers."
    },
    {
        "id": 185,
        "Question": "The Mapper implementation processes one line at a time via _________ method.",
        "Options": [
            "a) map",
            "b) reduce",
            "c) mapper",
            "d) reducer"
        ],
        "Answer": "Answer: a\nExplanation: The Mapper outputs are sorted and then partitioned per Reducer."
    },
    {
        "id": 186,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Mapper maps input key/value pairs to a set of intermediate key/value pairs",
            "b) Applications typically implement the Mapper and Reducer interfaces to provide the map and reduce methods",
            "c) Mapper and Reducer interfaces form the core of the job",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: The transformed intermediate records do not need to be of the same type as the input records."
    },
    {
        "id": 187,
        "Question": "The Hadoop MapReduce framework spawns one map task for each __________ generated by the InputFormat for the job.",
        "Options": [
            "a) OutputSplit",
            "b) InputSplit",
            "c) InputSplitStream",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Mapper implementations are passed the JobConf for the job via the JobConfigurable.configure(JobConf) method and override it to initialize themselves."
    },
    {
        "id": 188,
        "Question": "Users can control which keys (and hence records) go to which Reducer by implementing a custom?",
        "Options": [
            "a) Partitioner",
            "b) OutputSplit",
            "c) Reporter",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Users can control the grouping by specifying a Comparator via JobConf.setOutputKeyComparatorClass(Class)."
    },
    {
        "id": 189,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The Mapper outputs are sorted and then partitioned per Reducer",
            "b) The total number of partitions is the same as the number of reduce tasks for the job",
            "c) The intermediate, sorted outputs are always stored in a simple (key-len, key, value-len, value) format",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: All intermediate values associated with a given output key are subsequently grouped by the framework, and passed to the Reducer(s) to determine the final output."
    },
    {
        "id": 190,
        "Question": "Applications can use the ____________ to report progress and set application-level status messages.",
        "Options": [
            "a) Partitioner",
            "b) OutputSplit",
            "c) Reporter",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Reporter is also used to update Counters, or just indicate that they are alive."
    },
    {
        "id": 191,
        "Question": "The right level of parallelism for maps seems to be around _________ maps per-node.",
        "Options": [
            "a) 1-10",
            "b) 10-100",
            "c) 100-150",
            "d) 150-200"
        ],
        "Answer": "Answer: b\nExplanation: Task setup takes a while, so it is best if the maps take at least a minute to execute."
    },
    {
        "id": 192,
        "Question": "The number of reduces for the job is set by the user via _________",
        "Options": [
            "a) JobConf.setNumTasks(int)",
            "b) JobConf.setNumReduceTasks(int)",
            "c) JobConf.setNumMapTasks(int)",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Reducer has 3 primary phases: shuffle, sort and reduce."
    },
    {
        "id": 193,
        "Question": "The framework groups Reducer inputs by key in _________ stage.",
        "Options": [
            "a) sort",
            "b) shuffle",
            "c) reduce",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The shuffle and sort phases occur simultaneously; while map-outputs are being fetched they are merged."
    },
    {
        "id": 194,
        "Question": "The output of the reduce task is typically written to the FileSystem via _____________",
        "Options": [
            "a) OutputCollector.collect",
            "b) OutputCollector.get",
            "c) OutputCollector.receive",
            "d) OutputCollector.put"
        ],
        "Answer": "Answer: a\nExplanation: The output of the Reducer is not sorted."
    },
    {
        "id": 195,
        "Question": "Which of the following is the default Partitioner for Mapreduce?",
        "Options": [
            "a) MergePartitioner",
            "b) HashedPartitioner",
            "c) HashPartitioner",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The total number of partitions is the same as the number of reduce tasks for the job."
    },
    {
        "id": 196,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The right number of reduces seems to be 0.95 or 1.75",
            "b) Increasing the number of reduces increases the framework overhead",
            "c) With 0.95 all of the reduces can launch immediately and start transferring map outputs as the maps finish",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: With 1.75 the faster nodes will finish their first round of reduces and launch a second wave of reduces doing a much better job of load balancing."
    },
    {
        "id": 197,
        "Question": "Which of the following partitions the key space?",
        "Options": [
            "a) Partitioner",
            "b) Compactor",
            "c) Collector",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Partitioner controls the partitioning of the keys of the intermediate map-outputs."
    },
    {
        "id": 198,
        "Question": "____________ is a generalization of the facility provided by the MapReduce framework to collect data output by the Mapper or the Reducer.",
        "Options": [
            "a) OutputCompactor",
            "b) OutputCollector",
            "c) InputCollector",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Hadoop MapReduce comes bundled with a library of generally useful mappers, reducers, and partitioners."
    },
    {
        "id": 199,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) It is legal to set the number of reduce-tasks to zero if no reduction is desired",
            "b) The outputs of the map-tasks go directly to the FileSystem",
            "c) The Mapreduce framework does not sort the map-outputs before writing them out to the FileSystem",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Outputs of the map-tasks go directly to the FileSystem, into the output path set by setOutputPath(Path)."
    },
    {
        "id": 200,
        "Question": "__________ is the primary interface for a user to describe a MapReduce job to the Hadoop framework for execution.",
        "Options": [
            "a) JobConfig",
            "b) JobConf",
            "c) JobConfiguration",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: JobConf is typically used to specify the Mapper, combiner (if any), Partitioner, Reducer, InputFormat, OutputFormat and OutputCommitter implementations."
    },
    {
        "id": 201,
        "Question": "The ___________ executes the Mapper/ Reducer task as a child process in a separate jvm.",
        "Options": [
            "a) JobTracker",
            "b) TaskTracker",
            "c) TaskScheduler",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The child-task inherits the environment of the parent TaskTracker."
    },
    {
        "id": 202,
        "Question": "Maximum virtual memory of the launched child-task is specified using _________",
        "Options": [
            "a) mapv",
            "b) mapred",
            "c) mapvim",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Admins can also specify the maximum virtual memory of the launched child-task, and any sub-process it launches recursively, using mapred."
    },
    {
        "id": 203,
        "Question": "Which of the following parameter is the threshold for the accounting and serialization buffers?",
        "Options": [
            "a) io.sort.spill.percent",
            "b) io.sort.record.percent",
            "c) io.sort.mb",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: When the percentage of either buffer has filled, their contents will be spilled to disk in the background."
    },
    {
        "id": 204,
        "Question": "______________ is percentage of memory relative to the maximum heap size in which map outputs may be retained during the reduce.",
        "Options": [
            "a) mapred.job.shuffle.merge.percent",
            "b) mapred.job.reduce.input.buffer.percen",
            "c) mapred.inmem.merge.threshold",
            "d) io.sort.factor"
        ],
        "Answer": "Answer: b\nExplanation: When the reduce begins, map outputs will be merged to disk until those that remain are under the resource limit this defines."
    },
    {
        "id": 205,
        "Question": "____________ specifies the number of segments on disk to be merged at the same time.",
        "Options": [
            "a) mapred.job.shuffle.merge.percent",
            "b) mapred.job.reduce.input.buffer.percen",
            "c) mapred.inmem.merge.threshold",
            "d) io.sort.factor"
        ],
        "Answer": "Answer: d\nExplanation: io.sort.factor limits the number of open files and compression codecs during the merge."
    },
    {
        "id": 206,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The number of sorted map outputs fetched into memory before being merged to disk",
            "b) The memory threshold for fetched map outputs before an in-memory merge is finished",
            "c) The percentage of memory relative to the maximum heap size in which map outputs may not be retained during the reduce",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: When the reduce begins, map outputs will be merged to disk until those that remain are under the resource limit this defines. "
    },
    {
        "id": 207,
        "Question": "Map output larger than ___________ percent of the memory allocated to copying map outputs.",
        "Options": [
            "a) 10",
            "b) 15",
            "c) 25",
            "d) 35"
        ],
        "Answer": "Answer: c\nExplanation: Map output will be written directly to disk without first staging through memory."
    },
    {
        "id": 208,
        "Question": "Jobs can enable task JVMs to be reused by specifying the job configuration _________",
        "Options": [
            "a) mapred.job.recycle.jvm.num.tasks",
            "b) mapissue.job.reuse.jvm.num.tasks",
            "c) mapred.job.reuse.jvm.num.tasks",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Many of my tasks had performance improved over 50% using mapissue.job.reuse.jvm.num.tasks. "
    },
    {
        "id": 209,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The task tracker has local directory to create localized cache and localized job",
            "b) The task tracker can define multiple local directories",
            "c) The Job tracker cannot define multiple local directories",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: When the job starts, task tracker creates a localized job directory relative to the local directory specified in the configuration. "
    },
    {
        "id": 210,
        "Question": "During the execution of a streaming job, the names of the _______ parameters are transformed.",
        "Options": [
            "a) vmap",
            "b) mapvim",
            "c) mapreduce",
            "d) mapred"
        ],
        "Answer": "Answer: d\nExplanation: To get the values in a streaming job’s mapper/reducer use the parameter names with the underscores."
    },
    {
        "id": 211,
        "Question": "The standard output (stdout) and error (stderr) streams of the task are read by the TaskTracker and logged to _________",
        "Options": [
            "a) ${HADOOP_LOG_DIR}/user",
            "b) ${HADOOP_LOG_DIR}/userlogs",
            "c) ${HADOOP_LOG_DIR}/logs",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The child-jvm always has its current working directory added to the java.library.path and LD_LIBRARY_PATH."
    },
    {
        "id": 212,
        "Question": "____________ is the primary interface by which user-job interacts with the JobTracker.",
        "Options": [
            "a) JobConf",
            "b) JobClient",
            "c) JobServer",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: JobClient provides facilities to submit jobs, track their progress, access component-tasks’ reports and logs, get the MapReduce cluster status information and so on."
    },
    {
        "id": 213,
        "Question": "The _____________ can also be used to distribute both jars and native libraries for use in the map and/or reduce tasks.",
        "Options": [
            "a) DistributedLog",
            "b) DistributedCache",
            "c) DistributedJars",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Cached libraries can be loaded via System.loadLibrary or System.load."
    },
    {
        "id": 214,
        "Question": "__________ is used to filter log files from the output directory listing.",
        "Options": [
            "a) OutputLog",
            "b) OutputLogFilter",
            "c) DistributedLog",
            "d) DistributedJars"
        ],
        "Answer": "Answer: b\nExplanation: User can view the history logs summary in specified directory using the following command $ bin/hadoop job -history output-dir."
    },
    {
        "id": 215,
        "Question": "Which of the following class provides access to configuration parameters?",
        "Options": [
            "a) Config",
            "b) Configuration",
            "c) OutputConfig",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Configurations are specified by resources."
    },
    {
        "id": 216,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Configuration parameters may be declared static",
            "b) Unless explicitly turned off, Hadoop by default specifies two resources",
            "c) Configuration class provides access to configuration parameters",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Once a resource declares a value final, no subsequently-loaded resource can alter that value."
    },
    {
        "id": 217,
        "Question": "___________ gives site-specific configuration for a given hadoop installation.",
        "Options": [
            "a) core-default.xml",
            "b) core-site.xml",
            "c) coredefault.xml",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: core-default.xml is read-only defaults for hadoop."
    },
    {
        "id": 218,
        "Question": "Administrators typically define parameters as final in __________ for values that user applications may not alter.",
        "Options": [
            "a) core-default.xml",
            "b) core-site.xml",
            "c) coredefault.xml",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Value strings are first processed for variable expansion."
    },
    {
        "id": 219,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) addDeprecations adds a set of deprecated keys to the global deprecations",
            "b) configuration parameters cannot be declared final",
            "c) addDeprecations method is lockless",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Configuration parameters may be declared final."
    },
    {
        "id": 220,
        "Question": "_________ method clears all keys from the configuration.",
        "Options": [
            "a) clear",
            "b) addResource",
            "c) getClass",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: getClass is used to get the value of the name property as a Class."
    },
    {
        "id": 221,
        "Question": "________ method adds the deprecated key to the global deprecation map.",
        "Options": [
            "a) addDeprecits",
            "b) addDeprecation",
            "c) keyDeprecation",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: addDeprecation does not override any existing entries in the deprecation map."
    },
    {
        "id": 222,
        "Question": "________ checks whether the given key is deprecated.",
        "Options": [
            "a) isDeprecated",
            "b) setDeprecated",
            "c) isDeprecatedif",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Method returns true if the key is deprecated and false otherwise."
    },
    {
        "id": 223,
        "Question": "_________ is useful for iterating the properties when all deprecated properties for currently set properties need to be present.",
        "Options": [
            "a) addResource",
            "b) setDeprecatedProperties",
            "c) addDefaultResource",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: setDeprecatedProperties sets all deprecated properties that are not currently set but have a corresponding new property that is set. "
    },
    {
        "id": 224,
        "Question": "Which of the following adds a configuration resource?",
        "Options": [
            "a) addResource",
            "b) setDeprecatedProperties",
            "c) addDefaultResource",
            "d) addResource"
        ],
        "Answer": "Answer: d\nExplanation: The properties of this resource will override the properties of previously added resources unless they were marked final."
    },
    {
        "id": 225,
        "Question": "For running hadoop service daemons in Hadoop in secure mode ___________ principals are required.",
        "Options": [
            "a) SSL",
            "b) Kerberos",
            "c) SSH",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Each service reads authenticate information saved in keytab file with appropriate permission."
    },
    {
        "id": 226,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hadoop does have the definition of group by itself",
            "b) MapReduce JobHistory server run as same user such as mapred",
            "c) SSO environment is managed using Kerberos with LDAP for Hadoop in secure mode",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: You can change a way of mapping by specifying the name of mapping provider as a value of hadoop.security.group.mapping."
    },
    {
        "id": 227,
        "Question": "The simplest way to do authentication is using _________ command of Kerberos.",
        "Options": [
            "a) auth",
            "b) kinit",
            "c) authorize",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: HTTP web-consoles should be served by principal different from RPC’s one."
    },
    {
        "id": 228,
        "Question": "Data transfer between Web-console and clients are protected by using _________",
        "Options": [
            "a) SSL",
            "b) Kerberos",
            "c) SSH",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: AES offers the greatest cryptographic strength and the best performance."
    },
    {
        "id": 229,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Data transfer protocol of DataNode does not use the RPC framework of Hadoop",
            "b) Apache Oozie which access the services of Hadoop on behalf of end users need to be able to impersonate end users",
            "c) DataNode must authenticate itself by using privileged ports which are specified by dfs.datanode.address and dfs.datanode.http.address",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Authentication is based on the assumption that the attacker won’t be able to get root privileges."
    },
    {
        "id": 230,
        "Question": "In order to turn on RPC authentication in hadoop, set the value of hadoop.security.authentication property to _________",
        "Options": [
            "a) zero",
            "b) kerberos",
            "c) false",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Security settings need to be modified properly for robustness."
    },
    {
        "id": 231,
        "Question": "The __________ provides a proxy between the web applications exported by an application and an end user.",
        "Options": [
            "a) ProxyServer",
            "b) WebAppProxy",
            "c) WebProxy",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: If security is enabled it will warn users before accessing a potentially unsafe web application. Authentication and authorization using the proxy is handled just like any other privileged web application."
    },
    {
        "id": 232,
        "Question": "___________ used by YARN framework which defines how any container launched and controlled.",
        "Options": [
            "a) Container",
            "b) ContainerExecutor",
            "c) Executor",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The container process has the same Unix user as the NodeManager."
    },
    {
        "id": 233,
        "Question": "The ____________ requires that paths including and leading up to the directories specified in yarn.nodemanager.local-dirs.",
        "Options": [
            "a) TaskController",
            "b) LinuxTaskController",
            "c) LinuxController",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: LinuxTaskController keeps track of all paths and directories on datanode."
    },
    {
        "id": 234,
        "Question": "The configuration file must be owned by the user running _________",
        "Options": [
            "a) DataManager",
            "b) NodeManager",
            "c) ValidationManager",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: To recap, local file-system permissions need to be modified."
    },
    {
        "id": 235,
        "Question": "__________ storage is a solution to decouple growing storage capacity from compute capacity.",
        "Options": [
            "a) DataNode",
            "b) Archival",
            "c) Policy",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Nodes with higher density and less expensive storage with low compute power are becoming available."
    },
    {
        "id": 236,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) When there is enough space, block replicas are stored according to the storage type list",
            "b) One_SSD is used for storing all replicas in SSD",
            "c) Hot policy is useful only for single replica blocks",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The first phase of Heterogeneous Storage changed datanode storage model from a single storage."
    },
    {
        "id": 237,
        "Question": "___________ is added for supporting writing single replica files in memory.",
        "Options": [
            "a) ROM_DISK",
            "b) ARCHIVE",
            "c) RAM_DISK",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: DISK is the default storage type."
    },
    {
        "id": 238,
        "Question": "Which of the following has high storage density?",
        "Options": [
            "a) ROM_DISK",
            "b) ARCHIVE",
            "c) RAM_DISK",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Little compute power is added for supporting archival storage."
    },
    {
        "id": 239,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) A Storage policy consists of the Policy ID",
            "b) The storage policy can be specified using the “dfsadmin -setStoragePolicy” command",
            "c) dfs.storage.policy.enabled is used for enabling/disabling the storage policy feature",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: The effective storage policy can be retrieved by the “dfsadmin -getStoragePolicy” command."
    },
    {
        "id": 240,
        "Question": "Which of the following storage policy is used for both storage and compute?",
        "Options": [
            "a) Hot",
            "b) Cold",
            "c) Warm",
            "d) All_SSD"
        ],
        "Answer": "Answer: a\nExplanation: When a block is hot, all replicas are stored in DISK."
    },
    {
        "id": 241,
        "Question": "Which of the following is only for storage with limited compute?",
        "Options": [
            "a) Hot",
            "b) Cold",
            "c) Warm",
            "d) All_SSD"
        ],
        "Answer": "Answer: b\nExplanation: When a block is cold, all replicas are stored in the ARCHIVE."
    },
    {
        "id": 242,
        "Question": "When a block is warm, some of its replicas are stored in DISK and the remaining replicas are stored in _________",
        "Options": [
            "a) ROM_DISK",
            "b) ARCHIVE",
            "c) RAM_DISK",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Warm storage policy is partially hot and partially cold."
    },
    {
        "id": 243,
        "Question": "____________ is used for storing one of the replicas in SSD.",
        "Options": [
            "a) Hot",
            "b) Lazy_Persist",
            "c) One_SSD",
            "d) All_SSD"
        ],
        "Answer": "Answer: c\nExplanation: The remaining replicas are stored in DISK."
    },
    {
        "id": 244,
        "Question": "___________ is used for writing blocks with single replica in memory.",
        "Options": [
            "a) Hot",
            "b) Lazy_Persist",
            "c) One_SSD",
            "d) All_SSD"
        ],
        "Answer": "Answer: b\nExplanation: The replica is first written in RAM_DISK and then it is lazily persisted in DISK."
    },
    {
        "id": 245,
        "Question": "_________ is a  data migration tool added for archiving data.",
        "Options": [
            "a) Mover",
            "b) Hiver",
            "c) Serde",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Mover periodically scans the files in HDFS to check if the block placement satisfies the storage policy."
    },
    {
        "id": 246,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Mover is not similar to Balancer",
            "b) hdfs dfsadmin -setStoragePolicy <path> <policyName> puts a storage policy to a file or a directory.",
            "c) addCacheArchive add archives to be localized",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: addArchiveToClassPath(Path archive) adds an archive path to the current set of classpath entries."
    },
    {
        "id": 247,
        "Question": "Which of the following is used to list out the storage policies?",
        "Options": [
            "a) hdfs storagepolicies",
            "b) hdfs storage",
            "c) hd storagepolicies",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Arguments are none for the hdfs storagepolicies command."
    },
    {
        "id": 248,
        "Question": "Which of the following statement can be used to get the storage policy of a file or a directory?",
        "Options": [
            "a) hdfs dfsadmin -getStoragePolicy path",
            "b) hdfs dfsadmin -setStoragePolicy path policyName",
            "c) hdfs dfsadmin -listStoragePolicy path policyName",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation:  refers to the path referring to either a directory or a file."
    },
    {
        "id": 249,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) getInstance() creates a new Job with particular cluster",
            "b) getInstance(Configuration conf) creates a new Job with no particular Cluster and a given Configuration",
            "c) getInstance(JobStatus status, Configuration conf) creates a new Job with no particular Cluster and given Configuration and JobStatus",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: getInstance() creates a new Job with particular cluster."
    },
    {
        "id": 250,
        "Question": "Which of the following method is used to get user-specified job name?",
        "Options": [
            "a) getJobName()",
            "b) getJobState()",
            "c) getPriority()",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: getPriority() is used to get scheduling info of the job."
    },
    {
        "id": 251,
        "Question": "__________ get events indicating completion (success/failure) of component tasks.",
        "Options": [
            "a) getJobName()",
            "b) getJobState()",
            "c) getPriority()",
            "d) getTaskCompletionEvents(int startFrom)"
        ],
        "Answer": "Answer: d\nExplanation: getPriority() provides scheduling info of the job."
    },
    {
        "id": 252,
        "Question": "_________ gets the diagnostic messages for a given task attempt.",
        "Options": [
            "a) getTaskOutputFilter(Configuration conf)",
            "b) getTaskReports(TaskType type)",
            "c) getTrackingURL()",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: getTaskDiagnostics(TaskAttemptID taskid) gets the diagnostic messages for a given task attempt."
    },
    {
        "id": 253,
        "Question": "reduceProgress() gets the progress of the job’s reduce-tasks, as a float between _________",
        "Options": [
            "a) 0.0-1.0",
            "b) 1.0-2.0",
            "c) 2.0-3.0",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: mapProgress() is used to get the progress of the job’s map-tasks, as a float between 0.0 and 1.0."
    },
    {
        "id": 254,
        "Question": "The Job makes a copy of the _____________ so that any necessary internal modifications do not reflect on the incoming parameter.",
        "Options": [
            "a) Component",
            "b) Configuration",
            "c) Collector",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: A Cluster will be created from the conf parameter only when it’s needed."
    },
    {
        "id": 255,
        "Question": "________ is the architectural center of Hadoop that allows multiple data processing engines.",
        "Options": [
            "a) YARN",
            "b) Hive",
            "c) Incubator",
            "d) Chuckwa"
        ],
        "Answer": "Answer: a\nExplanation: YARN is the prerequisite for Enterprise Hadoop, providing resource management and a central platform to deliver consistent operations, security, and data governance tools across Hadoop clusters."
    },
    {
        "id": 256,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) YARN also extends the power of Hadoop to incumbent and new technologies found within the data center",
            "b) YARN is the central point of investment for Hortonworks within the Apache community",
            "c) YARN enhances a Hadoop compute cluster in many ways",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: YARN provides ISVs and developers a consistent framework for writing data access applications that run IN Hadoop."
    },
    {
        "id": 257,
        "Question": "YARN’s dynamic allocation of cluster resources improves utilization over more static _______ rules used in early versions of Hadoop.",
        "Options": [
            "a) Hive",
            "b) MapReduce",
            "c) Imphala",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Multi-tenant data processing improves an enterprise’s return on its Hadoop investments."
    },
    {
        "id": 258,
        "Question": "The __________ is a framework-specific entity that negotiates resources from the ResourceManager.",
        "Options": [
            "a) NodeManager",
            "b) ResourceManager",
            "c) ApplicationMaster",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Each ApplicationMaster has the responsibility for negotiating appropriate resource containers from the schedule."
    },
    {
        "id": 259,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) From the system perspective, the ApplicationMaster runs as a normal container",
            "b) The ResourceManager is the per-machine slave, which is responsible for launching the applications’ containers",
            "c) The NodeManager is the per-machine slave, which is responsible for launching the applications’ containers, monitoring their resource usage",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: ResourceManager has a scheduler, which is responsible for allocating resources to the various applications running in the cluster, according to constraints such as queue capacities and user limits."
    },
    {
        "id": 260,
        "Question": "Apache Hadoop YARN stands for _________",
        "Options": [
            "a) Yet Another Reserve Negotiator",
            "b) Yet Another Resource Network",
            "c) Yet Another Resource Negotiator",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: YARN is a cluster management technology."
    },
    {
        "id": 261,
        "Question": "MapReduce has undergone a complete overhaul in hadoop is _________",
        "Options": [
            "a) 0.21",
            "b) 0.23",
            "c) 0.24",
            "d) 0.26"
        ],
        "Answer": "Answer: b\nExplanation: The fundamental idea of MRv2 is to split up the two major functionalities of the JobTracker."
    },
    {
        "id": 262,
        "Question": "The ____________ is the ultimate authority that arbitrates resources among all the applications in the system.",
        "Options": [
            "a) NodeManager",
            "b) ResourceManager",
            "c) ApplicationMaster",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The ResourceManager and per-node slave, the NodeManager (NM), form the data-computation framework."
    },
    {
        "id": 263,
        "Question": "The __________ is responsible for allocating resources to the various running applications subject to familiar constraints of capacities, queues etc.",
        "Options": [
            "a) Manager",
            "b) Master",
            "c) Scheduler",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The Scheduler is a pure scheduler in the sense that it performs no monitoring or tracking of status for the application."
    },
    {
        "id": 264,
        "Question": "The CapacityScheduler supports _____________ queues to allow for more predictable sharing of cluster resources.",
        "Options": [
            "a) Networked",
            "b) Hierarchical",
            "c) Partition",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The Scheduler has a pluggable policy plugin, which is responsible for partitioning the cluster resources among the various queues, applications etc."
    },
    {
        "id": 265,
        "Question": "Yarn commands are invoked by the ________ script.",
        "Options": [
            "a) hive",
            "b) bin",
            "c) hadoop",
            "d) home"
        ],
        "Answer": "Answer: b\nExplanation: Running the yarn script without any arguments prints the description for all commands."
    },
    {
        "id": 266,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Each queue has strict ACLs which controls which users can submit applications to individual queues",
            "b) Hierarchy of queues is supported to ensure resources are shared among the sub-queues of an organization",
            "c) Queues are allocated a fraction of the capacity of the grid in the sense that a certain capacity of resources will be at their disposal",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: All applications submitted to a queue will have access to the capacity allocated to the queue."
    },
    {
        "id": 267,
        "Question": "The queue definitions and properties such as ________ ACLs can be changed, at runtime.",
        "Options": [
            "a) tolerant",
            "b) capacity",
            "c) speed",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Administrators can add additional queues at runtime, but queues cannot be deleted at runtime."
    },
    {
        "id": 268,
        "Question": "The CapacityScheduler has a predefined queue called _________",
        "Options": [
            "a) domain",
            "b) root",
            "c) rear",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: All queues in the system are children of the root queue."
    },
    {
        "id": 269,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The multiple of the queue capacity which can be configured to allow a single user to acquire more resources",
            "b) Changing queue properties and adding new queues is very simple",
            "c) Queues cannot be deleted, only addition of new queues is supported",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: You need to edit conf/capacity-scheduler.xml and run yarn rmadmin -refreshQueues for changing queue properties."
    },
    {
        "id": 270,
        "Question": "The updated queue configuration should be a valid one i.e. queue-capacity at each level should be equal to _________",
        "Options": [
            "a) 50%",
            "b) 75%",
            "c) 100%",
            "d) 0%"
        ],
        "Answer": "Answer: c\nExplanation: Queues cannot be deleted, only the addition of new queues is supported."
    },
    {
        "id": 271,
        "Question": "Users can bundle their Yarn code in a _________ file and execute it using jar command.",
        "Options": [
            "a) java",
            "b) jar",
            "c) C code",
            "d) xml"
        ],
        "Answer": "Answer: b\nExplanation: Usage: yarn jar <jar> [mainClass] args…"
    },
    {
        "id": 272,
        "Question": "Which of the following command is used to dump the log container?",
        "Options": [
            "a) logs",
            "b) log",
            "c) dump",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Usage: yarn logs -applicationId <application ID> <options>."
    },
    {
        "id": 273,
        "Question": "__________ will clear the RMStateStore and is useful if past applications are no longer needed.",
        "Options": [
            "a) -format-state",
            "b) -form-state-store",
            "c) -format-state-store",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: -format-state-store formats the RMStateStore."
    },
    {
        "id": 274,
        "Question": "Which of the following command runs ResourceManager admin client?",
        "Options": [
            "a) proxyserver",
            "b) run",
            "c) admin",
            "d) rmadmin"
        ],
        "Answer": "Answer: d\nExplanation: proxyserver command starts the web proxy server."
    },
    {
        "id": 275,
        "Question": "___________ generates keys of type LongWritable and values of type Text.",
        "Options": [
            "a) TextOutputFormat",
            "b) TextInputFormat",
            "c) OutputInputFormat",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: If K2 and K3 are the same, you don’t need to call setMapOutputKeyClass()."
    },
    {
        "id": 276,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The reduce input must have the same types as the map output, although the reduce output types may be different again",
            "b) The map input key and value types (K1 and V1) are different from the map output types",
            "c) The partition function operates on the intermediate key",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: In practice, the partition is determined solely by the key (the value is ignored).\n"
    },
    {
        "id": 277,
        "Question": "In _____________ the default job is similar, but not identical, to the Java equivalent.",
        "Options": [
            "a) Mapreduce",
            "b) Streaming",
            "c) Orchestration",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: MapReduce Types and Formats MapReduce has a simple model of data processing."
    },
    {
        "id": 278,
        "Question": "An input _________ is a chunk of the input that is processed by a single map.",
        "Options": [
            "a) textformat",
            "b) split",
            "c) datanode",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Each split is divided into records, and the map processes each record—a key-value pair—in turn.\n"
    },
    {
        "id": 279,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) If V2 and V3 are the same, you only need to use setOutputValueClass()",
            "b) The overall effect of Streaming job is to perform a sort of the input",
            "c) A Streaming application can control the separator that is used when a key-value pair is turned into a series of bytes and sent to the map or reduce process over standard input",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: If a combine function is used then it is the same form as the reduce function, except its output types are the intermediate key and value types (K2 and V2), so they can feed the reduce function."
    },
    {
        "id": 280,
        "Question": "An ___________ is responsible for creating the input splits, and dividing them into records.",
        "Options": [
            "a) TextOutputFormat",
            "b) TextInputFormat",
            "c) OutputInputFormat",
            "d) InputFormat"
        ],
        "Answer": "Answer: d\nExplanation: As a MapReduce application writer, you don’t need to deal with InputSplits directly, as they are created by an InputFormat."
    },
    {
        "id": 281,
        "Question": "______________ is another implementation of the MapRunnable interface that runs mappers concurrently in a configurable number of threads.",
        "Options": [
            "a) MultithreadedRunner",
            "b) MultithreadedMap",
            "c) MultithreadedMapRunner",
            "d) SinglethreadedMapRunner"
        ],
        "Answer": "Answer: c\nExplanation: A RecordReader is little more than an iterator over records, and the map task uses one to generate record key-value pairs, which it passes to the map function."
    },
    {
        "id": 282,
        "Question": "Which of the following is the only way of running mappers?",
        "Options": [
            "a) MapReducer",
            "b) MapRunner",
            "c) MapRed",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Having calculated the splits, the client sends them to the jobtracker."
    },
    {
        "id": 283,
        "Question": "_________ is the base class for all implementations of InputFormat that use files as their data source.",
        "Options": [
            "a) FileTextFormat",
            "b) FileInputFormat",
            "c) FileOutputFormat",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: FileInputFormat provides implementation for generating splits for the input files.\n"
    },
    {
        "id": 284,
        "Question": "Which of the following method add a path or paths to the list of inputs?",
        "Options": [
            "a) setInputPaths()",
            "b) addInputPath()",
            "c) setInput()",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: FileInputFormat offers four static convenience methods for setting a JobConf input paths."
    },
    {
        "id": 285,
        "Question": "___________ takes node and rack locality into account when deciding which blocks to place in the same split.",
        "Options": [
            "a) CombineFileOutputFormat",
            "b) CombineFileInputFormat",
            "c) TextFileInputFormat",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: CombineFileInputFormat does not compromise the speed at which it can process the input in a typical MapReduce job.\n"
    },
    {
        "id": 286,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) With TextInputFormat and KeyValueTextInputFormat, each mapper receives a variable number of lines of input",
            "b) StreamXmlRecordReader, the page elements can be interpreted as records for processing by a mapper",
            "c) The number depends on the size of the split and the length of the lines.",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Large XML documents that are composed of a series of “records” can be broken into these records using simple string or regular-expression matching to find start and end tags of records."
    },
    {
        "id": 287,
        "Question": "The key, a ____________ is the byte offset within the file of the beginning of the line.",
        "Options": [
            "a) LongReadable",
            "b) LongWritable",
            "c) LongWritable",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The value is the contents of the line, excluding any line terminators (newline, carriage return), and is packaged as a Text object."
    },
    {
        "id": 288,
        "Question": "_________ is the output produced by TextOutputFor mat, Hadoop default OutputFormat.",
        "Options": [
            "a) KeyValueTextInputFormat",
            "b) KeyValueTextOutputFormat",
            "c) FileValueTextInputFormat",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: To interpret such files correctly, KeyValueTextInputFormat is appropriate."
    },
    {
        "id": 289,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Hadoop sequence file format stores sequences of binary key-value pairs",
            "b) SequenceFileAsBinaryInputFormat is a variant of SequenceFileInputFormat that retrieves the sequence file’s keys and values as opaque binary objects",
            "c) SequenceFileAsTextInputFormat is a variant of SequenceFileInputFormat that retrieves the sequence file’s keys and values as opaque binary objects.",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: SequenceFileAsBinaryInputFormat is used for reading keys, values from SequenceFiles in binary (raw) format."
    },
    {
        "id": 290,
        "Question": "__________ is a variant of SequenceFileInputFormat that converts the sequence file’s keys and values to Text objects.",
        "Options": [
            "a) SequenceFile",
            "b) SequenceFileAsTextInputFormat",
            "c) SequenceAsTextInputFormat",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: With multiple reducers, records will be allocated evenly across reduce tasks, with all records that share the same key being processed by the same reduce task."
    },
    {
        "id": 291,
        "Question": "__________ class allows you to specify the InputFormat and Mapper to use on a per-path basis.",
        "Options": [
            "a) MultipleOutputs",
            "b) MultipleInputs",
            "c) SingleInputs",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: One might be tab-separated plain text, the other a binary sequence file. Even if they are in the same format, they may have different representations, and therefore need to be parsed differently."
    },
    {
        "id": 292,
        "Question": "___________ is an input format for reading data from a relational database, using JDBC.",
        "Options": [
            "a) DBInput",
            "b) DBInputFormat",
            "c) DBInpFormat",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: DBInputFormat is the most frequently used format for reading data."
    },
    {
        "id": 293,
        "Question": "Which of the following is the default output format?",
        "Options": [
            "a) TextFormat",
            "b) TextOutput",
            "c) TextOutputFormat",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: TextOutputFormat keys and values may be of any type."
    },
    {
        "id": 294,
        "Question": "Which of the following writes MapFiles as output?",
        "Options": [
            "a) DBInpFormat",
            "b) MapFileOutputFormat",
            "c) SequenceFileAsBinaryOutputFormat",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: SequenceFileAsBinaryOutputFormat writes keys and values in raw binary format into a SequenceFile container."
    },
    {
        "id": 295,
        "Question": "The split size is normally the size of a ________ block, which is appropriate for most applications.",
        "Options": [
            "a) Generic",
            "b) Task",
            "c) Library",
            "d) HDFS"
        ],
        "Answer": "Answer: d\nExplanation: FileInputFormat splits only large files(Here “large” means larger than an HDFS block)."
    },
    {
        "id": 296,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The minimum split size is usually 1 byte, although some formats have a lower bound on the split size",
            "b) Applications may impose a minimum split size",
            "c) The maximum split size defaults to the maximum value that can be represented by a Java long type",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The maximum split size has an effect only when it is less than the block size, forcing splits to be smaller than a block."
    },
    {
        "id": 297,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Hadoop works better with a small number of large files than a large number of small files",
            "b) CombineFileInputFormat is designed to work well with small files",
            "c) CombineFileInputFormat does not compromise the speed at which it can process the input in a typical MapReduce job",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: If the file is very small (“small” means significantly smaller than an HDFS block) and there are a lot of them, then each map task will process very little input, and there will be a lot of them (one per file), each of which imposes extra bookkeeping overhead."
    },
    {
        "id": 298,
        "Question": "Which hdfs command is used to check for various inconsistencies?",
        "Options": [
            "a) fsk",
            "b) fsck",
            "c) fetchdt",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: fsck is designed for reporting problems with various files, for example, missing blocks for a file or under-replicated blocks."
    },
    {
        "id": 299,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) All hadoop commands are invoked by the bin/hadoop script",
            "b) Hadoop has an option parsing framework that employs only parsing generic options",
            "c) Archive command creates a hadoop archive",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Running the hadoop script without any arguments prints the description for all commands."
    },
    {
        "id": 300,
        "Question": "HDFS supports the ____________ command to fetch Delegation Token and store it in a file on the local system.",
        "Options": [
            "a) fetdt",
            "b) fetchdt",
            "c) fsk",
            "d) rec"
        ],
        "Answer": "Answer: b\nExplanation: Delegation token can be later used to access secure server from a non secure client."
    },
    {
        "id": 301,
        "Question": "In ___________ mode, the NameNode will interactively prompt you at the command line about possible courses of action you can take to recover your data.",
        "Options": [
            "a) full",
            "b) partial",
            "c) recovery",
            "d) commit"
        ],
        "Answer": "Answer: c\nExplanation: Recovery mode can cause you to lose data, you should always backup your edit log and fsimage before using it."
    },
    {
        "id": 302,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) classNAME displays the class name needed to get the Hadoop jar",
            "b) Balancer Runs a cluster balancing utility",
            "c) An administrator can simply press Ctrl-C to stop the rebalancing process",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: classpath prints the class path needed to get the Hadoop jar and the required libraries."
    },
    {
        "id": 303,
        "Question": "_________ command is used to copy file or directories recursively.",
        "Options": [
            "a) dtcp",
            "b) distcp",
            "c) dcp",
            "d) distc"
        ],
        "Answer": "Answer: b\nExplanation: Usage of the distcp command: hadoop distcp <srcurl> <desturl>."
    },
    {
        "id": 304,
        "Question": "__________ mode is a Namenode state in which it does not accept changes to the name space.",
        "Options": [
            "a) Recover",
            "b) Safe",
            "c) Rollback",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: dfsadmin runs a HDFS dfsadmin client."
    },
    {
        "id": 305,
        "Question": "__________ command is used to interact and view Job Queue information in HDFS.",
        "Options": [
            "a) queue",
            "b) priority",
            "c) dist",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Hadoop script can be used to invoke any class."
    },
    {
        "id": 306,
        "Question": "Which of the following command runs the HDFS secondary namenode?",
        "Options": [
            "a) secondary namenode",
            "b) secondarynamenode",
            "c) secondary_namenode",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The secondary NameNode merges the fsimage and the edits log files periodically and keeps edits log size within a limit."
    },
    {
        "id": 307,
        "Question": "Which of the following is used for the MapReduce job Tracker node?",
        "Options": [
            "a) mradmin",
            "b) tasktracker",
            "c) jobtracker",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: tasktracker runs a MapReduce task Tracker node."
    },
    {
        "id": 308,
        "Question": "Which of the following is a common hadoop maintenance issue?",
        "Options": [
            "a) Lack of tools",
            "b) Lack of configuration management",
            "c) Lack of web interface",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Without a centralized configuration management framework, you end up with a number of issues that can cascade just as your usage picks up."
    },
    {
        "id": 309,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) RAID is turned off by default",
            "b) Hadoop is designed to be a highly redundant distributed system",
            "c) Hadoop has a networked configuration system",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Hadoop deployment is sometimes difficult to implement."
    },
    {
        "id": 310,
        "Question": "___________ mode allows you to suppress alerts for a host, service, role, or even the entire cluster.",
        "Options": [
            "a) Safe",
            "b) Maintenance",
            "c) Secure",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Maintenance mode can be useful when you need to take actions in your cluster and do not want to see the alerts that will be generated due to those actions."
    },
    {
        "id": 311,
        "Question": "Which of the following is a configuration management system?",
        "Options": [
            "a) Alex",
            "b) Puppet",
            "c) Acem",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Administrators may use configuration management systems such as Puppet and Chef to manage processes."
    },
    {
        "id": 312,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) If you set the HBase service into maintenance mode, then its roles (HBase Master and all Region Servers) are put into effective maintenance mode",
            "b) If you set a host into maintenance mode, then any roles running on that host are put into effective maintenance mode",
            "c) Putting a component into maintenance mode prevent events from being logged",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Maintenance mode only suppresses the alerts that those events would otherwise generate."
    },
    {
        "id": 313,
        "Question": "Which of the following is a common reason to restart hadoop process?",
        "Options": [
            "a) Upgrade Hadoop",
            "b) React to incidents",
            "c) Remove worker nodes",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: The most common reason administrators restart Hadoop processes is to enact configuration changes."
    },
    {
        "id": 314,
        "Question": "__________ Manager’s Service feature monitors dozens of service health and performance metrics about the services and role instances running on your cluster.",
        "Options": [
            "a) Microsoft",
            "b) Cloudera",
            "c) Amazon",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Manager’s Service feature presents health and performance data in a variety of formats."
    },
    {
        "id": 315,
        "Question": "Which of the tab shows all the role instances that have been instantiated for this service?",
        "Options": [
            "a) Service",
            "b) Status",
            "c) Instance",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The Instances page displays the results of the configuration validation checks it performs for all the role instances for this service."
    },
    {
        "id": 316,
        "Question": "__________ is a standard Java API for monitoring and managing applications.",
        "Options": [
            "a) JVX",
            "b) JVM",
            "c) JMX",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Hadoop includes several managed beans (MBeans), which expose Hadoop metrics to JMX-aware applications."
    },
    {
        "id": 317,
        "Question": "NameNode is monitored and upgraded in a __________ transition.",
        "Options": [
            "a) safemode",
            "b) securemode",
            "c) servicemode",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The HDFS service has some unique functions that may result in additional information on its Status and Instances pages."
    },
    {
        "id": 318,
        "Question": "Pig operates in mainly how many nodes?",
        "Options": [
            "a) Two",
            "b) Three",
            "c) Four",
            "d) Five"
        ],
        "Answer": "Answer: a\nExplanation: You can run Pig (execute Pig Latin statements and Pig commands) using various mode: Interactive and Batch Mode."
    },
    {
        "id": 319,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) You can run Pig in either mode using the “pig” command",
            "b) You can run Pig in batch mode using the Grunt shell",
            "c) You can run Pig in interactive mode using the FS shell",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: You can run Pig in either mode using the “pig” command (the bin/pig Perl script) or the “java” command (java -cp pig.jar …)."
    },
    {
        "id": 320,
        "Question": "You can run Pig in batch mode using __________",
        "Options": [
            "a) Pig shell command",
            "b) Pig scripts",
            "c) Pig options",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Pig script contains Pig Latin statements."
    },
    {
        "id": 321,
        "Question": "Pig Latin statements are generally organized in one of the following ways?",
        "Options": [
            "a) A LOAD statement to read data from the file system",
            "b) A series of “transformation” statements to process the data",
            "c) A DUMP statement to view results or a STORE statement to save the results",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: A DUMP or STORE statement is required to generate output."
    },
    {
        "id": 322,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) To run Pig in local mode, you need access to a single machine",
            "b) The DISPLAY operator will display the results to your terminal screen",
            "c) To run Pig in mapreduce mode, you need access to a Hadoop cluster and HDFS installation",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The DUMP operator will display the results to your terminal screen."
    },
    {
        "id": 323,
        "Question": "Which of the following function is used to read data in PIG?",
        "Options": [
            "a) WRITE",
            "b) READ",
            "c) LOAD",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: PigStorage is the default load function."
    },
    {
        "id": 324,
        "Question": "You can run Pig in interactive mode using the ______ shell.",
        "Options": [
            "a) Grunt",
            "b) FS",
            "c) HDFS",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Invoke the Grunt shell using the “pig” command (as shown below) and then enter your Pig Latin statements and Pig commands interactively at the command line."
    },
    {
        "id": 325,
        "Question": "Which of the following is the default mode?",
        "Options": [
            "a) Mapreduce",
            "b) Tez",
            "c) Local",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: To run Pig in mapreduce mode, you need access to a Hadoop cluster and HDFS installation. "
    },
    {
        "id": 326,
        "Question": "Which of the following will run pig in local mode?",
        "Options": [
            "a) $ pig -x local …",
            "b) $ pig -x tez_local …",
            "c) $ pig …",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Specify local mode using the -x flag (pig -x local)."
    },
    {
        "id": 327,
        "Question": "10.$ pig -x tez_local … will enable ________ mode in Pig.",
        "Options": [
            "a) Mapreduce",
            "b) Tez",
            "c) Local",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Tez Local Mode is similar to local mode, except internally Pig will invoke tez runtime engine."
    },
    {
        "id": 328,
        "Question": "1._________  operator is used to review the schema of a relation.",
        "Options": [
            "a) DUMP",
            "b) DESCRIBE",
            "c) STORE",
            "d) EXPLAIN"
        ],
        "Answer": "Answer: b\nExplanation: DESCRIBE returns the schema of a relation."
    },
    {
        "id": 329,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) During the testing phase of your implementation, you can use LOAD to display results to your terminal screen",
            "b) You can view outer relations as well as relations defined in a nested FOREACH statement",
            "c) Hadoop properties are interpreted by Pig",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Viewing outer relations is possible using DESCRIBE operator."
    },
    {
        "id": 330,
        "Question": "Which of the following operator is used to view the map reduce execution plans?",
        "Options": [
            "a) DUMP",
            "b) DESCRIBE",
            "c) STORE",
            "d) EXPLAIN"
        ],
        "Answer": "Answer: d\nExplanation: EXPLAIN displays execution plans. "
    },
    {
        "id": 331,
        "Question": "___________ operator is used to view the step-by-step execution of a series of statements.",
        "Options": [
            "a) ILLUSTRATE",
            "b) DESCRIBE",
            "c) STORE",
            "d) EXPLAIN"
        ],
        "Answer": "Answer: a\nExplanation: ILLUSTRATE allows you to test your programs on small datasets and get faster turnaround times."
    },
    {
        "id": 332,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) ILLUSTRATE operator is used to review how data is transformed through a sequence of Pig Latin statements",
            "b) ILLUSTRATE is based on an example generator",
            "c) Several new private classes make it harder for external tools such as Oozie to integrate with Pig statistics",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Several new public classes make it easier for external tools such as Oozie to integrate with Pig statistics."
    },
    {
        "id": 333,
        "Question": "__________ is a framework for collecting and storing script-level statistics for Pig Latin.",
        "Options": [
            "a) Pig Stats",
            "b) PStatistics",
            "c) Pig Statistics",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The new Pig statistics and the existing Hadoop statistics can also be accessed via the Hadoop job history file."
    },
    {
        "id": 334,
        "Question": "The ________ class mimics the behavior of the Main class but gives users a statistics object back.",
        "Options": [
            "a) PigRun",
            "b) PigRunner",
            "c) RunnerPig",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Optionally, you can call the API with an implementation of progress listener which will be invoked by Pig runtime during the execution."
    },
    {
        "id": 335,
        "Question": "___________ is a simple xUnit framework that enables you to easily test your Pig scripts.",
        "Options": [
            "a) PigUnit",
            "b) PigXUnit",
            "c) PigUnitX",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: With PigUnit you can perform unit testing, regression testing, and rapid prototyping. No cluster setup is required if you run Pig in local mode."
    },
    {
        "id": 336,
        "Question": "Which of the following will compile the Pigunit?",
        "Options": [
            "a) $pig_trunk ant pigunit-jar",
            "b) $pig_tr ant pigunit-jar",
            "c) $pig_ ant pigunit-jar",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The compile will create the pigunit.jar file."
    },
    {
        "id": 337,
        "Question": "PigUnit runs in Pig’s _______ mode by default.",
        "Options": [
            "a) local",
            "b) tez",
            "c) mapreduce",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Local mode does not require a real cluster but a new local one is created each time."
    },
    {
        "id": 338,
        "Question": "__________ abstract class has three main methods for loading data and for most use cases it would suffice to extend it.",
        "Options": [
            "a) Load",
            "b) LoadFunc",
            "c) FuncLoad",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: LoadFunc and StoreFunc implementations should use the Hadoop 20 API based classes."
    },
    {
        "id": 339,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) LoadMeta has methods to convert byte arrays to specific types",
            "b) The Pig load/store API is aligned with Hadoop InputFormat class only",
            "c) LoadPush has methods to push operations from Pig runtime into loader implementations",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Currently only the pushProjection() method is called by Pig to communicate to the loader the exact fields that are required in the Pig script."
    },
    {
        "id": 340,
        "Question": "Which of the following has methods to deal with metadata?",
        "Options": [
            "a) LoadPushDown",
            "b) LoadMetadata",
            "c) LoadCaster",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Most implementation of loaders don’t need to implement this unless they interact with some metadata system."
    },
    {
        "id": 341,
        "Question": "____________ method will be called by Pig both in the front end and back end to pass a unique signature to the Loader.",
        "Options": [
            "a) relativeToAbsolutePath()",
            "b) setUdfContextSignature()",
            "c) getCacheFiles()",
            "d) getShipFiles()"
        ],
        "Answer": "Answer: b\nExplanation: The signature can be used to store into the UDFContext any information which the Loader needs to store between various method invocations in the front end and back end."
    },
    {
        "id": 342,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The load/store UDFs control how data goes into Pig and comes out of Pig.",
            "b) LoadCaster has methods to convert byte arrays to specific types.",
            "c) The meaning of getNext() has changed and is called by Pig runtime to get the last tuple in the data",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The meaning of getNext() has not changed and is called by Pig runtime to get the next tuple in the data."
    },
    {
        "id": 343,
        "Question": "___________ return a list of hdfs files to ship to distributed cache.",
        "Options": [
            "a) relativeToAbsolutePath()",
            "b) setUdfContextSignature()",
            "c) getCacheFiles()",
            "d) getShipFiles()"
        ],
        "Answer": "Answer: d\nExplanation: The default implementation provided in LoadFunc handles this for FileSystem locations."
    },
    {
        "id": 344,
        "Question": "The loader should use ______ method to communicate the load information to the underlying InputFormat.",
        "Options": [
            "a) relativeToAbsolutePath()",
            "b) setUdfContextSignature()",
            "c) getCacheFiles()",
            "d) setLocation()"
        ],
        "Answer": "Answer: d\nExplanation: setLocation() method is called by Pig to communicate the load location to the loader."
    },
    {
        "id": 345,
        "Question": "____________ method enables the RecordReader associated with the InputFormat provided by the LoadFunc is passed to the LoadFunc.",
        "Options": [
            "a) getNext()",
            "b) relativeToAbsolutePath()",
            "c) prepareToRead()",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The RecordReader can then be used by the implementation in getNext() to return a tuple representing a record of data back to pig."
    },
    {
        "id": 346,
        "Question": "__________  method tells LoadFunc which fields are required in the Pig script.",
        "Options": [
            "a) pushProjection()",
            "b) relativeToAbsolutePath()",
            "c) prepareToRead()",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation:  Pig will use the column index requiredField.index to communicate with the LoadFunc about the fields required by the Pig script."
    },
    {
        "id": 347,
        "Question": "A loader implementation should implement __________  if casts (implicit or explicit) from DataByteArray fields to other types need to be supported.",
        "Options": [
            "a) LoadPushDown",
            "b) LoadMetadata",
            "c) LoadCaster",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: LoadCaster has methods to convert byte arrays to specific types."
    },
    {
        "id": 348,
        "Question": "Which of the following is shortcut for DUMP operator?",
        "Options": [
            "a) \\de alias",
            "b) \\d alias",
            "c) \\q",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: If alias is ignored last defined alias will be used."
    },
    {
        "id": 349,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Invoke the Grunt shell using the “enter” command",
            "b) Pig does not support jar files",
            "c) Both the run and exec commands are useful for debugging because you can modify a Pig script in an editor",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Both commands promote Pig script modularity as they allow you to reuse existing components."
    },
    {
        "id": 350,
        "Question": "Which of the following command is used to show values to keys used in Pig?",
        "Options": [
            "a) set",
            "b) declare",
            "c) display",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: All Pig and Hadoop properties can be set, either in the Pig script or via the Grunt command line."
    },
    {
        "id": 351,
        "Question": "Use the __________ command to run a Pig script that can interact with the Grunt shell (interactive mode).",
        "Options": [
            "a) fetch",
            "b) declare",
            "c) run",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: With the run command, every store triggers execution."
    },
    {
        "id": 352,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) You can run Pig scripts from the command line and from the Grunt shell",
            "b) DECLARE defines a Pig macro",
            "c) Use Pig scripts to place Pig Latin statements and Pig commands in a single file",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: DEFINE defines a Pig macro."
    },
    {
        "id": 353,
        "Question": "Which of the following command can be used for debugging?",
        "Options": [
            "a) exec",
            "b) execute",
            "c) error",
            "d) throw"
        ],
        "Answer": "Answer: a\nExplanation: With the exec command, store statements will not trigger execution; rather, the entire script is parsed before execution starts."
    },
    {
        "id": 354,
        "Question": "Which of the following file contains user defined functions (UDFs)?",
        "Options": [
            "a) script2-local.pig",
            "b) pig.jar",
            "c) tutorial.jar",
            "d) excite.log.bz2"
        ],
        "Answer": "Answer: c\nExplanation: tutorial.jar contains java classes also."
    },
    {
        "id": 355,
        "Question": "Which of the following is correct syntax for parameter substitution using cmd?",
        "Options": [
            "a) pig {-param param_name = param_value | -param_file file_name} [-debug | -dryrun] script",
            "b) {%declare | %default} param_name param_value",
            "c) {%declare | %default} param_name param_value cmd",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Parameter Substitution is used to substitute values for parameters at run time."
    },
    {
        "id": 356,
        "Question": "You can specify parameter names and parameter values in one of the ways?",
        "Options": [
            "a) As part of a command line.",
            "b) In parameter file, as part of a command line",
            "c) With the declare statement, as part of Pig script",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Parameter substitution may be used inside of macros."
    },
    {
        "id": 357,
        "Question": "_________ are scanned in the order they are specified on the command line.",
        "Options": [
            "a) Command line parameters",
            "b) Parameter files",
            "c) Declare and default preprocessors",
            "d) Both parameter files and command line parameters"
        ],
        "Answer": "Answer: d\nExplanation: Parameters and command parameters are scanned in FIFO manner."
    },
    {
        "id": 358,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) LoadPredicatePushdown is same as LoadMetadata.setPartitionFilter",
            "b) getOutputFormat() is called by Pig to get the InputFormat used by the loader",
            "c) Pig works with data from many sources",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: For each MapReduce job, the loader produces a tuple with schema (j:map[], m:map[], r:map[])."
    },
    {
        "id": 359,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Pig can invoke code in language like Java Only",
            "b) Pig enables data workers to write complex data transformations without knowing Java",
            "c) Pig’s simple SQL-like scripting language is called Pig Latin, and appeals to developers already familiar with scripting languages and SQL",
            "d) Pig is complete, so you can do all required data manipulations in Apache Hadoop with Pig"
        ],
        "Answer": "Answer: c\nExplanation: Data sources include structured and unstructured data, and store the results into the Hadoop Data File System."
    },
    {
        "id": 360,
        "Question": "Pig Latin is _______ and fits very naturally in the pipeline paradigm while SQL is instead declarative.",
        "Options": [
            "a) functional",
            "b) procedural",
            "c) declarative",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The HadoopJobHistoryLoader in Piggybank loads Hadoop job history files and job xml files from file system. "
    },
    {
        "id": 361,
        "Question": "In comparison to SQL, Pig uses ______________",
        "Options": [
            "a) Lazy evaluation",
            "b) ETL",
            "c) Supports pipeline splits",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: EmbeddedPigStats contains a map of SimplePigStats or TezPigScriptStats, which captures the Pig job launched in the embedded script."
    },
    {
        "id": 362,
        "Question": "Which of the following is an entry in jobconf?",
        "Options": [
            "a) pig.job",
            "b) pig.input.dirs",
            "c) pig.feature",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Through the User Defined Functions(UDF) facility in Pig, Pig can invoke code in many languages like JRuby, Jython and Java."
    },
    {
        "id": 363,
        "Question": "Which of the following command sets the value of a particular configuration variable (key)?",
        "Options": [
            "a) set -v",
            "b) set <key>=<value>",
            "c) set",
            "d) reset"
        ],
        "Answer": "Answer: b\nExplanation:  If you misspell the variable name, the CLI will not show an error."
    },
    {
        "id": 364,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hive Commands are non-SQL statement such as setting a property or adding a resource",
            "b) Set -v prints a list of configuration variables that are overridden by the user or Hive",
            "c) Set sets a list of variables that are overridden by the user or Hive",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Commands can be used in HiveQL scripts or directly in the CLI or Beeline."
    },
    {
        "id": 365,
        "Question": "Which of the following operator executes a shell command from the Hive shell?",
        "Options": [
            "a) |",
            "b) !",
            "c) ^",
            "d) +"
        ],
        "Answer": "Answer: b\nExplanation: Exclamation operator is for execution of command."
    },
    {
        "id": 366,
        "Question": "Which of the following will remove the resource(s) from the distributed cache?",
        "Options": [
            "a) delete FILE[S] <filepath>*",
            "b) delete JAR[S] <filepath>*",
            "c) delete ARCHIVE[S] <filepath>*",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Delete command is used to remove existing resource."
    },
    {
        "id": 367,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) source FILE <filepath> executes a script file inside the CLI",
            "b) bfs <bfs command> executes a dfs command from the Hive shell",
            "c) hive is Query language similar to SQL",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: dfs <dfs command> executes a dfs command from the Hive shell."
    },
    {
        "id": 368,
        "Question": "_________ is a shell utility which can be used to run Hive queries in either interactive or batch mode.",
        "Options": [
            "a) $HIVE/bin/hive",
            "b) $HIVE_HOME/hive",
            "c) $HIVE_HOME/bin/hive",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Various types of command line operations are available in the shell utility."
    },
    {
        "id": 369,
        "Question": "Which of the following is a command line option?",
        "Options": [
            "a) -d,–define <key=value>",
            "b) -e,–define <key=value>",
            "c) -f,–define <key=value>",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation:  Variable substitution to apply to hive commands. e.g. -d A=B or –define A=B."
    },
    {
        "id": 370,
        "Question": "Which is the additional command line option is available in Hive 0.10.0?",
        "Options": [
            "a) –database <dbname>",
            "b) –db <dbname>",
            "c) –dbase <<dbname>",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Database is specified which is to be used.\n"
    },
    {
        "id": 371,
        "Question": "The CLI when invoked without the -i option will attempt to load $HIVE_HOME/bin/.hiverc and $HOME/.hiverc as _______ files.",
        "Options": [
            "a) processing",
            "b) termination",
            "c) initialization",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Hiverc file is loaded as per options selected."
    },
    {
        "id": 372,
        "Question": "When $HIVE_HOME/bin/hive is run without either the -e or -f option, it enters _______ mode.",
        "Options": [
            "a) Batch",
            "b) Interactive shell",
            "c) Multiple",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Use “;” (semicolon) to terminate commands for multiple options available."
    },
    {
        "id": 373,
        "Question": "Hive uses _________ for logging.",
        "Options": [
            "a) logj4",
            "b) log4l",
            "c) log4i",
            "d) log4j"
        ],
        "Answer": "Answer: d\nExplanation:  By default Hive will use hive-log4j.default in the conf/ directory of the Hive installation."
    },
    {
        "id": 374,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) list FILE[S] <filepath>*  executes a Hive query and prints results to standard output",
            "b) <query string> executes a Hive query and prints results to standard output",
            "c) <query> executes a Hive query and prints results to standard output",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: list FILE[S] <filepath>* checks whether the given resources are already added to the distributed cache or not. See Hive Resources below for more information."
    },
    {
        "id": 375,
        "Options": [
            "a) Log level",
            "b) Log modes",
            "c) Log source",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: hive.root.logger specified the logging level as well as the log destination. Specifying console as the target sends the logs to the standard error."
    },
    {
        "id": 376,
        "Question": "HiveServer2 introduced in Hive 0.11 has a new CLI called __________",
        "Options": [
            "a) BeeLine",
            "b) SqlLine",
            "c) HiveLine",
            "d) CLilLine"
        ],
        "Answer": "Answer: a\nExplanation:  Beeline is a JDBC client based on SQLLine."
    },
    {
        "id": 377,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) There are four namespaces for variables in Hive",
            "b) Custom variables can be created in a separate namespace with the define",
            "c) Custom variables can also be created in a separate namespace with hivevar",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Three namespaces for variables are hiveconf, system, and env."
    },
    {
        "id": 378,
        "Question": "HCatalog is installed with Hive, starting with Hive release is ___________",
        "Options": [
            "a) 0.10.0",
            "b) 0.9.0",
            "c) 0.11.0",
            "d) 0.12.0"
        ],
        "Answer": "Answer: c\nExplanation: hcat commands can be issued as hive commands, and vice versa."
    },
    {
        "id": 379,
        "Question": "hiveconf variables are set as normal by using the following statement?",
        "Options": [
            "a) set -v x=myvalue",
            "b) set x=myvalue",
            "c) reset x=myvalue",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: The hiveconf variables are set as normal by set x=myvalue."
    },
    {
        "id": 380,
        "Question": "Variable Substitution is disabled by using ___________",
        "Options": [
            "a) set hive.variable.substitute=false;",
            "b) set hive.variable.substitutevalues=false;",
            "c) set hive.variable.substitute=true;",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Variable substitution is on by default (hive.variable.substitute=true)."
    },
    {
        "id": 381,
        "Question": "_______ supports a new command shell Beeline that works with HiveServer2.",
        "Options": [
            "a) HiveServer2",
            "b) HiveServer3",
            "c) HiveServer4",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The Beeline shell works in both embedded mode as well as remote mode. "
    },
    {
        "id": 382,
        "Question": "In ______ mode HiveServer2 only accepts valid Thrift calls.",
        "Options": [
            "a) Remote",
            "b) HTTP",
            "c) Embedded",
            "d) Interactive"
        ],
        "Answer": "Answer: a\nExplanation: In HTTP mode, the message body contains Thrift payloads."
    },
    {
        "id": 383,
        "Question": "Hive specific commands can be run from Beeline, when the Hive _______ driver is used.",
        "Options": [
            "a) ODBC",
            "b) JDBC",
            "c) ODBC-JDBC",
            "d) All of the Mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Hive specific commands are same as Hive CLI commands."
    },
    {
        "id": 384,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) –helpusage display a usage message",
            "b) The JDBC connection URL format has the prefix jdbc:hive:",
            "c) Starting with Hive 0.14, there are improved SV output formats",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Output formats available are namely DSV, CSV2 and TSV2."
    },
    {
        "id": 385,
        "Question": "_________ reduce the amount of informational messages displayed (true) or not (false).",
        "Options": [
            "a) –silent=[true/false] ",
            "b) –autosave=[true/false] ",
            "c) –force=[true/false] ",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: It also stops displaying the log messages for the query from HiveServer2."
    },
    {
        "id": 386,
        "Question": "Which of the following is used to set transaction isolation level?",
        "Options": [
            "a) –incremental=[true/false] ",
            "b) –isolation=LEVEL",
            "c) –force=[true/false] ",
            "d) –truncateTable=[true/false]"
        ],
        "Answer": "Answer: b\nExplanation: Set the transaction isolation level to TRANSACTION_READ_COMMITTED or TRANSACTION_SERIALIZABLE."
    },
    {
        "id": 387,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) HiveServer2 has a new JDBC driver",
            "b) CSV and TSV output formats are maintained for forward compatibility",
            "c) HiveServer2 supports both embedded and remote access to HiveServer2",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: CSV and TSV output formats are maintained for backward compatibility."
    },
    {
        "id": 388,
        "Question": "The ________ allows users to read or write Avro data as Hive tables.",
        "Options": [
            "a) AvroSerde",
            "b) HiveSerde",
            "c) SqlSerde",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: AvroSerde understands compressed Avro files."
    },
    {
        "id": 389,
        "Question": "Starting in Hive _______ the Avro schema can be inferred from the Hive table schema.",
        "Options": [
            "a) 0.14",
            "b) 0.12",
            "c) 0.13",
            "d) 0.11"
        ],
        "Answer": "Answer: a\nExplanation: Starting in Hive 0.14, columns can be added to an Avro backed Hive table using the Alter Table statement."
    },
    {
        "id": 390,
        "Question": "The AvroSerde has been built and tested against Hive 0.9.1 and later, and uses Avro _______ as of Hive 0.13 and 0.14.",
        "Options": [
            "a) 1.7.4",
            "b) 1.7.2",
            "c) 1.7.3",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: The AvroSerde uses Avro 1.7.5."
    },
    {
        "id": 391,
        "Question": "Which of the following data type is supported by Hive?",
        "Options": [
            "a) map",
            "b) record",
            "c) string",
            "d) enum"
        ],
        "Answer": "Answer: d\nExplanation: Hive has no concept of enums."
    },
    {
        "id": 392,
        "Question": "Which of the following data type is converted to Array prior to Hive 0.12.0?",
        "Options": [
            "a) map",
            "b) long",
            "c) float",
            "d) bytes"
        ],
        "Answer": "Answer: d\nExplanation: Bytes are converted to Array[smallint] prior to Hive 0.12.0."
    },
    {
        "id": 393,
        "Question": "Avro-backed tables can simply be created by using _________ in a DDL statement.",
        "Options": [
            "a) “STORED AS AVRO”",
            "b) “STORED AS HIVE”",
            "c) “STORED AS AVROHIVE”",
            "d) “STORED AS SERDE”"
        ],
        "Answer": "Answer: a\nExplanation: AvroSerDe takes care of creating the appropriate Avro schema from the Hive table schema."
    },
    {
        "id": 394,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Avro Fixed type should be defined in Hive as lists of tiny ints",
            "b) Avro Bytes type should be defined in Hive as lists of tiny ints",
            "c) Avro Enum type should be defined in Hive as strings",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The AvroSerde will convert these to Bytes during the saving process."
    },
    {
        "id": 395,
        "Question": "Types that may be null must be defined as a ______ of that type and Null within Avro.",
        "Options": [
            "a) Union",
            "b) Intersection",
            "c) Set",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: A null in a field that is not so defined will result in an exception during the save. No changes need be made to the Hive schema to support this, as all fields in Hive can be null."
    },
    {
        "id": 396,
        "Question": "The files that are written by the _______ job are valid Avro files.",
        "Options": [
            "a) Avro",
            "b) Map Reduce",
            "c) Hive",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: If you copy these files out, you’ll likely want to rename them with .avro."
    },
    {
        "id": 397,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) To create an Avro-backed table, specify the serde as org.apache.hadoop.hive.serde2.avro.AvroSerDe",
            "b) Avro-backed tables can be created in Hive using AvroSerDe",
            "c) The AvroSerde cannot serialize any Hive table to Avro files",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The AvroSerde can serialize any Hive table to Avro files."
    },
    {
        "id": 398,
        "Question": "Use ________ and embed the schema in the create statement.",
        "Options": [
            "a) schema.literal",
            "b) schema.lit",
            "c) row.literal",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: You can embed the schema directly into the create statement."
    },
    {
        "id": 399,
        "Question": "_______ is interpolated into the quotes to correctly handle spaces within the schema.",
        "Options": [
            "a) $SCHEMA",
            "b) $ROW",
            "c) $SCHEMASPACES",
            "d) $NAMESPACES"
        ],
        "Answer": "Answer: a\nExplanation: Use none to ignore either avro.schema.literal or avro.schema.url."
    },
    {
        "id": 400,
        "Question": "To force Hive to be more verbose, it can be started with ___________",
        "Options": [
            "a) *hive –hiveconf hive.root.logger=INFO,console*",
            "b) *hive –hiveconf hive.subroot.logger=INFO,console*",
            "c) *hive –hiveconf hive.root.logger=INFOVALUE,console*",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: This Statement will spit orders of magnitude more information to the console and will likely include any information the AvroSerde is trying to get you about what went wrong. "
    },
    {
        "id": 401,
        "Question": "________ was designed to overcome limitations of the other Hive file formats.",
        "Options": [
            "a) ORC",
            "b) OPC",
            "c) ODC",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The Optimized Row Columnar (ORC) file format provides a highly efficient way to store Hive data."
    },
    {
        "id": 402,
        "Question": "An ORC file contains groups of row data called __________",
        "Options": [
            "a) postscript",
            "b) stripes",
            "c) script",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The default stripe size is 250 MB. Large stripe sizes enable large, efficient reads from HDFS."
    },
    {
        "id": 403,
        "Question": "Serialization of string columns uses a ________ to form unique column values.",
        "Options": [
            "a) Footer",
            "b) STRIPES",
            "c) Dictionary",
            "d) Index"
        ],
        "Answer": "Answer: c\nExplanation: The dictionary is sorted to speed up predicate filtering and improve compression ratios."
    },
    {
        "id": 404,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The Avro file dump utility analyzes ORC files",
            "b) Streams are compressed using a codec, which is specified as a table property for all streams in that table",
            "c) The ODC file dump utility analyzes ORC files",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The codec can be Snappy, Zlib, or none."
    },
    {
        "id": 405,
        "Question": "_______ is a lossless data compression library that favors speed over compression ratio.",
        "Options": [
            "a) LOZ",
            "b) LZO",
            "c) OLZ",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: lzo and lzop need to be installed on every node in the Hadoop cluster."
    },
    {
        "id": 406,
        "Question": "Which of the following will prefix the query string with parameters?",
        "Options": [
            "a) SET hive.exec.compress.output=false",
            "b) SET hive.compress.output=false",
            "c) SET hive.exec.compress.output=true",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Use lzop command utility or your custom Java to generate .lzo.index for the .lzo files."
    },
    {
        "id": 407,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) TIMESTAMP is Only available starting with Hive 0.10.0",
            "b) DECIMAL introduced in Hive 0.11.0 with a precision of 38 digits",
            "c) Hive 0.13.0 introduced user definable precision and scale",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: TIMESTAMP is available starting with Hive 0.8.0"
    },
    {
        "id": 408,
        "Question": "Integral literals are assumed to be _________ by default.",
        "Options": [
            "a) SMALL INT",
            "b) INT",
            "c) BIG INT",
            "d) TINY INT"
        ],
        "Answer": "Answer: b\nExplanation: Integral literals are assumed to be INT by default, unless the number exceeds the range of INT in which case it is interpreted as a BIGINT, or if one of the following postfixes is present on the number."
    },
    {
        "id": 409,
        "Question": "Hive uses _____ style escaping within the strings.",
        "Options": [
            "a) C",
            "b) Java",
            "c) Python",
            "d) Scala"
        ],
        "Answer": "Answer: a\nExplanation: String literals can be expressed with either single quotes (‘) or double quotes (“). "
    },
    {
        "id": 410,
        "Question": "Which of the following statement will create a column with varchar datatype?",
        "Options": [
            "a) CREATE TABLE foo (bar CHAR(10))",
            "b) CREATE TABLE foo (bar VARCHAR(10))",
            "c) CREATE TABLE foo (bar CHARVARYING(10))",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Varchar datatype was introduced in Hive 0.12.0"
    },
    {
        "id": 411,
        "Question": "_________ will overwrite any existing data in the table or partition.",
        "Options": [
            "a) INSERT WRITE",
            "b) INSERT OVERWRITE",
            "c) INSERT INTO",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: INSERT INTO will append to the table or partition, keeping the existing data intact."
    },
    {
        "id": 412,
        "Question": "Hive does not support literals for ______ types.",
        "Options": [
            "a) Scalar",
            "b) Complex",
            "c) INT",
            "d) CHAR"
        ],
        "Answer": "Answer: b\nExplanation: It is not possible to use them in INSERT INTO…VALUES clauses."
    },
    {
        "id": 413,
        "Question": "HBase is a distributed ________ database built on top of the Hadoop file system.",
        "Options": [
            "a) Column-oriented",
            "b) Row-oriented",
            "c) Tuple-oriented",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: HBase is a data model that is similar to Google’s big table designed to provide quick random access to huge amounts of structured data."
    },
    {
        "id": 414,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) HDFS provides low latency access to single rows from billions of records (Random access)",
            "b) HBase sits on top of the Hadoop File System and provides read and write access",
            "c) HBase is a distributed file system suitable for storing large files",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: One can store the data in HDFS either directly or through HBase. Data consumer reads/accesses the data in HDFS randomly using HBase."
    },
    {
        "id": 415,
        "Question": "HBase is ________ defines only column families.",
        "Options": [
            "a) Row Oriented",
            "b) Schema-less",
            "c) Fixed Schema",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: HBase doesn’t have the concept of fixed columns schema."
    },
    {
        "id": 416,
        "Question": "Apache HBase is a non-relational database modeled after Google’s _________",
        "Options": [
            "a) BigTop",
            "b) Bigtable",
            "c) Scanner",
            "d) FoundationDB"
        ],
        "Answer": "Answer: b\nExplanation: Bigtable acts up on Google File System, likewise Apache HBase works on top of Hadoop and HDFS."
    },
    {
        "id": 417,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) HBase provides only sequential access to data",
            "b) HBase provides high latency batch processing",
            "c) HBase internally provides serialized access",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: HBase internally uses Hash tables and provides random access."
    },
    {
        "id": 418,
        "Question": "The _________ Server assigns regions to the region servers and takes the help of Apache ZooKeeper for this task.",
        "Options": [
            "a) Region",
            "b) Master",
            "c) Zookeeper",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Master Server maintains the state of the cluster by negotiating the load balancing."
    },
    {
        "id": 419,
        "Question": "Which of the following command provides information about the user?",
        "Options": [
            "a) status",
            "b) version",
            "c) whoami",
            "d) user"
        ],
        "Answer": "Answer: c\nExplanation: status command provides the status of HBase, for example, the number of servers."
    },
    {
        "id": 420,
        "Question": "Which of the following command does not operate on tables?",
        "Options": [
            "a) enabled",
            "b) disabled",
            "c) drop",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: is_disabled command verifies whether a table is disabled."
    },
    {
        "id": 421,
        "Question": "_________ command fetches the contents of a row or a cell.",
        "Options": [
            "a) select",
            "b) get",
            "c) put",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: put command puts a cell value at a specified column in a specified row in a particular table."
    },
    {
        "id": 422,
        "Question": "HBaseAdmin and ____________ are the two important classes in this package that provide DDL functionalities.",
        "Options": [
            "a) HTableDescriptor",
            "b) HDescriptor",
            "c) HTable",
            "d) HTabDescriptor"
        ],
        "Answer": "Answer: a\nExplanation: Java provides an Admin API to achieve DDL functionalities through programming."
    },
    {
        "id": 423,
        "Question": "The minimum number of row versions to keep is configured per column family via _____________",
        "Options": [
            "a) HBaseDecriptor",
            "b) HTabDescriptor",
            "c) HColumnDescriptor",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The minimum number of row versions parameter is used together with the time-to-live parameter and can be combined with the number of row versions parameter."
    },
    {
        "id": 424,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The default for max versions is 1",
            "b) It is recommended setting the number of max versions to an exceedingly high level",
            "c) HBase does overwrite row values",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The number of max versions may need to be increased or decreased depending on application needs."
    },
    {
        "id": 425,
        "Question": "HBase supports a ____________ interface via Put and Result.",
        "Options": [
            "a) “bytes-in/bytes-out”",
            "b) “bytes-in”",
            "c) “bytes-out”",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Input could be strings, numbers, complex objects, or even images as long as they can rendered as bytes."
    },
    {
        "id": 426,
        "Question": "One supported data type that deserves special mention are ____________",
        "Options": [
            "a) money",
            "b) counters",
            "c) smallint",
            "d) tinyint"
        ],
        "Answer": "Answer: b\nExplanation: Synchronization on counters are done on the RegionServer, not in the client."
    },
    {
        "id": 427,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Where time-ranges are very wide (e.g., year-long report) and where the data is voluminous, summary tables are a common approach",
            "b) Coprocessors act like RDBMS triggers",
            "c) HBase does not currently support ‘constraints’ in traditional (SQL) database parlance",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The advised usage for Constraints is in enforcing business rules for attributes in the table. "
    },
    {
        "id": 428,
        "Question": "The _________ suffers from the monotonically increasing rowkey problem.",
        "Options": [
            "a) rowkey",
            "b) columnkey",
            "c) counterkey",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Attention must be paid to the number of buckets because this will require the same number of scans to return results."
    },
    {
        "id": 429,
        "Question": "__________ does re-write data and pack rows into columns for certain time-periods.",
        "Options": [
            "a) OpenTS",
            "b) OpenTSDB",
            "c) OpenTSD",
            "d) OpenDB"
        ],
        "Answer": "Answer: b\nExplanation: OpenTSDB is a very advanced processing technique."
    },
    {
        "id": 430,
        "Question": "Which command is used to disable all the tables matching the given regex?",
        "Options": [
            "a) remove all",
            "b) drop all",
            "c) disable_all",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The syntax for disable_all command is as follows : hbase> disable_all ‘r.*’"
    },
    {
        "id": 431,
        "Question": "__________ command disables drops and recreates a table.",
        "Options": [
            "a) drop",
            "b) truncate",
            "c) delete",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The syntax of truncate is as follows: hbase> truncate ‘table name’.\n"
    },
    {
        "id": 432,
        "Question": "Correct and valid syntax for count command is ____________",
        "Options": [
            "a) count ‘<row number>’",
            "b) count ‘<table name>’",
            "c) count ‘<column name>’",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: You can count the number of rows of a table using the count command."
    },
    {
        "id": 433,
        "Question": "_______ can change the maximum number of cells of a column family.",
        "Options": [
            "a) set",
            "b) reset",
            "c) alter",
            "d) select"
        ],
        "Answer": "Answer: c\nExplanation: Alter is the command used to make changes to an existing table."
    },
    {
        "id": 434,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) You can add a column family to a table using the method addColumn()",
            "b) Using alter, you can also create a column family",
            "c) Using disable-all, you can truncate a column family",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Columns can also be added through HbaseAdmin."
    },
    {
        "id": 435,
        "Question": "Which of the following is not a table scope operator?",
        "Options": [
            "a) MEMSTORE_FLUSH",
            "b) MEMSTORE_FLUSHSIZE",
            "c) MAX_FILESIZE",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Using alter, you can set and remove table scope operators such as MAX_FILESIZE, READONLY, MEMSTORE_FLUSHSIZE, DEFERRED_LOG_FLUSH, etc."
    },
    {
        "id": 436,
        "Question": "You can delete a column family from a table using the method _________ of HBAseAdmin class.",
        "Options": [
            "a) delColumn()",
            "b) removeColumn()",
            "c) deleteColumn()",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Alter command also can be used to delete a column family."
    },
    {
        "id": 437,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) To read data from an HBase table, use the get() method of the HTable class",
            "b) You can retrieve data from the HBase table using the get() method of the HTable class",
            "c) While retrieving data, you can get a single row by id, or get a set of rows by a set of row ids, or scan an entire table or a subset of rows",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: You can retrieve an HBase table data using the add method variants in Get class."
    },
    {
        "id": 438,
        "Question": "__________ class adds HBase configuration files to its object.",
        "Options": [
            "a) Configuration",
            "b) Collector",
            "c) Component",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: You can create a configuration object using the create() method of the HbaseConfiguration class."
    },
    {
        "id": 439,
        "Question": "The ________ class provides the getValue() method to read the values from its instance.",
        "Options": [
            "a) Get",
            "b) Result",
            "c) Put",
            "d) Value"
        ],
        "Answer": "Answer: b\nExplanation: Get the result by passing your Get class instance to the get method of the HTable class. This method returns the Result class object, which holds the requested result."
    },
    {
        "id": 440,
        "Question": "________ communicate with the client and handle data-related operations.",
        "Options": [
            "a) Master Server",
            "b) Region Server",
            "c) Htable",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Region Server handle read and write requests for all the regions under it."
    },
    {
        "id": 441,
        "Question": "_________ is the main configuration file of HBase.",
        "Options": [
            "a) hbase.xml",
            "b) hbase-site.xml",
            "c) hbase-site-conf.xml",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Set the data directory to an appropriate location by opening the HBase home folder in /usr/local/HBase."
    },
    {
        "id": 442,
        "Question": "HBase uses the _______ File System to store its data.",
        "Options": [
            "a) Hive",
            "b) Imphala",
            "c) Hadoop",
            "d) Scala"
        ],
        "Answer": "Answer: c\nExplanation: The data storage will be in the form of regions (tables). These regions will be split up and stored in region servers."
    },
    {
        "id": 443,
        "Question": "ZooKeeper itself is intended to be replicated over a sets of hosts called ____________",
        "Options": [
            "a) chunks",
            "b) ensemble",
            "c) subdomains",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: As long as a majority of the servers are available, the ZooKeeper service will be available."
    },
    {
        "id": 444,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) ZooKeeper can achieve high throughput and high latency numbers",
            "b) The fault tolerant ordering means that sophisticated synchronization primitives can be implemented at the client",
            "c) The ZooKeeper implementation puts a premium on high performance, highly available, strictly ordered access",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The performance aspects of ZooKeeper means it can be used in large, distributed systems. "
    },
    {
        "id": 445,
        "Question": "Which of the guarantee is provided by Zookeeper?",
        "Options": [
            "a) Interactivity",
            "b) Flexibility",
            "c) Scalability",
            "d) Reliability"
        ],
        "Answer": "Answer: d\nExplanation: Once an update has been applied, it will persist from that time forward until a client overwrites the update."
    },
    {
        "id": 446,
        "Question": "ZooKeeper is especially fast in ___________ workloads.",
        "Options": [
            "a) write",
            "b) read-dominant",
            "c) read-write",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: ZooKeeper applications run on thousands of machines, and it performs best where reads are more common than writes, at ratios of around 10:1."
    },
    {
        "id": 447,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Distributed applications use SQL to store important configuration information",
            "b) The service maintains a record of all transactions, which can be used for higher-level abstractions, like synchronization primitives",
            "c) ZooKeeper maintains a standard hierarchical name space, similar to files and directories",
            "d) ZooKeeper provides superior reliability through redundant services"
        ],
        "Answer": "Answer: d\nExplanation: Distributed applications use Zookeeper to store and mediate updates to important configuration information."
    },
    {
        "id": 448,
        "Question": "When a _______ is triggered the client receives a packet saying that the znode has changed.",
        "Options": [
            "a) event",
            "b) watch",
            "c) row",
            "d) value"
        ],
        "Answer": "Answer: b\nExplanation: ZooKeeper supports the concept of watches. Clients can set a watch on a znodes."
    },
    {
        "id": 449,
        "Question": "The underlying client-server protocol has changed in version _______ of ZooKeeper.",
        "Options": [
            "a) 2.0.0",
            "b) 3.0.0",
            "c) 4.0.0",
            "d) 6.0.0"
        ],
        "Answer": "Answer: b\nExplanation: Old pre-3.0.0 clients are not guaranteed to operate against upgraded 3.0.0 servers and vice-versa."
    },
    {
        "id": 450,
        "Question": "The java package structure has changed from com.yahoo.zookeeper* to ___________",
        "Options": [
            "a) apache.zookeeper",
            "b) org.apache.zookeeper",
            "c) org.apache.zookeeper.package",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: A number of constants used in the client ZooKeeper API were re-specified using enums (rather than ints)."
    },
    {
        "id": 451,
        "Question": "A number of constants used in the client ZooKeeper API were renamed in order to reduce ________ collision.",
        "Options": [
            "a) value",
            "b) namespace",
            "c) counter",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: ZOOKEEPER-18 removed KeeperStateChanged, use KeeperStateDisconnected instead."
    },
    {
        "id": 452,
        "Question": "ZooKeeper allows distributed processes to coordinate with each other through registers, known as ___________",
        "Options": [
            "a) znodes",
            "b) hnodes",
            "c) vnodes",
            "d) rnodes"
        ],
        "Answer": "Answer: a\nExplanation: Every znode is identified by a path, with path elements separated by a slash."
    },
    {
        "id": 453,
        "Question": "Zookeeper essentially mirrors the _______ functionality exposed in the Linux kernel.",
        "Options": [
            "a) iread",
            "b) inotify",
            "c) iwrite",
            "d) icount"
        ],
        "Answer": "Answer: b\nExplanation: A client can request for Zookeeper to generate the node name to avoid collisions."
    },
    {
        "id": 454,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) If we are to deploy Zookeeper in a distributed environment, we have to think about both the availability and scalability of the service",
            "b) Chubby and Hbase are both much more than a distributed lock service",
            "c) Given a cluster of Zookeeper servers, many can act as leader",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: At its core, Zookeeper is modeled after a straightforward, tree based, file system API."
    },
    {
        "id": 455,
        "Question": " A ___________ server is a machine that keeps a copy of the state of the entire system and persists this information in local log files.",
        "Options": [
            "a) Master",
            "b) Region",
            "c) Zookeeper",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: A very large Hadoop cluster can be supported by multiple ZooKeeper servers."
    },
    {
        "id": 456,
        "Question": "ZooKeeper’s architecture supports high ____________ through redundant services.",
        "Options": [
            "a) flexibility",
            "b) scalability",
            "c) availability",
            "d) interactivity"
        ],
        "Answer": "Answer: c\nExplanation: The clients can thus ask another ZooKeeper master if the first fails to answer."
    },
    {
        "id": 457,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Cluster-wide status centralization service is essential for management and serialization tasks across a large distributed set of servers",
            "b) Within ZooKeeper, an application can create what is called a znode",
            "c) The znode can be updated by any node in the cluster, and any node in the cluster can register to be informed of changes to that znode",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: ZooKeeper provides an infrastructure for cross-node synchronization and can be used by applications to ensure that tasks across the cluster are serialized or synchronized."
    },
    {
        "id": 458,
        "Question": "_________ serves distributed Lucene indexes in a grid environment.",
        "Options": [
            "a) Katta",
            "b) Helprace",
            "c) Neo4j",
            "d) 101tec"
        ],
        "Answer": "Answer: a\nExplanation: Zookeeper is used for node, master and index management in the grid."
    },
    {
        "id": 459,
        "Question": "The Email & Apps team of ___________ uses ZooKeeper to coordinate sharding and responsibility changes in a distributed email client.",
        "Options": [
            "a) Katta",
            "b) Helprace",
            "c) Rackspace",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: ZooKeeper also provides distributed locking for connections to prevent a cluster from overwhelming servers."
    },
    {
        "id": 460,
        "Question": "ZooKeeper is used for configuration, leader election in Cloud edition of ______________",
        "Options": [
            "a) Solr",
            "b) Solur",
            "c) Solar101",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: ZooKeeper is used for internal application development with Solr and Hadoop with Hbase."
    },
    {
        "id": 461,
        "Question": "Helprace is using ZooKeeper on a _______ cluster in conjunction with Hadoop and HBase.",
        "Options": [
            "a) 3-node",
            "b) 4-node",
            "c) 5-node",
            "d) 6-node"
        ],
        "Answer": "Answer: a\nExplanation: Zookeeper is used to manage a system build out of hadoop, katta, oracle batch jobs and a web component."
    },
    {
        "id": 462,
        "Question": "The ZooKeeper Data Directory contains files which are _________ copy of the znodes stored by a particular serving ensemble.",
        "Options": [
            "a) transient",
            "b) read only",
            "c) persistent",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: As changes are made to the znodes these changes are appended to a transaction log, occasionally, when a log grows large, a snapshot of the current state of all znodes will be written to the filesystem. "
    },
    {
        "id": 463,
        "Question": "You need to have _________ installed before running ZooKeeper.",
        "Options": [
            "a) Java",
            "b) C",
            "c) C++",
            "d) SQLGUI"
        ],
        "Answer": "Answer: a\nExplanation: Client bindings are available in several other languages."
    },
    {
        "id": 464,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) All znodes are ephemeral, which means they are describing a “temporary” state",
            "b) /hbase/replication/state contains the list of RegionServers in the main cluster",
            "c) Offline snapshots are coordinated by the Master using ZooKeeper to communicate with the RegionServers using a two-phase-commit-like transaction",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Although the Replication znodes do not describe a temporary state, they are meant to be the source of truth for the replication state, describing the replication state of each machine."
    },
    {
        "id": 465,
        "Question": "How many types of special znodes are present in Zookeeper?",
        "Options": [
            "a) 1",
            "b) 2",
            "c) 3",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: There are two special types of znode: sequential and ephemeral."
    },
    {
        "id": 466,
        "Question": "To register a “watch” on a znode data, you need to use the _______ commands to access the current content or metadata.",
        "Options": [
            "a) stat",
            "b) put",
            "c) receive",
            "d) gets"
        ],
        "Answer": "Answer: a\nExplanation: ZooKeeper can also notify you of changes in a znode content or changes in a znode children."
    },
    {
        "id": 467,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) All the znodes are prefixed using the default /hbase location",
            "b) ZooKeeper provides an interactive shell that allows you to explore the ZooKeeper state",
            "c) The znodes that you’ll most often see are the ones that coordinate operations like Region Assignment",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: The HBase root znode path is configurable using hbase-site.xml, and by default the location is “/hbase”."
    },
    {
        "id": 468,
        "Question": "_______ has a design policy of using ZooKeeper only for transient data.",
        "Options": [
            "a) Hive",
            "b) Imphala",
            "c) Hbase",
            "d) Oozie"
        ],
        "Answer": "Answer: c\nExplanation: If the HBase ZooKeeper data is removed, only the transient operations are affected – data can continue to be written and read to/from HBase."
    },
    {
        "id": 469,
        "Question": "Zookeeper keep track of the cluster state such as the ______ table location.",
        "Options": [
            "a) DOMAIN",
            "b) NODE",
            "c) ROOT",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Zookeeper keeps track of list of online RegionServers, unassigned Regions."
    },
    {
        "id": 470,
        "Question": "The ________ master will register its own address in this znode at startup, making this znode the source of truth for identifying which server is the Master.",
        "Options": [
            "a) active",
            "b) passive",
            "c) region",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Each inactive Master will register itself as backup Master by creating a sub-znode."
    },
    {
        "id": 471,
        "Question": "___________ is used to decommission more than one RegionServer at a time by creating sub-znodes.",
        "Options": [
            "a) /hbase/master",
            "b) /hbase/draining",
            "c) /hbase/passive",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: /hbase/draining lets you decommission multiple RegionServers without having the risk of regions temporarily moved to a RegionServer that will be decommissioned later."
    },
    {
        "id": 472,
        "Question": "The ______ znode is used for synchronizing the changes made to the _acl_ table by the grant/revoke commands.",
        "Options": [
            "a) zcl",
            "b) acl",
            "c) scl",
            "d) bnl"
        ],
        "Answer": "Answer: b\nExplanation: Each table will have a sub-znode (/hbase/acl/tableName) containing the ACLs of the table."
    },
    {
        "id": 473,
        "Question": "BigDecimal is comprised of a ________ with an integer ‘scale’ field.",
        "Options": [
            "a) BigInt",
            "b) BigInteger",
            "c) MediumInt",
            "d) SmallInt"
        ],
        "Answer": "Answer: b\nExplanation: The BigDecimal/BigInteger can also return itself as a ‘long’ value."
    },
    {
        "id": 474,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) BooleanSerializer is used to parse string representations of boolean values into boolean scalar types",
            "b) BlobRef is a wrapper that holds a BLOB either directly",
            "c) BooleanParse is used to parse string representations of boolean values into boolean scalar types",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: BlobRef is used for reference to a file that holds the BLOB data."
    },
    {
        "id": 475,
        "Question": "ClobRef is a wrapper that holds a CLOB either directly or a reference to a file that holds the ______ data.",
        "Options": [
            "a) CLOB",
            "b) BLOB",
            "c) MLOB",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Create a ClobRef based on parsed data from a line of text."
    },
    {
        "id": 476,
        "Question": "__________ encapsulates a set of delimiters used to encode a record.",
        "Options": [
            "a) LargeObjectLoader",
            "b) FieldMapProcessor",
            "c) DelimiterSet",
            "d) LobSerializer"
        ],
        "Answer": "Answer: c\nExplanation: Delimiter set is created with the specified delimiters."
    },
    {
        "id": 477,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Abstract base class that holds a reference to a Blob or a Clob",
            "b) ACCESSORTYPE is the type used to access this data in a streaming fashion",
            "c) CONTAINERTYPE is the type used to hold this data (e.g., BytesWritable)",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: DATATYPE is the type being held (e.g., a byte array)."
    },
    {
        "id": 478,
        "Question": "_________ supports null values for all types.",
        "Options": [
            "a) SmallObjectLoader",
            "b) FieldMapProcessor",
            "c) DelimiterSet",
            "d) JdbcWritableBridge"
        ],
        "Answer": "Answer: d\nExplanation: JdbcWritableBridge class contains a set of methods which can read db columns from a ResultSet into Java types."
    },
    {
        "id": 479,
        "Question": "Which of the following is a singleton instance class?",
        "Options": [
            "a) LargeObjectLoader",
            "b) FieldMapProcessor",
            "c) DelimiterSet",
            "d) LobSerializer"
        ],
        "Answer": "Answer: a\nExplanation: Lifetime is limited to the current TaskInputOutputContext’s life."
    },
    {
        "id": 480,
        "Question": "Which of the following class is used for general processing of error?",
        "Options": [
            "a) LargeObjectLoader",
            "b) ProcessingException",
            "c) DelimiterSet",
            "d) LobSerializer"
        ],
        "Answer": "Answer: b\nExplanation: General error occurs during the processing of a SqoopRecord."
    },
    {
        "id": 481,
        "Question": "Records are terminated by a __________ character.",
        "Options": [
            "a) RECORD_DELIMITER",
            "b) FIELD_DELIMITER",
            "c) FIELD_LIMITER",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Class RecordParser parses a record containing one or more fields."
    },
    {
        "id": 482,
        "Question": "10.The fields parsed by ____________ are backed by an internal buffer.",
        "Options": [
            "a) LargeObjectLoader",
            "b) ProcessingException",
            "c) RecordParser",
            "d) None of the Mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Multiple threads must use separate instances of RecordParser."
    },
    {
        "id": 483,
        "Question": "Which of the following interface is implemented by Sqoop for recording?",
        "Options": [
            "a) SqoopWrite",
            "b) SqoopRecord",
            "c) SqoopRead",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Class SqoopRecord is an interface implemented by the classes generated by sqoop orm.ClassWriter."
    },
    {
        "id": 484,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Interface FieldMapping is used for mapping of field",
            "b) Interface FieldMappable is used for mapping of field",
            "c) Sqoop is nothing but NoSQL to Hadoop",
            "d) Sqoop internally uses ODBC interface so it should work with any JDBC compatible database"
        ],
        "Answer": "Answer: b\nExplanation: FieldMappable Interface describes a class capable of returning a map of the fields of the object to their values."
    },
    {
        "id": 485,
        "Question": "Sqoop is an open source tool written at ________",
        "Options": [
            "a) Cloudera",
            "b) IBM",
            "c) Microsoft",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Sqoop allows users to import data from their relational databases into HDFS and vice versa."
    },
    {
        "id": 486,
        "Question": "Sqoop uses _________ to fetch data from RDBMS and stores that on HDFS.",
        "Options": [
            "a) Hive",
            "b) Map reduce",
            "c) Imphala",
            "d) BigTOP"
        ],
        "Answer": "Answer: b\nExplanation: While fetching, it throttles the number of mappers accessing data on RDBMS to avoid DDoS."
    },
    {
        "id": 487,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Sqoop is used to import complete database",
            "b) Sqoop is used to import selected columns from a particular table",
            "c) Sqoop is used to import selected tables",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Apache Sqoop is a tool which allows users to import data from relational databases to HDFS and export data from HDFS to relational database. "
    },
    {
        "id": 488,
        "Question": "_________ allows users to specify the target location inside of Hadoop.",
        "Options": [
            "a) Imphala",
            "b) Oozie",
            "c) Sqoop",
            "d) Hive"
        ],
        "Answer": "Answer: c\nExplanation: Sqoop is a connectivity tool for moving data from non-Hadoop data stores – such as relational databases and data warehouses – into Hadoop."
    },
    {
        "id": 489,
        "Question": "Microsoft uses a Sqoop-based connector to help transfer data from _________ databases to Hadoop.",
        "Options": [
            "a) PostreSQL",
            "b) SQL Server",
            "c) Oracle",
            "d) MySQL"
        ],
        "Answer": "Answer: b\nExplanation: Sqoop is a command-line interface application for transferring data between relational databases and Hadoop."
    },
    {
        "id": 490,
        "Question": "__________ provides a Couchbase Server-Hadoop connector by means of Sqoop.",
        "Options": [
            "a) MemCache",
            "b) Couchbase",
            "c) Hbase",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Exports can be used to put data from Hadoop into a relational database."
    },
    {
        "id": 491,
        "Question": "Sqoop direct mode does not support imports of ______ columns.",
        "Options": [
            "a) BLOB",
            "b) LONGVARBINARY",
            "c) CLOB",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Use JDBC-based imports for these columns; do not supply the –direct argument to the import tool."
    },
    {
        "id": 492,
        "Question": "Sqoop has been tested with Oracle ______ Express Edition.",
        "Options": [
            "a) 11.2.0",
            "b) 10.2.0",
            "c) 12.2.0",
            "d) 10.3.0"
        ],
        "Answer": "Answer: b\nExplanation: Oracle is notable in its different approach to SQL from the ANSI standard and its non-standard JDBC driver. Therefore, several features work differently."
    },
    {
        "id": 493,
        "Question": "_________  tool can list all the available database schemas.",
        "Options": [
            "a) sqoop-list-tables",
            "b) sqoop-list-databases",
            "c) sqoop-list-schema",
            "d) sqoop-list-columns"
        ],
        "Answer": "Answer: b\nExplanation: Sqoop also includes a primitive SQL execution shell (the sqoop-eval tool)."
    },
    {
        "id": 494,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The sqoop command-line program is a wrapper which runs the bin/hadoop script shipped with Hadoop",
            "b) If $HADOOP_HOME is set, Sqoop will use the default installation location for Cloudera’s Distribution for Hadoop",
            "c) The active Hadoop configuration is loaded from $HADOOP_HOME/conf/, unless the $HADOOP_CONF_DIR environment variable is unset",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: If you have multiple installations of Hadoop present on your machine, you can select the Hadoop installation by setting the $HADOOP_HOME environment variable."
    },
    {
        "id": 495,
        "Question": "Data can be imported in maximum ______ file formats.",
        "Options": [
            "a) 1",
            "b) 2",
            "c) 3",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: You can import data in one of two file formats: delimited text or SequenceFiles."
    },
    {
        "id": 496,
        "Question": "________ text is appropriate for most non-binary data types.",
        "Options": [
            "a) Character",
            "b) Binary",
            "c) Delimited",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Delimited text is the default import format."
    },
    {
        "id": 497,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Avro data files are a compact, efficient binary format that provides interoperability with applications written in other programming languages",
            "b) By default, data is compressed while importing",
            "c) Delimited text also readily supports further manipulation by other tools, such as Hive",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: You can compress your data by using the deflate (gzip) algorithm with the -z or –compress argument, or specify any Hadoop compression codec using the –compression-codec argument."
    },
    {
        "id": 498,
        "Question": "If you set the inline LOB limit to ________ all large objects will be placed in external storage.",
        "Options": [
            "a) 0",
            "b) 1",
            "c) 2",
            "d) 3"
        ],
        "Answer": "Answer: a\nExplanation: The size at which lobs spill into separate files is controlled by the –inline-lob-limit argument, which takes a parameter specifying the largest lob size to keep inline, in bytes."
    },
    {
        "id": 499,
        "Question": "________ does not support the notion of enclosing characters that may include field delimiters in the enclosed string.",
        "Options": [
            "a) Imphala",
            "b) Oozie",
            "c) Sqoop",
            "d) Hive"
        ],
        "Answer": "Answer: d\nExplanation: Even though Hive supports escaping characters, it does not handle escaping of new-line character."
    },
    {
        "id": 500,
        "Question": "Sqoop can also import the data into Hive by generating and executing a ____________ statement to define the data’s layout in Hive.",
        "Options": [
            "a) SET TABLE",
            "b) CREATE TABLE",
            "c) INSERT TABLE",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Importing data into Hive is as simple as adding the –hive-import option to your Sqoop command line."
    },
    {
        "id": 501,
        "Question": "The __________ tool imports a set of tables from an RDBMS to HDFS.",
        "Options": [
            "a) export-all-tables",
            "b) import-all-tables",
            "c) import-tables",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Data from each table is stored in a separate directory in HDFS."
    },
    {
        "id": 502,
        "Question": "Which of the following argument is not supported by import-all-tables tool?",
        "Options": [
            "a) –class-name",
            "b) –package-name",
            "c) –database-name",
            "d) –table-name"
        ],
        "Answer": "Answer: a\nExplanation: You may, however, specify a package with –package-name in which all generated classes will be placed."
    },
    {
        "id": 503,
        "Question": "Apache Cassandra is a massively scalable open source _______ database.",
        "Options": [
            "a) SQL",
            "b) NoSQL",
            "c) NewSQL",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Cassandra is perfect for managing large amounts of data across multiple data centers and the cloud. "
    },
    {
        "id": 504,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Cassandra delivers continuous availability, linear scalability, and operational simplicity across many commodity servers",
            "b) Cassandra has a “masterless” architecture, meaning all nodes are the same",
            "c) Cassandra also provides customizable replication, storing redundant copies of data across nodes that participate in a Cassandra ring",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Cassandra provides automatic data distribution across all nodes that participate in a “ring” or database cluster."
    },
    {
        "id": 505,
        "Question": "Cassandra uses a protocol called _______ to discover location and state information.",
        "Options": [
            "a) gossip",
            "b) intergos",
            "c) goss",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Gossip is used for internode communication."
    },
    {
        "id": 506,
        "Question": "A __________ determines which data centers and racks nodes belong to it.",
        "Options": [
            "a) Client requests",
            "b) Snitch",
            "c) Partitioner",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Client read or write requests can be sent to any node in the cluster because all nodes in Cassandra are peers."
    },
    {
        "id": 507,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Cassandra supplies linear scalability, meaning that capacity may be easily added simply by adding new nodes online",
            "b) Cassandra 2.0 included major enhancements to CQL, security, and performance",
            "c) CQL for Cassandra 2.0.6 adds several important features including batching of conditional updates, static columns, and increased control over slicing of clustering columns",
            "d) None of the Mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Cassandra is a highly scalable, eventually consistent, distributed, structured key-value store."
    },
    {
        "id": 508,
        "Question": "User accounts may be altered and dropped using the __________ Query Language.",
        "Options": [
            "a) Hive",
            "b) Cassandra",
            "c) Sqoop",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Cassandra manages user accounts and access to the database cluster using passwords."
    },
    {
        "id": 509,
        "Question": "Authorization capabilities for Cassandra use the familiar _________ security paradigm to manage object permissions.",
        "Options": [
            "a) COMMIT",
            "b) GRANT",
            "c) ROLLBACK",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Once authenticated into a database cluster using either internal authentication, the next security issue to be tackled is permission management."
    },
    {
        "id": 510,
        "Question": "Client-to-node encryption protects data in flight from client machines to a database cluster using ___________",
        "Options": [
            "a) SSL",
            "b) SSH",
            "c) SSN",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Client-to-node encryption establishes a secure channel between the client and the coordinator node."
    },
    {
        "id": 511,
        "Question": "Using ___________ file means you don’t have to override the SSL_CERTFILE environmental variables every time.",
        "Options": [
            "a) qlshrc",
            "b) cqshrc",
            "c) cqlshrc",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: cqlsh is used with SSL encryption."
    },
    {
        "id": 512,
        "Question": "Internal authentication stores usernames and bcrypt-hashed passwords in the ____________ table.",
        "Options": [
            "a) system_auth.creds",
            "b) system_auth.credentials",
            "c) system.credentials",
            "d) sys_auth.credentials"
        ],
        "Answer": "Answer: b\nExplanation: PasswordAuthenticator is an IAuthenticator implementation that you can use to configure Cassandra for internal authentication out-of-the-box."
    },
    {
        "id": 513,
        "Question": "A _________ grants initial permissions, and subsequently a user may or may not be given the permission to grant/revoke permissions.",
        "Options": [
            "a) keyspace",
            "b) superuser",
            "c) sudouser",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Object permission management is based on internal authorization."
    },
    {
        "id": 514,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Cassandra accommodates expensive, consumer SSDs extremely well",
            "b) Cassandra re-writes or re-reads existing data, and never overwrites the rows in place",
            "c) Cassandra uses a storage structure similar to a Log-Structured Merge Tree",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: A log-structured engine that avoids overwrites and uses sequential IO to update data is essential for writing to hard disks (HDD) and solid-state disks (SSD)."
    },
    {
        "id": 515,
        "Question": "__________ is one of many possible IAuthorizer implementations and the one that stores permissions in the system_auth.permissions table to support all authorization-related CQL statements.",
        "Options": [
            "a) CassandraAuth",
            "b) CassandraAuthorizer",
            "c) CassAuthorizer",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Configuration consists mainly of changing the authorizer option in the cassandra.yaml to use the CassandraAuthorizer."
    },
    {
        "id": 516,
        "Question": "Cassandra creates a ___________ for each table, which allows you to symlink a table to a chosen physical drive or data volume.",
        "Options": [
            "a) directory",
            "b) subdirectory",
            "c) domain",
            "d) path"
        ],
        "Answer": "Answer: b\nExplanation: The new file name format includes the keyspace name to distinguish which keyspace and table the file contains when streaming or bulk loading data."
    },
    {
        "id": 517,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Cassandra provides fine-grained control of table storage on disk, writing tables to disk using separate table directories within each keyspace directory",
            "b) The hinted handoff feature and Cassandra conformance and conformance to the ACID",
            "c) Client utilities and application programming interfaces (APIs) for developing applications for data storage and retrieval are available",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The hinted handoff feature and Cassandra conformance and non-conformance to the ACID."
    },
    {
        "id": 518,
        "Question": "When ___________ contents exceed a configurable threshold, the memtable data, which includes indexes, is put in a queue to be flushed to disk.",
        "Options": [
            "a) subtable",
            "b) memtable",
            "c) intable",
            "d) memorytable"
        ],
        "Answer": "Answer: b\nExplanation: You can configure the length of the queue by changing memtable_flush_queue_size in the cassandra.yaml."
    },
    {
        "id": 519,
        "Question": "Data in the commit log is purged after its corresponding data in the memtable is flushed to an _________",
        "Options": [
            "a) SSHables",
            "b) SSTable",
            "c) Memtables",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: SSTables are immutable, not written to again after the memtable is flushed."
    },
    {
        "id": 520,
        "Question": "For each SSTable, Cassandra creates _________ index.",
        "Options": [
            "a) memory",
            "b) partition",
            "c) in memory",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Partition index is list of partition keys and the start position of rows in the data file (on disk).\n"
    },
    {
        "id": 521,
        "Question": "Cassandra marks data to be deleted using _________",
        "Options": [
            "a) tombstone",
            "b) combstone",
            "c) tenstone",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Cassandra also does not delete in place because the SSTable is immutable."
    },
    {
        "id": 522,
        "Question": "Tombstones exist for a configured time period defined by the _______ value set on the table.",
        "Options": [
            "a) gc_grace_minutes",
            "b) gc_grace_time",
            "c) gc_grace_seconds",
            "d) gc_grace_hours"
        ],
        "Answer": "Answer: c\nExplanation: During compaction, there is a temporary spike in disk space usage and disk I/O because the old and new SSTables co-exist."
    },
    {
        "id": 523,
        "Question": "_________ is a Cassandra feature that optimizes the cluster consistency process.",
        "Options": [
            "a) Hinted handon",
            "b) Hinted handoff",
            "c) Tombstone",
            "d) Hinted tomb"
        ],
        "Answer": "Answer: b\nExplanation: You can enable or disable hinted handoff in the cassandra.yaml file."
    },
    {
        "id": 524,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Cassandra does not immediately remove data marked for deletion from disk",
            "b) A deleted column can reappear if you do not run node repair routinely",
            "c) The deletion of marked data occurs during compaction",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Marking data with a tombstone signals Cassandra to retry sending a delete request to a replica that was down at the time of delete."
    },
    {
        "id": 525,
        "Question": "Cassandra searches the __________ to determine the approximate location on disk of the index entry.",
        "Options": [
            "a) partition record",
            "b) partition summary",
            "c) partition search",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: If the Bloom filter does not rule out the SSTable, Cassandra checks the partition key cache."
    },
    {
        "id": 526,
        "Question": "You configure sample frequency by changing the ________ property in the table definition.",
        "Options": [
            "a) index_time",
            "b) index_interval",
            "c) index_secs",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: By default, the partition summary is a sample of the partition index."
    },
    {
        "id": 527,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) A hint indicates that a write needs to be replayed to one or more unavailable nodes",
            "b) When the cluster cannot meet the consistency level specified by the client, Cassandra does store a hint",
            "c) By default, hints are saved for three hours after a replica fails because if the replica is down longer than that, it is likely permanently dead",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: When the cluster cannot meet the consistency level specified by the client, Cassandra does not store a hint. "
    },
    {
        "id": 528,
        "Question": "The compression offset map grows to ____ GB per terabyte compressed.",
        "Options": [
            "a) 1-3",
            "b) 10-16",
            "c) 20-22",
            "d) 0-1"
        ],
        "Answer": "Answer: a\nExplanation: The more you compress data, the greater number of compressed blocks you have and the larger the compression offset table."
    },
    {
        "id": 529,
        "Question": "The type of __________ strategy Cassandra performs on your data is configurable and can significantly affect read performance.",
        "Options": [
            "a) compression",
            "b) collection",
            "c) compaction",
            "d) decompression"
        ],
        "Answer": "Answer: c\nExplanation: Using the SizeTieredCompactionStrategy or DateTieredCompactionStrategy tends to cause data fragmentation when rows are frequently updated."
    },
    {
        "id": 530,
        "Question": "There are _________ types of read requests that a coordinator can send to a replica.",
        "Options": [
            "a) two",
            "b) three",
            "c) four",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The coordinator node contacts one replica node with a direct read request."
    },
    {
        "id": 531,
        "Question": "_________ can be configured per table for non-QUORUM consistency levels.",
        "Options": [
            "a) Read repair",
            "b) Read damage",
            "c) Write repair",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation:  If the replicas are inconsistent, the coordinator issues writes to the out-of-date replicas to update the row to the most recent values. This process is known as read repair."
    },
    {
        "id": 532,
        "Question": "If the table has been configured with the __________ property, the coordinator node for the read request will retry the request with another replica node.",
        "Options": [
            "a) rapid_retry",
            "b) speculative_retry",
            "c) speculative_rapid",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Rapid read protection allows Cassandra to still deliver read requests when the originally selected replica nodes are either down or taking too long to respond."
    },
    {
        "id": 533,
        "Question": "BatchEE projects aims to provide an _________ implementation. (aka JSR352)",
        "Options": [
            "a) JBat",
            "b) JBatch",
            "c) JBash",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: BatchEE provides a set of useful extensions for this specification."
    },
    {
        "id": 534,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Blur is a search platform capable of searching massive amounts of data in a cloud computing environment",
            "b) Calcite is a not a very good customizable engine for parsing",
            "c) Broklyn is a highly customizable engine for parsing",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Blur is an incubator developed by Doug Cutting."
    },
    {
        "id": 535,
        "Question": "_________  allows database-like access, and in particular a SQL interface.",
        "Options": [
            "a) JBatch",
            "b) Calcite",
            "c) Blur",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Calcite also provides advanced query optimization, for data not residing in a traditional database."
    },
    {
        "id": 536,
        "Question": "___________ is a toolkit/application for converting between and editing common office file formats.",
        "Options": [
            "a) Droids",
            "b) DataFu",
            "c) Corinthia",
            "d) Ignite"
        ],
        "Answer": "Answer: c\nExplanation: The toolkit is small, portable, and flexible, with minimal dependencies."
    },
    {
        "id": 537,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Droids aims to be an intelligent standalone robot framework that allows to create and extend existing droids",
            "b) HTrace is a tracing framework intended for use with distributed systems written in java",
            "c) DataFu provides a collection of Hadoop MapReduce jobs and functions in higher level languages",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: DataFu also provides Hadoop jobs for incremental data processing in MapReduce."
    },
    {
        "id": 538,
        "Question": "Ignite is a unified ______ data fabric providing high-performance, distributed in-memory data management.",
        "Options": [
            "a) Column",
            "b) In-Memory",
            "c) Row oriented",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Ignite can be used for various data sources and user applications."
    },
    {
        "id": 539,
        "Question": "_________ is a distributed and scalable OLAP engine built on Hadoop to support extremely large datasets.",
        "Options": [
            "a) Kylin",
            "b) Lens",
            "c) log4cxx2",
            "d) MRQL"
        ],
        "Answer": "Answer: a\nExplanation: MRQL is a query processing and optimization system for large-scale, distributed data analysis."
    },
    {
        "id": 540,
        "Question": "NiFi is a dataflow system based on the concepts of ________ programming.",
        "Options": [
            "a) structured",
            "b) relational",
            "c) set",
            "d) flow-based"
        ],
        "Answer": "Answer: d\nExplanation: NiFi is incubator made by Billie Rinaldi."
    },
    {
        "id": 541,
        "Question": "__________ is a columnar storage format for Hadoop.",
        "Options": [
            "a) Ranger",
            "b) Parquet",
            "c) REEF",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The Ranger project is a framework to enable, monitor and manage comprehensive data security across the Hadoop platform."
    },
    {
        "id": 542,
        "Question": "Ripple is a browser based mobile phone emulator designed to aid in the development of _______ based mobile applications.",
        "Options": [
            "a) Javascript",
            "b) Java",
            "c) C++",
            "d) HTML5"
        ],
        "Answer": "Answer: d\nExplanation: Ripple is a cross platform and cross runtime testing/debugging tool."
    },
    {
        "id": 543,
        "Question": "____________ is a query processing and optimization system for large-scale.",
        "Options": [
            "a) MRQL",
            "b) NiFi",
            "c) OpenAz",
            "d) ODF Toolkit"
        ],
        "Answer": "Answer: a\nExplanation: MRQL is built on top of Apache Hadoop, Hama, Spark, and Flink."
    },
    {
        "id": 544,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) SAMOA provides a collection of distributed streaming algorithms",
            "b) REEF is a cross platform and cross runtime testing/debugging tool",
            "c) Sentry is a highly modular system for providing fine grained role",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: SAMOA provides a collection of distributed streaming algorithms for the most common data mining and machine learning tasks such as classification, clustering, and regression."
    },
    {
        "id": 545,
        "Question": "________ is a columnar storage format for Hadoop.",
        "Options": [
            "a) MRQL",
            "b) NiFi",
            "c) OpenAz",
            "d) Parquet"
        ],
        "Answer": "Answer: d\nExplanation: NiFi is a dataflow system based on the concepts of flow-based programming."
    },
    {
        "id": 546,
        "Question": "REEF is a scale-out computing fabric that eases the development of Big Data applications are ___________",
        "Options": [
            "a) MRQL",
            "b) NiFi",
            "c) REEF",
            "d) Ripple"
        ],
        "Answer": "Answer: c\nExplanation: REEF stands for Retainable Evaluator Execution Framework."
    },
    {
        "id": 547,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) OpenAz is a browser based mobile phone emulator",
            "b) Ripple is a cross platform and cross runtime testing/debugging tool",
            "c) Ripple currently supports such runtimes as Cordova, WebWorks and the Mobile Web",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Ripple is a browser based mobile phone emulator designed to aid in the development of HTML5 based mobile applications."
    },
    {
        "id": 548,
        "Question": "Which of the following is a monitoring solution for hadoop?",
        "Options": [
            "a) Sirona",
            "b) Sentry",
            "c) Slider",
            "d) Streams"
        ],
        "Answer": "Answer: a\nExplanation: Slider is a collection of tools and technologies to package, deploy, and manage long running applications on Apache Hadoop YARN clusters."
    },
    {
        "id": 549,
        "Question": "Apache ________ is a lightweight server for ActivityStreams.",
        "Options": [
            "a) Sirona",
            "b) Taverna",
            "c) Slider",
            "d) Streams"
        ],
        "Answer": "Answer: d\nExplanation: Taverna is a domain-independent suite of tools used to design and execute data-driven workflows."
    },
    {
        "id": 550,
        "Question": "Which of the following provides extendible modern and functional API leveraging SE, ME and EE environments?",
        "Options": [
            "a) Sirona",
            "b) Taverna",
            "c) Tamaya",
            "d) Streams"
        ],
        "Answer": "Answer: c\nExplanation: Tamaya is a highly flexible configuration solution based on a modular, extensible and injectable key/value based design."
    },
    {
        "id": 551,
        "Question": "__________ is an abstraction over Apache Hadoop YARN that reduces the complexity of developing distributed applications.",
        "Options": [
            "a) Wave",
            "b) Twill",
            "c) Usergrid",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Twill allows developers to focus more on their business logic."
    },
    {
        "id": 552,
        "Question": "A _________ is a hosted, live, concurrent data structure for rich communication.",
        "Options": [
            "a) Wave",
            "b) Twill",
            "c) Usergrid",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Usergrid is Backend-as-a-Service (BaaS) composed of an integrated database (Cassandra), application layer and client tier with SDKs for developers."
    },
    {
        "id": 553,
        "Question": "Which of the following are a collaborative data analytics and visualization tool?",
        "Options": [
            "a) ACE",
            "b) Abdera",
            "c) Zeppelin",
            "d) Accumulo"
        ],
        "Answer": "Answer: c\nExplanation: Zeppelin is used for general-purpose data processing systems such as Apache Spark, Apache Flink, etc."
    },
    {
        "id": 554,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Buildr is a simple and intuitive build system for Java projects written in Ruby",
            "b) Celix is an OSGi like implementation in C with a distinct focus on interoperability between Java and C",
            "c) The Bean Validation project will create an implementation of Bean Validation as defined by the Java EE specifications",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Beehive provides extensible Java application framework with an integrated metadata-driven programming model for web services."
    },
    {
        "id": 555,
        "Question": "_____________ is a software distribution framework based on OSGi.",
        "Options": [
            "a) ACE",
            "b) Abdera",
            "c) Zeppelin",
            "d) Accumulo"
        ],
        "Answer": "Answer: a\nExplanation: ACE allows you to manage and distribute artifacts."
    },
    {
        "id": 556,
        "Question": "___________ forge software for the development of software projects.",
        "Options": [
            "a) Oozie",
            "b) Allura",
            "c) Ambari",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Projects include source control systems, issue tracking, discussion, wiki, and other software project management tools."
    },
    {
        "id": 557,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Ambari is a monitoring, administration and lifecycle management project for Apache Hadoop clusters",
            "b) The Amber project will deliver a Java development framework mainly aimed to build OAuth-aware applications",
            "c) Bigtop is a project for the development of packaging and tests of the Hadoop ecosystem",
            "d) All of the Mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Amber graduated with the name Apache Oltu."
    },
    {
        "id": 558,
        "Question": "___________ is a software development collaboration tool.",
        "Options": [
            "a) Buildr",
            "b) Cassandra",
            "c) Bloodhound",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Buildr is a simple and intuitive build system for Java projects written in Ruby."
    },
    {
        "id": 559,
        "Question": "_____________ is an IaaS (“Infrastracture as a Service”) cloud orchestration platform.",
        "Options": [
            "a) CloudStack",
            "b) Cazerra",
            "c) Click",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Click is a component based Java Web Framework."
    },
    {
        "id": 560,
        "Question": "Apache __________  is a platform for building native mobile applications using HTML, CSS and JavaScript (formerly Phonegap).",
        "Options": [
            "a) Cazerra",
            "b) Cordova",
            "c) CouchDB",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The project entered incubation as Callback, but decided to change its name to Cordova on 2011-11-28."
    },
    {
        "id": 561,
        "Question": "___________ is a Java library for writing, testing, and running pipelines of MapReduce jobs on Apache Hadoop.",
        "Options": [
            "a) cTakes",
            "b) Crunch",
            "c) CouchDB",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: cTAKES (clinical Text Analysis and Knowledge Extraction System) is a natural language processing tool for information extraction from electronic medical record clinical free-text."
    },
    {
        "id": 562,
        "Question": "Which of the following project will create an SOA services framework?",
        "Options": [
            "a) DeltaCloud",
            "b) CXF",
            "c) DeltaSpike",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: DeltaSpike is a collection of JSR-299 (CDI) Extensions for building applications on the Java SE and EE platforms."
    },
    {
        "id": 563,
        "Question": "________ includes a ﬂexible and powerful toolkit for displaying monitoring and analyzing results.",
        "Options": [
            "a) Imphala",
            "b) Chukwa",
            "c) BigTop",
            "d) Oozie"
        ],
        "Answer": "Answer: b\nExplanation: Chukwa is built on top of the Hadoop distributed filesystem (HDFS) and MapReduce framework and inherits Hadoop’s scalability and robustness."
    },
    {
        "id": 564,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Log processing was one of the original purposes of MapReduce",
            "b) Chukwa is a Hadoop subproject devoted to bridging that gap between logs processing and Hadoop ecosystem",
            "c) HICC stands for Hadoop Infrastructure Care Center",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Chukwa is a scalable distributed monitoring and analysis system, particularly logs from Hadoop and other large systems."
    },
    {
        "id": 565,
        "Question": "The items stored on _______ are organized in a hierarchy of widget category.",
        "Options": [
            "a) HICE",
            "b) HICC",
            "c) HIEC",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: HICC stands for Hadoop Infrastructure Care Center. It is the central dashboard for visualize and monitoring of metrics collected by Chukwa."
    },
    {
        "id": 566,
        "Question": "HICC, the Chukwa visualization interface, requires HBase version _____________",
        "Options": [
            "a) 0.90.5+.",
            "b) 0.10.4+.",
            "c) 0.90.4+.",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The Chukwa cluster management scripts rely on ssh; these scripts, however, are not required if you have some alternate mechanism for starting and stopping daemons."
    },
    {
        "id": 567,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Using Hadoop for MapReduce processing of logs is easy",
            "b) Chukwa should work on any POSIX platform",
            "c) Chukwa is a system for large-scale reliable log collection and processing with Hadoop",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Logs are generated incrementally across many machines, but Hadoop MapReduce works best on a small number of large files."
    },
    {
        "id": 568,
        "Question": "__________ are the Chukwa processes that actually produce data.",
        "Options": [
            "a) Collectors",
            "b) Agents",
            "c) HBase Table",
            "d) HCatalog"
        ],
        "Answer": "Answer: b\nExplanation: Setting the option chukwaAgent.control.remote will disallow remote connections to the agent control socket."
    },
    {
        "id": 569,
        "Question": "Chukwa ___________ are responsible for accepting incoming data from Agents, and storing the data.",
        "Options": [
            "a) HBase Table",
            "b) Agents",
            "c) Collectors",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation:  Most commonly, collectors simply write all received to HBase or HDFS."
    },
    {
        "id": 570,
        "Question": "For enabling streaming data to _________ chukwa collector writer class can be configured in chukwa-collector-conf.xml.",
        "Options": [
            "a) HCatalog",
            "b) HBase",
            "c) Hive",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: In this mode, the filesystem to write to is determined by the option writer.hdfs.filesystem in chukwa-collector-conf.xml."
    },
    {
        "id": 571,
        "Question": "By default, collector’s listen on port _________",
        "Options": [
            "a) 8008",
            "b) 8070",
            "c) 8080",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Port number can be configured in chukwa-collector.conf.xml"
    },
    {
        "id": 572,
        "Question": "_________ class allows other programs to get incoming chunks fed to them over a socket by the collector.",
        "Options": [
            "a) PipelineStageWriter",
            "b) PipelineWriter",
            "c) SocketTeeWriter",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: PipelineStageWriter lets you string together a series of PipelineableWriters for pre-processing or post-processing incoming data."
    },
    {
        "id": 573,
        "Question": "__________ runs Demux parsers inside for convert unstructured data to semi-structured data, then load the key value pairs to HBase table.",
        "Options": [
            "a) HCatWriter",
            "b) HBWriter",
            "c) HBaseWriter",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Demux parser class package, HBaseWriter uses hbase.demux.package to validate HBase for annotated demux parser classes."
    },
    {
        "id": 574,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) chukwa supports two different reliability strategies",
            "b) chukwaCollector.asyncAcks.scantime affects how often collectors will check the filesystem for commits",
            "c) chukwaCollector.asyncAcks.scanperiod defaults to thrice the rotation interval",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The first, default strategy, is as follows: collectors write data to HDFS, and as soon as the HDFS write call returns success, report success to the agent, which advances its checkpoint state."
    },
    {
        "id": 575,
        "Question": "The __________ streams chunks of data to HDFS, and write data in temp filename with .chukwa suffix.",
        "Options": [
            "a) LocalWriter",
            "b) SeqFileWriter",
            "c) SocketTeeWriter",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: When the file is completed writing, the filename is renamed with .done suffix. SeqFileWriter has the following configuration in chukwa-collector-conf.xml."
    },
    {
        "id": 576,
        "Question": "Conceptually, each _________ emits a semi-infinite stream of bytes, numbered starting from zero.",
        "Options": [
            "a) Collector",
            "b) Adaptor",
            "c) Compactor",
            "d) LocalWriter"
        ],
        "Answer": "Answer: b\nExplanation:  A Chunk is a sequence of bytes, with some metadata. Several of these are set automatically by the Agent or Adaptors."
    },
    {
        "id": 577,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Filters use the same syntax as the Dump command",
            "b) “RAW” will send the internal data of the Chunk, without any metadata, prefixed by its length encoded as a 32-bit int",
            "c) Specifying “WRITABLE” will cause the chunks to be written using Hadoop Writable serialization framework",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: “HEADER” is similar to “RAW”, but with a one-line header in front of the content."
    },
    {
        "id": 578,
        "Question": "The _____________ allows external processes to watch the stream of chunks passing through the collector.",
        "Options": [
            "a) LocalWriter",
            "b) SeqFileWriter",
            "c) SocketTeeWriter",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: SocketTeeWriter listens on a port (specified by conf option chukwaCollector.tee.port, defaulting to 9094.)"
    },
    {
        "id": 579,
        "Question": "Data analytics scripts are written in ____________",
        "Options": [
            "a) Hive",
            "b) CQL",
            "c) PigLatin",
            "d) Java"
        ],
        "Answer": "Answer: c\nExplanation: Data stored in HBase are aggregated by data analytic scripts to provide visualization and interpretation of health of Hadoop cluster."
    },
    {
        "id": 580,
        "Question": "If demux is successful within ____________ attempts, archives the completed files in Chukwa.",
        "Options": [
            "a) one",
            "b) two",
            "c) three",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The Demux MapReduce job is run on the data in demuxProcessing/mrInput."
    },
    {
        "id": 581,
        "Question": "Chukwa is ___________ data collection system for managing large distributed systems.",
        "Options": [
            "a) open source",
            "b) proprietary",
            "c) service based",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Chukwa is built on top of the Hadoop Distributed File System (HDFS) and Map/Reduce framework and inherits Hadoop scalability and robustness."
    },
    {
        "id": 582,
        "Question": "Collectors write chunks to logs/*.chukwa files until a __________ MB chunk is reached.",
        "Options": [
            "a) 64",
            "b) 108",
            "c) 256",
            "d) 1024"
        ],
        "Answer": "Answer: a\nExplanation: PostProcessManager wakes up every few minutes and aggregates, orders and de-dups record files."
    },
    {
        "id": 583,
        "Question": "___________ provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.",
        "Options": [
            "a) Oozie",
            "b) Ambari",
            "c) Hive",
            "d) Imphala"
        ],
        "Answer": "Answer: b\nExplanation: The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters."
    },
    {
        "id": 584,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Ambari provides a dashboard for monitoring the health and status of the Hadoop cluster",
            "b) Ambari provides a step-by-step wizard for installing Hadoop services across any number of hosts",
            "c) Ambari handles configuration of Hadoop services for the cluster",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Ambari provides central management for starting, stopping, and reconfiguring Hadoop services across the entire cluster."
    },
    {
        "id": 585,
        "Question": "Ambari leverages ________ for metrics collection.",
        "Options": [
            "a) Nagios",
            "b) Nagaond",
            "c) Ganglia",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Ganglia is a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids."
    },
    {
        "id": 586,
        "Question": "Ambari leverages ___________ for system alerting and will send emails when your attention is needed.",
        "Options": [
            "a) Nagios",
            "b) Nagaond",
            "c) Ganglia",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Nagios Is The Industry Standard In IT Infrastructure Monitoring."
    },
    {
        "id": 587,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Ambari Views framework was greatly improved to better support instantiating and loading custom views",
            "b) The Ambari shell is written is Java, and uses the Groovy bases Ambari REST client",
            "c) Ambari-Shell is distributed as a single-file executable jar",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: The uber jar is generated with the help of spring-boot-maven-plugin."
    },
    {
        "id": 588,
        "Question": "A ________ is a way of extending Ambari that allows 3rd parties to plug in new resource types along with the APIs.",
        "Options": [
            "a) trigger",
            "b) view",
            "c) schema",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: A view is an application that is deployed into the Ambari container."
    },
    {
        "id": 589,
        "Question": "Ambari ___________ deliver a template approach to cluster deployment.",
        "Options": [
            "a) View",
            "b) Stack Advisor",
            "c) Blueprints",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Ambari Blueprints deliver a template approach to cluster deployment."
    },
    {
        "id": 590,
        "Question": "___________ facilitates installation of Hadoop across any number of hosts.",
        "Options": [
            "a) API-driven installations",
            "b) Wizard-driven interface",
            "c) Extensible framework",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Extensible framework brings custom services under management via Ambari Stacks."
    },
    {
        "id": 591,
        "Question": "Ambari provides a ________  API that enables integration with existing tools, such as Microsoft System Center.",
        "Options": [
            "a) RestLess",
            "b) Web Service",
            "c) RESTful",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: RESTful APIs enables integration with enterprise systems."
    },
    {
        "id": 592,
        "Question": "If Ambari Agent has any output in /var/log/ambari-agent/ambari-agent.out, it is indicative of a __________ problem.",
        "Options": [
            "a) Less Severe",
            "b) Significant",
            "c) Extremely Severe",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Ambari enables system administrators to provision, manage and monitor a Hadoop cluster, and also to integrate Hadoop with the existing enterprise infrastructure."
    },
    {
        "id": 593,
        "Question": "A fully secure Hadoop cluster needs ___________",
        "Options": [
            "a) SSH",
            "b) SSL",
            "c) Kerberos",
            "d) REST"
        ],
        "Answer": "Answer: c\nExplanation: Kerberos requires a client side library and complex client side configuration."
    },
    {
        "id": 594,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Knox is a stateless reverse proxy framework",
            "b) Knox also intercepts REST/HTTP calls and provides authentication",
            "c) Knox scales linearly by adding more Knox nodes as the load increases",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Knox can be deployed as a cluster of Knox instances that route requests to Hadoop’s REST APIs."
    },
    {
        "id": 595,
        "Question": "A __________ can route requests to multiple Knox instances.",
        "Options": [
            "a) collector",
            "b) load balancer",
            "c) comparator",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Knox is a stateless reverse proxy framework."
    },
    {
        "id": 596,
        "Question": "Knox provides perimeter _________ for Hadoop clusters.",
        "Options": [
            "a) reliability",
            "b) security",
            "c) flexibility",
            "d) fault tolerant"
        ],
        "Answer": "Answer: b\nExplanation: Kerberos requires a client side library and complex client side configuration."
    },
    {
        "id": 597,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Knox eliminates the need for client software or client configuration and thus simplifies the access model",
            "b) Simplified access entend Hadoop’s REST/HTTP services by encapsulating Kerberos within the cluster",
            "c) Knox intercepts web vulnerability removal and other security services through a series of extensible interceptor pipelines",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Knox aggregates REST/HTTP calls to various components within the Hadoop ecosystem."
    },
    {
        "id": 598,
        "Question": "Knox integrates with prevalent identity management and _______ systems.",
        "Options": [
            "a) SSL",
            "b) SSO",
            "c) SSH",
            "d) Kerberos"
        ],
        "Answer": "Answer: b\nExplanation: Knox allows identities from those enterprise systems to be used for seamless, secure access to Hadoop clusters."
    },
    {
        "id": 599,
        "Question": "The easiest way to have an HDP cluster is to download the _____________",
        "Options": [
            "a) Hadoop",
            "b) Sandbox",
            "c) Dashboard",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The Apache Knox Gateway is a system that provides a single point of authentication and access for Apache™ Hadoop® services."
    },
    {
        "id": 600,
        "Question": "Apache Knox Eliminates _______ edge node risks.",
        "Options": [
            "a) SSL",
            "b) SSO",
            "c) SSH",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Knox hides Network Topology."
    },
    {
        "id": 601,
        "Question": "Apache Knox accesses Hadoop Cluster over _________",
        "Options": [
            "a) HTTP",
            "b) TCP",
            "c) ICMP",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Knox supports LDAP/AD Authentication, Service Authorization and Audit."
    },
    {
        "id": 602,
        "Question": "Apache Knox provides __________ REST API Access Point.",
        "Options": [
            "a) Single",
            "b) Double",
            "c) Multiple",
            "d) Zero"
        ],
        "Answer": "Answer: a\nExplanation: The Apache Knox Gateway is a system that provides a single point of authentication and access for Apache."
    },
    {
        "id": 603,
        "Question": "Apache Hadoop Development Tools is an effort undergoing incubation at _________",
        "Options": [
            "a) ADF",
            "b) ASF",
            "c) HCC",
            "d) AFS"
        ],
        "Answer": "Answer: b\nExplanation: The Apache Software Foundation(ASF) is sponsored by the Apache Incubator PMC."
    },
    {
        "id": 604,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) HDT tool allows you to allow working with only 1.1 version of Hadoop",
            "b) HDT tool allows you to allow working with multiple versions of Hadoop",
            "c) HDT tool allows you to allow working with multiple versions of Hadoop from multiple IDE",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: HDT project is currently a member of the Apache Incubator."
    },
    {
        "id": 605,
        "Question": "HDT project works with eclipse version ________ and above.",
        "Options": [
            "a) 3.4",
            "b) 3.5",
            "c) 3.6",
            "d) 3.7"
        ],
        "Answer": "Answer: c\nExplanation: The user should be able to install using a single update site for all Hadoop-related Eclipse tools."
    },
    {
        "id": 606,
        "Question": "HDT has been tested on __________ and Juno, and can work on Kepler as well.",
        "Options": [
            "a) Rainbow",
            "b) Indigo",
            "c) Indiavo",
            "d) Hadovo"
        ],
        "Answer": "Answer: b\nExplanation: HDT aims at bringing plugins in eclipse to simplify development on Hadoop platform."
    },
    {
        "id": 607,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) There is support for creating Hadoop project in HDT",
            "b) HDT aims at bringing plugins in eclipse to simplify development on Hadoop platform",
            "c) HDT is based on eclipse plugin architecture and can  possibly support other versions like 0.23, CDH4 etc in next releases",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: HDT aims to simplify the Hadoop platform for developers."
    },
    {
        "id": 608,
        "Question": "Which of the following tool is intended to be more compatible with HDT?",
        "Options": [
            "a) Git",
            "b) Juno",
            "c) Indigo",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The HDT uses a git repository, which anyone is free to checkout."
    },
    {
        "id": 609,
        "Question": "Which of the following has the core Eclipse PDE tools for HDT development?",
        "Options": [
            "a) RVP",
            "b) RAP",
            "c) RBP",
            "d) RVP"
        ],
        "Answer": "Answer: b\nExplanation: RCP/RAP developers package has the core Eclipse PDE tools."
    },
    {
        "id": 610,
        "Question": "HDT provides plugin for inspecting ________ nodes.",
        "Options": [
            "a) LocalWriter",
            "b) HICC",
            "c) HDFS",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The Hadoop Development Tools (HDT) is a set of plugins for the Eclipse IDE for developing against the Hadoop platform."
    },
    {
        "id": 611,
        "Question": "HDT is used for listing running Jobs on __________ Cluster.",
        "Options": [
            "a) MR",
            "b) Hive",
            "c) Pig",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: HDT can be used for launching Mapreduce programs on a Hadoop cluster."
    },
    {
        "id": 612,
        "Question": "HDT provides wizards for creating Java Classes for ___________",
        "Options": [
            "a) Mapper",
            "b) Reducer",
            "c) Driver",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: HDT provides wizards for the creation of Hadoop Based Projects."
    },
    {
        "id": 613,
        "Question": "Spark was initially started by ____________ at UC Berkeley AMPLab in 2009.",
        "Options": [
            "a) Mahek Zaharia",
            "b) Matei Zaharia",
            "c) Doug Cutting",
            "d) Stonebraker"
        ],
        "Answer": "Answer: b\nExplanation: Apache Spark is an open-source cluster computing framework originally developed in the AMPLab at UC Berkeley."
    },
    {
        "id": 614,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) RSS abstraction provides distributed task dispatching, scheduling, and basic I/O functionalities",
            "b) For cluster manager, Spark supports standalone Hadoop YARN",
            "c) Hive SQL is a component on top of Spark Core",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Spark requires a cluster manager and a distributed storage system."
    },
    {
        "id": 615,
        "Question": "____________ is a component on top of Spark Core.",
        "Options": [
            "a) Spark Streaming",
            "b) Spark SQL",
            "c) RDDs",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Spark SQL introduces a new data abstraction called SchemaRDD, which provides support for structured and semi-structured data. "
    },
    {
        "id": 616,
        "Question": "Spark SQL provides a domain-specific language to manipulate ___________ in Scala, Java, or Python.",
        "Options": [
            "a) Spark Streaming",
            "b) Spark SQL",
            "c) RDDs",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Spark SQL provides SQL language support, with command-line interfaces and ODBC/JDBC server."
    },
    {
        "id": 617,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) For distributed storage, Spark can interface with a wide variety, including Hadoop Distributed File System (HDFS)",
            "b) Spark also supports a pseudo-distributed mode, usually used only for development or testing purposes",
            "c) Spark has over 465 contributors in 2014",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Spark is the most active project in the Apache Software Foundation and among Big Data open source projects."
    },
    {
        "id": 618,
        "Question": "______________ leverages Spark Core fast scheduling capability to perform streaming analytics.",
        "Options": [
            "a) MLlib",
            "b) Spark Streaming",
            "c) GraphX",
            "d) RDDs"
        ],
        "Answer": "Answer: b\nExplanation: Spark Streaming ingests data in mini-batches and performs RDD transformations on those mini-batches of data."
    },
    {
        "id": 619,
        "Question": "____________ is a distributed machine learning framework on top of Spark.",
        "Options": [
            "a) MLlib",
            "b) Spark Streaming",
            "c) GraphX",
            "d) RDDs"
        ],
        "Answer": "Answer: a\nExplanation: MLlib implements many common machine learning and statistical algorithms to simplify large scale machine learning pipelines."
    },
    {
        "id": 620,
        "Question": "________ is a distributed graph processing framework on top of Spark.",
        "Options": [
            "a) MLlib",
            "b) Spark Streaming",
            "c) GraphX",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: GraphX started initially as a research project at UC Berkeley AMPLab and Databricks, and was later donated to the Spark project."
    },
    {
        "id": 621,
        "Question": "GraphX provides an API for expressing graph computation that can model the __________ abstraction.",
        "Options": [
            "a) GaAdt",
            "b) Spark Core",
            "c) Pregel",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: GraphX is used for machine learning."
    },
    {
        "id": 622,
        "Question": "Spark architecture is ___________ times as fast as Hadoop disk-based Apache Mahout and even scales better than Vowpal Wabbit.",
        "Options": [
            "a) 10",
            "b) 20",
            "c) 50",
            "d) 100"
        ],
        "Answer": "Answer: a\nExplanation: Spark architecture has proven scalability to over 8000 nodes in production."
    },
    {
        "id": 623,
        "Question": "Users can easily run Spark on top of Amazon’s __________",
        "Options": [
            "a) Infosphere",
            "b) EC2",
            "c) EMR",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Users can easily run Spark (and Shark) on top of Amazon’s EC2 either using the scripts that come with Spark."
    },
    {
        "id": 624,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Spark enables Apache Hive users to run their unmodified queries much faster",
            "b) Spark interoperates only with Hadoop",
            "c) Spark is a popular data warehouse solution running on top of Hadoop",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Shark can accelerate Hive queries by as much as 100x when the input data fits into memory, and up 10x when the input data is stored on disk."
    },
    {
        "id": 625,
        "Question": "Spark runs on top of ___________ a cluster manager system which provides efficient resource isolation across distributed applications.",
        "Options": [
            "a) Mesjs",
            "b) Mesos",
            "c) Mesus",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation:  Mesos enables fine grained sharing which allows a Spark job to dynamically take advantage of the idle resources in the cluster during its execution."
    },
    {
        "id": 626,
        "Question": "Which of the following can be used to launch Spark jobs inside MapReduce?",
        "Options": [
            "a) SIM",
            "b) SIMR",
            "c) SIR",
            "d) RIS"
        ],
        "Answer": "Answer: b\nExplanation: With SIMR, users can start experimenting with Spark and use its shell within a couple of minutes after downloading it."
    },
    {
        "id": 627,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Spark is intended to replace, the Hadoop stack",
            "b) Spark was designed to read and write data from and to HDFS, as well as other storage systems",
            "c) Hadoop users who have already deployed or are planning to deploy Hadoop Yarn can simply run Spark on YARN",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Spark is intended to enhance, not replace, the Hadoop stack."
    },
    {
        "id": 628,
        "Question": "Which of the following language is not supported by Spark?",
        "Options": [
            "a) Java",
            "b) Pascal",
            "c) Scala",
            "d) Python"
        ],
        "Answer": "Answer: b\nExplanation: The Spark engine runs in a variety of environments, from cloud services to Hadoop or Mesos clusters."
    },
    {
        "id": 629,
        "Question": "Spark is packaged with higher level libraries, including support for _________ queries.",
        "Options": [
            "a) SQL",
            "b) C",
            "c) C++",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Standard libraries increase developer productivity and can be seamlessly combined to create complex workflows."
    },
    {
        "id": 630,
        "Question": "Spark includes a collection over ________ operators for transforming data and familiar data frame APIs for manipulating semi-structured data.",
        "Options": [
            "a) 50",
            "b) 60",
            "c) 70",
            "d) 80"
        ],
        "Answer": "Answer: d\nExplanation: Spark provides easy-to-use APIs for operating on large datasets."
    },
    {
        "id": 631,
        "Question": "Spark is engineered from the bottom-up for performance, running ___________ faster than Hadoop by exploiting in memory computing and other optimizations.",
        "Options": [
            "a) 100x",
            "b) 150x",
            "c) 200x",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Spark is fast on disk too; it currently holds the world record in large scale on-disk sorting."
    },
    {
        "id": 632,
        "Question": "Spark powers a stack of high-level tools including Spark SQL, MLlib for _________",
        "Options": [
            "a) regression models",
            "b) statistics",
            "c) machine learning",
            "d) reproductive research"
        ],
        "Answer": "Answer: c\nExplanation: Spark is used at a wide range of organizations to process large datasets."
    },
    {
        "id": 633,
        "Question": "Apache Flume 1.3.0 is the fourth release under the auspices of Apache of the so-called ________ codeline.",
        "Options": [
            "a) NG",
            "b) ND",
            "c) NF",
            "d) NR"
        ],
        "Answer": "Answer: a\nExplanation: Flume 1.3.0 has been put through many stress and regression tests, is stable, production-ready software, and is backwards-compatible with Flume 1.2.0."
    },
    {
        "id": 634,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Flume is a distributed, reliable, and available service",
            "b) Version 1.5.2 is the eighth Flume release as an Apache top-level project",
            "c) Flume 1.5.2 is production-ready software for integration with hadoop",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Flume is used for efficiently collecting, aggregating, and moving large amounts of streaming event data."
    },
    {
        "id": 635,
        "Question": "___________ was created to allow you to flow data from a source into your Hadoop environment.",
        "Options": [
            "a) Imphala",
            "b) Oozie",
            "c) Flume",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: In Flume, the entities you work with are called sources, decorators, and sinks. "
    },
    {
        "id": 636,
        "Question": "A ____________ is an operation on the stream that can transform the stream.",
        "Options": [
            "a) Decorator",
            "b) Source",
            "c) Sinks",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: A source can be any data source, and Flume has many predefined source adapters. "
    },
    {
        "id": 637,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Version 1.4.0 is the fourth Flume release as an Apache top-level project",
            "b) Apache Flume 1.5.2 is a security and maintenance release that disables SSLv3 on all components in Flume that support SSL/TLS",
            "c) Flume is backwards-compatible with previous versions of the Flume 1.x codeline",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Apache Flume 1.3.1 is a maintenance release for the 1.3.0 release, and includes several bug fixes and performance enhancements."
    },
    {
        "id": 638,
        "Question": "A number of ____________ source adapters give you the granular control to grab a specific file.",
        "Options": [
            "a) multimedia file",
            "b) text file",
            "c) image file",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: A number of predefined source adapters are built into Flume."
    },
    {
        "id": 639,
        "Question": "____________  is used when you want the sink to be the input source for another operation.",
        "Options": [
            "a) Collector Tier Event",
            "b) Agent Tier Event",
            "c) Basic",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: All agents in a specific tier could be given the same name; One configuration file with … Clients send Events to Agents; Agents hosts number Flume components."
    },
    {
        "id": 640,
        "Question": "___________ is where you would land a flow (or possibly multiple flows joined together) into an HDFS-formatted file system.",
        "Options": [
            "a) Collector Tier Event",
            "b) Agent Tier Event",
            "c) Basic",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: A number of other predefined source adapters, as well as a command exit, allow you to use any executable command to feed the flow of data."
    },
    {
        "id": 641,
        "Question": "____________ sink can be a text file, the console display, a simple HDFS path, or a null bucket where the data is simply deleted.",
        "Options": [
            "a) Collector Tier Event",
            "b) Agent Tier Event",
            "c) Basic",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Flume will also ensure the integrity of the flow by sending back acknowledgments that data has actually arrived at the sink."
    },
    {
        "id": 642,
        "Question": "Flume deploys as one or more agents, each contained within its own instance of _________",
        "Options": [
            "a) JVM",
            "b) Channels",
            "c) Chunks",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: An agent must have at least one of each in order to run."
    },
    {
        "id": 643,
        "Question": "___________ provides Java-based indexing and search technology.",
        "Options": [
            "a) Solr",
            "b) Lucene Core",
            "c) Lucy",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Lucene provides spellchecking, hit highlighting and advanced analysis/tokenization capabilities."
    },
    {
        "id": 644,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Building PyLucene requires GNU Make, a recent version of Ant capable of building Java Lucene and a C++ compiler",
            "b) PyLucene is supported on Mac OS X, Linux, Solaris and Windows",
            "c) Use of setuptools is recommended for Lucene",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: PyLucene requires Python version 2.x (x >= 3.5) and Java version 1.x (x &t;= 5)."
    },
    {
        "id": 645,
        "Question": "___________ is a high performance search server built using Lucene Core.",
        "Options": [
            "a) Solr",
            "b) Lucene Core",
            "c) Lucy",
            "d) PyLucene"
        ],
        "Answer": "Answer: a\nExplanation: Solr provides hit highlighting, faceted search, caching, replication, and a web admin interface."
    },
    {
        "id": 646,
        "Question": "____________ is a subproject with the aim of collecting and distributing free materials.",
        "Options": [
            "a) OSR",
            "b) OPR",
            "c) ORP",
            "d) ORS"
        ],
        "Answer": "Answer: c\nExplanation: Open Relevance Project is used for relevance testing and performance."
    },
    {
        "id": 647,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) PyLucene is a Lucene port",
            "b) PyLucene embeds a Java VM with Lucene into a Python process",
            "c) The PyLucene Python extension, a Python module called lucene is machine-generated by JCC",
            "d) PyLucene is built with JCC"
        ],
        "Answer": "Answer: a\nExplanation: PyLucene is not a Lucene port but a Python wrapper around Java Lucene."
    },
    {
        "id": 648,
        "Question": "_______ is a Python port of the Core project.",
        "Options": [
            "a) Solr",
            "b) Lucene Core",
            "c) Lucy",
            "d) PyLucene"
        ],
        "Answer": "Answer: d\nExplanation: PyLucene is a Python extension for accessing Java LuceneTM."
    },
    {
        "id": 649,
        "Question": "The Lucene _________ is pleased to announce the availability of Apache Lucene 5.0.0 and Apache Solr 5.0.0.",
        "Options": [
            "a) PMC",
            "b) RPC",
            "c) CPM",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: PyLucene was previously hosted at the Open Source Applications Foundation."
    },
    {
        "id": 650,
        "Question": "___________ is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",
        "Options": [
            "a) Lucene",
            "b) Oozie",
            "c) Lucy",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java."
    },
    {
        "id": 651,
        "Question": "Lucene provides scalable, high-Performance indexing over ______  per hour on modern hardware.",
        "Options": [
            "a) 1 TB",
            "b) 150GB",
            "c) 10 GB",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Lucene offers powerful features through a simple API."
    },
    {
        "id": 652,
        "Question": "Lucene index size is roughly _______ the size of text indexed.",
        "Options": [
            "a) 10%",
            "b) 20%",
            "c) 50%",
            "d) 70%"
        ],
        "Answer": "Answer: b\nExplanation: Lucene provides incremental indexing as fast as batch indexing."
    },
    {
        "id": 653,
        "Question": "All file access uses Java’s __________ APIs which give Lucene stronger index safety.",
        "Options": [
            "a) NIO.2",
            "b) NIO.3",
            "c) NIO.4",
            "d) NIO.5"
        ],
        "Answer": "Answer: a\nExplanation: Index safety is provided in terms of better error handling and safer commits."
    },
    {
        "id": 654,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Every Lucene segment now stores a unique id per-segment and per-commit to aid in accurate replication of index files",
            "b) The default norms format now uses sparse encoding when appropriate",
            "c) Tokenizers and Analyzers no longer require Reader on init",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: NormsFormat now gets its own dedicated NormsConsumer/Producer."
    },
    {
        "id": 655,
        "Question": "During merging, __________ now always checks the incoming segments for corruption before merging.",
        "Options": [
            "a) LocalWriter",
            "b) IndexWriter",
            "c) ReadWriter",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Lucene supports random-writable and advance-able sparse bitsets."
    },
    {
        "id": 656,
        "Question": "Heap usage during IndexWriter merging is also much lower with the new _________",
        "Options": [
            "a) LucCodec",
            "b) Lucene50Codec",
            "c) Lucene20Cod",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Doc values and norms for the segments being merged are no longer fully loaded into heap for all fields"
    },
    {
        "id": 657,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) ConcurScheduler detects whether the index is on SSD or not",
            "b) Memory index supports payloads",
            "c) Auto-IO-throttling has been added to ConcurrentMergeScheduler, to rate limit IO writes for each merge depending on incoming merge rate",
            "d) The default codec has an option to control BEST_SPEED or BEST_COMPRESSION for stored fields"
        ],
        "Answer": "Answer: a\nExplanation: ConcurrentMergeScheduler does a better job defaulting its settings."
    },
    {
        "id": 658,
        "Question": "PostingsFormat now uses a __________ API when writing postings, just like doc values.",
        "Options": [
            "a) push",
            "b) pull",
            "c) read",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: This is powerful because you can do things in your postings format that require making more than one pass through the postings such as iterating over all postings."
    },
    {
        "id": 659,
        "Question": "New ____________ type enables Indexing and searching of date ranges, particularly multi-valued ones.",
        "Options": [
            "a) RangeField",
            "b) DateField",
            "c) DateRangeField",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: A new ExitableDirectoryReader extends FilterDirectoryReader and enables exiting requests that take too long to enumerate over terms."
    },
    {
        "id": 660,
        "Question": "SolrJ now has first class support for __________ API.",
        "Options": [
            "a) Compactions",
            "b) Collections",
            "c) Distribution",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Solr is the popular, blazing-fast, open source enterprise search platform built on Apache Lucene."
    },
    {
        "id": 661,
        "Question": "____________ Collection API  allows for even distribution of custom replica properties.",
        "Options": [
            "a) BALANUNIQUE",
            "b) BALANCESHARDUNIQUE",
            "c) BALANCEUNIQUE",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Solr powers the search and navigation features of many of the world’s largest internet sites."
    },
    {
        "id": 662,
        "Question": "____________ can be used to generate stats over the results of arbitrary numeric functions.",
        "Options": [
            "a) stats.field",
            "b) sta.field",
            "c) stats.value",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: stats.field allows for requesting for statistics for pivot facets using tags."
    },
    {
        "id": 663,
        "Question": "How many types of modes are present in Hama?",
        "Options": [
            "a) 2",
            "b) 3",
            "c) 4",
            "d) 5"
        ],
        "Answer": "Answer: b\nExplanation: Just like Hadoop, Hama has distinct between three modes."
    },
    {
        "id": 664,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) In local mode, nothing must be launched via the start scripts",
            "b) Distributed Mode is just like the “Pseudo Distributed Mode”",
            "c) Apache Hama is one of the under-hyped projects in the Hadoop ecosystem",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: You can adjust the number of threads used in this utility by setting the bsp.local.tasks.maximum property."
    },
    {
        "id": 665,
        "Question": "__________ is the default mode if you download Hama.",
        "Options": [
            "a) Local Mode",
            "b) Pseudo Distributed Mode",
            "c) Distributed Mode",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: This mode can be configured via the bsp.master.address property to local."
    },
    {
        "id": 666,
        "Question": "_________ mode is used when you just have a single server and want to launch all the daemon processes.",
        "Options": [
            "a) Local Mode",
            "b) Pseudo Distributed Mode",
            "c) Distributed Mode",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Pseudo Distributed Mode can be configured when you set the bsp.master.address to a host address."
    },
    {
        "id": 667,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Apache Hama is not a pure Bulk Synchronous Parallel Engine",
            "b) Hama uses the Hadoop Core for RPC calls",
            "c) Apache Hama is optimized for massive scientific computations such as matrix, graph and network algorithms",
            "d) Hama is a relatively newer project than Hadoop"
        ],
        "Answer": "Answer: a\nExplanation: Apache Hama is not a pure BSP."
    },
    {
        "id": 668,
        "Question": "Distributed Mode are mapped in the __________ file.",
        "Options": [
            "a) groomservers",
            "b) grervers",
            "c) grsvers",
            "d) groom"
        ],
        "Answer": "Answer: a\nExplanation: Distributed Mode is used when you have multiple machines."
    },
    {
        "id": 669,
        "Question": "The web UI provides information about ________ job statistics of the Hama cluster.",
        "Options": [
            "a) MPP",
            "b) BSP",
            "c) USP",
            "d) ISP"
        ],
        "Answer": "Answer: b\nExplanation: Running/completed/Failed jobs is detailed in UI interface."
    },
    {
        "id": 670,
        "Question": "Apache Hama provides complete clone of _________",
        "Options": [
            "a) Pragmatic",
            "b) Pregel",
            "c) ServePreg",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Pregel is used for large processing of graphs."
    },
    {
        "id": 671,
        "Question": "A __________ in a social graph is a group of people who interact frequently with each other and less frequently with others.",
        "Options": [
            "a) semi-cluster",
            "b) partial cluster",
            "c) full cluster",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: semi-cluster is different from ordinary clustering in the sense that a vertex may belong to more than one semi-cluster."
    },
    {
        "id": 672,
        "Question": "Which of the following apache project is gaining a lot of traction steadily with the efforts of its committers?",
        "Options": [
            "a) Hama",
            "b) Hadoop",
            "c) Hive",
            "d) Pig"
        ],
        "Answer": "Answer: a\nExplanation: HAMA is a distributed framework on Hadoop for massive matrix algorithms."
    },
    {
        "id": 673,
        "Question": "Hama is a general ________________ computing engine on top of Hadoop.",
        "Options": [
            "a) BSP",
            "b) ASP",
            "c) MPP",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Hama provides high-Performance computing engine for performing massive scientific and iterative algorithms on existing open source or enterprise Hadoop cluster."
    },
    {
        "id": 674,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Apache Hama is a distributed computing framework based on Bulk Synchronous Parallel computing techniques for massive scientific computations",
            "b) Hama is a Top Level Project under the Apache Software Foundation",
            "c) BSP stands for Bulk Synchronous Parallel",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Hama is open source project for many OS."
    },
    {
        "id": 675,
        "Question": "Hama was inspired by Google’s _________ large-scale graph computing framework.",
        "Options": [
            "a) Pragmatic",
            "b) Pregel",
            "c) Preghad",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Hama is a distributed computing model similar to MR (MapReduce)."
    },
    {
        "id": 676,
        "Question": "Hama requires JRE _______ or higher and ssh to be set up between nodes in the cluster.",
        "Options": [
            "a) 1.6",
            "b) 1.7",
            "c) 1.8",
            "d) 2.0"
        ],
        "Answer": "Answer: a\nExplanation: Apache Hama releases are available under the Apache License, Version 2.0."
    },
    {
        "id": 677,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The major difference between Hadoop and Hama is map/reduce tasks can’t communicate with each other",
            "b) Hama follows master/slave pattern",
            "c) A JobTracker maps to a BSPMaster, TaskTracker maps to a GroomServer and Map/Reduce task maps to a BSPTask",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: BSPTask can communicate to each other."
    },
    {
        "id": 678,
        "Question": "Hama consist of mainly ________ components for large scale processing of graphs.",
        "Options": [
            "a) two",
            "b) three",
            "c) four",
            "d) five"
        ],
        "Answer": "Answer: b\nExplanation: Hama consists of three major components: BSPMaster, GroomServers and Zookeeper."
    },
    {
        "id": 679,
        "Question": "________ is responsible for maintaining groom server status.",
        "Options": [
            "a) GroomServers",
            "b) BSPMaster",
            "c) Zookeeper",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: A BSP Master and multiple grooms are started by the script."
    },
    {
        "id": 680,
        "Question": "A __________ server and a data node should be run on one physical node.",
        "Options": [
            "a) groom",
            "b) web",
            "c) client",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Each groom is designed to run with HDFS or other distributed storage."
    },
    {
        "id": 681,
        "Question": "A ________ is used to manage the efficient barrier synchronization of the BSPPeers.",
        "Options": [
            "a) GroomServers",
            "b) BSPMaster",
            "c) Zookeeper",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: A Groom Server is a process that performs bsp tasks assigned by BSPMaster."
    },
    {
        "id": 682,
        "Question": "Groom servers starts up with a ________ instance and an RPC proxy to contact the bsp master.",
        "Options": [
            "a) RPC",
            "b) BSPPeer",
            "c) LPC",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Each groom periodically sends a heartbeat message that encloses its groom server status, including maximum task capacity, unused memory, and so on."
    },
    {
        "id": 683,
        "Question": "HCatalog supports reading and writing files in any format for which a ________ can be written.",
        "Options": [
            "a) SerDE",
            "b) SaerDear",
            "c) DocSear",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: By default, HCatalog supports RCFile, CSV, JSON, and SequenceFile, and ORC file formats. To use a custom format, you must provide the InputFormat, OutputFormat, and SerDe."
    },
    {
        "id": 684,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) HCat provides connectors for MapReduce",
            "b) Apache HCatalog provides table data access for CDH components such as Pig and MapReduce",
            "c) HCat makes Hive metadata available to users of other Hadoop tools like Pig, MapReduce and Hive",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Table definitions are maintained in the Hive metastore."
    },
    {
        "id": 685,
        "Question": "Hive version ___________ is the first release that includes HCatalog.",
        "Options": [
            "a) 0.10.0",
            "b) 0.11.0",
            "c) 0.12.0",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: HCatalog graduated from the Apache incubator and merged with the Hive project on March 26, 2013."
    },
    {
        "id": 686,
        "Question": "HCatalog is built on top of the Hive metastore and incorporates Hive’s is ____________",
        "Options": [
            "a) DDL",
            "b) DML",
            "c) TCL",
            "d) DCL"
        ],
        "Answer": "Answer: a\nExplanation: HCatalog provides read and write interfaces for Pig and MapReduce and uses Hive’s command line interface for issuing data definition and metadata exploration commands."
    },
    {
        "id": 687,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) HCatalog is a table and storage management layer for Hadoop that enables users with different data processing tools",
            "b) There is Hive-specific interface for HCatalog",
            "c) Data is defined using HCatalog’s command line interface (CLI)",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Since HCatalog uses Hive’s metastore, Hive can read data in HCatalog directly."
    },
    {
        "id": 688,
        "Question": "The HCatalog interface for Pig consists of ____________ and HCatStorer, which implement the Pig load and store interfaces respectively.",
        "Options": [
            "a) HCLoader",
            "b) HCatLoader",
            "c) HCatLoad",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: HCatLoader accepts a table to read data from; you can indicate which partitions to scan by immediately following the load statement with a partition filter statement."
    },
    {
        "id": 689,
        "Question": "_____________ accepts a table to read data from and optionally a selection predicate to indicate which partitions to scan.",
        "Options": [
            "a) HCatOutputFormat",
            "b) HCatInputFormat",
            "c) OutputFormat",
            "d) InputFormat"
        ],
        "Answer": "Answer: b\nExplanation: The HCatalog interface for MapReduce — HCatInputFormat and HCatOutputFormat — is an implementation of Hadoop InputFormat and OutputFormat."
    },
    {
        "id": 690,
        "Question": "The HCatalog __________ supports all Hive DDL that does not require MapReduce to execute.",
        "Options": [
            "a) Powershell",
            "b) CLI",
            "c) CMD",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Data is defined using HCatalog’s command line interface (CLI)."
    },
    {
        "id": 691,
        "Question": "You can write to a single partition by specifying the partition key(s) and value(s) in the ___________ method.",
        "Options": [
            "a) setOutput",
            "b) setOut",
            "c) put",
            "d) get"
        ],
        "Answer": "Answer: a\nExplanation: You can write to multiple partitions if the partition key(s) are columns in the data being stored."
    },
    {
        "id": 692,
        "Question": "HCatalog supports the same data types as _________",
        "Options": [
            "a) Pig",
            "b) Hama",
            "c) Hive",
            "d) Oozie"
        ],
        "Answer": "Answer: c\nExplanation: Partitions are multi-dimensional and not hierarchical. Records are divided into columns."
    },
    {
        "id": 693,
        "Question": "__________ is a REST API for HCatalog.",
        "Options": [
            "a) WebHCat",
            "b) WbHCat",
            "c) InpHCat",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: REST stands for “representational state transfer”, a style of API based on HTTP verbs."
    },
    {
        "id": 694,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) There is no guaranteed read consistency when a partition is dropped",
            "b) Unpartitioned tables effectively have one default partition that must be created at table creation time",
            "c) Once a partition is created, records cannot be added to it, removed from it, or updated in it",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Partitioned tables have no partitions at create time."
    },
    {
        "id": 695,
        "Question": "With HCatalog _________ does not need to modify the table structure.",
        "Options": [
            "a) Partition",
            "b) Columns",
            "c) Robert",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Without HCatalog, Robert must alter the table to add the required partition."
    },
    {
        "id": 696,
        "Question": "Sally in data processing uses __________ to cleanse and prepare the data.",
        "Options": [
            "a) Pig",
            "b) Hive",
            "c) HCatalog",
            "d) Impala"
        ],
        "Answer": "Answer: a\nExplanation: Without HCatalog, Sally must be manually informed by Joe when data is available, or poll on HDFS."
    },
    {
        "id": 697,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The original name of WebHCat was Templeton",
            "b) Robert in client management uses Hive to analyze his clients’ results",
            "c) With HCatalog, HCatalog cannot send a JMS message that data is available",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The Pig job can then be restarted after analyzing client."
    },
    {
        "id": 698,
        "Question": "For ___________ partitioning jobs, simply specifying a custom directory is not good enough.",
        "Options": [
            "a) static",
            "b) semi cluster",
            "c) dynamic",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Since it writes to multiple destinations, and thus, instead of a directory specification, it requires a pattern specification."
    },
    {
        "id": 699,
        "Question": "___________  property allows us to specify a custom dir location pattern for all the writes, and will interpolate each variable.",
        "Options": [
            "a) hcat.dynamic.partitioning.custom.pattern",
            "b) hcat.append.limit",
            "c) hcat.pig.storer.external.location",
            "d) hcatalog.hive.client.cache.expiry.time"
        ],
        "Answer": "Answer: a\nExplanation: hcat.append.limit allows an HCatalog user to specify a custom append limit."
    },
    {
        "id": 700,
        "Question": "HCatalog maintains a cache of _________ to talk to the metastore.",
        "Options": [
            "a) HiveServer",
            "b) HiveClients",
            "c) HCatClients",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: HCatalog manages a cache of 1 metastore client per thread, defaulting to an expiry of 120 seconds."
    },
    {
        "id": 701,
        "Question": "On the write side, it is expected that the user pass in valid _________ with data correctly.",
        "Options": [
            "a) HRecords",
            "b) HCatRecos",
            "c) HCatRecords",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: In some cases where a user of HCat (such as some older versions of pig) does not support all the datatypes supported by hive, there are a few config parameters provided to handle data promotions/conversions to allow them to read data through HCatalog."
    },
    {
        "id": 702,
        "Question": "A float parameter, defaults to 0.0001f, which means we can deal with 1 error every __________ rows.",
        "Options": [
            "a) 1000",
            "b) 10000",
            "c) 1 million rows",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: hcat.input.bad.record.threshold property is throw out error on encountering bad record."
    },
    {
        "id": 703,
        "Question": "_________________ property allow users to override the expiry time specified.",
        "Options": [
            "a) hcat.desired.partition.num.splits",
            "b) hcatalog.hive.client.cache.expiry.time",
            "c) hcatalog.hive.client.cache.disabled",
            "d) hcat.append.limit"
        ],
        "Answer": "Answer: b\nExplanation: This property is an int, and specifies number of seconds."
    },
    {
        "id": 704,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The HCatLoader and HCatStorer interfaces are used with Pig scripts to read and write data in HCatalog-managed tables",
            "b) HCatalog is not thread safe",
            "c) HCatLoader is used with Pig scripts to read data from HCatalog-managed tables.",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: HCatLoader is accessed via a Pig load statement."
    },
    {
        "id": 705,
        "Question": "____________ is used with Pig scripts to write data to HCatalog-managed tables.",
        "Options": [
            "a) HamaStorer",
            "b) HCatStam",
            "c) HCatStorer",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: HCatStorer is accessed via a Pig store statement."
    },
    {
        "id": 706,
        "Question": "Hive does not have a data type corresponding to the ____________ type in Pig.",
        "Options": [
            "a) decimal",
            "b) short",
            "c) biginteger",
            "d) datetime"
        ],
        "Answer": "Answer: c\nExplanation: Hive 0.12.0 and earlier releases support writing Pig primitive data types with HCatStorer."
    },
    {
        "id": 707,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The Hive metastore lets you create tables without specifying a database",
            "b) Restrictions apply to the types of columns HCatLoader can read from HCatalog-managed tables",
            "c) If the table is partitioned, you can indicate which partitions to scan by immediately following the load statement with a partition filter statement",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: If you created tables using metastore, then the database name is ‘default’ and is not required when specifying the table for HCatLoader."
    },
    {
        "id": 708,
        "Question": "_______________ method is used to include a projection schema, to specify the output fields.",
        "Options": [
            "a) OutputSchema",
            "b) setOut",
            "c) setOutputSchema",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: If a schema is not specified, all the columns in the table will be returned."
    },
    {
        "id": 709,
        "Question": "The first call on the HCatOutputFormat must be ____________",
        "Options": [
            "a) setOutputSchema",
            "b) setOutput",
            "c) setOut",
            "d) OutputSchema"
        ],
        "Answer": "Answer: b\nExplanation: Any other call will throw an exception saying the output format is not initialized."
    },
    {
        "id": 710,
        "Question": "___________ is the type supported for storing values in HCatalog tables.",
        "Options": [
            "a) HCatRecord",
            "b) HCatColumns",
            "c) HCatValues",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The types in an HCatalog table schema determine the types of objects returned for different fields in HCatRecord. "
    },
    {
        "id": 711,
        "Question": "The output descriptor for the table to be written is created by calling ____________",
        "Options": [
            "a) OutputJobInfo.describe",
            "b) OutputJobInfo.create",
            "c) OutputJobInfo.put",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The implementation of Map takes HCatRecord as an input and the implementation of Reduce produces it as an output."
    },
    {
        "id": 712,
        "Question": "Which of the following Hive commands is not supported by HCatalog?",
        "Options": [
            "a) ALTER INDEX … REBUILD",
            "b) CREATE VIEW",
            "c) SHOW FUNCTIONS",
            "d) DROP TABLE"
        ],
        "Answer": "Answer: a\nExplanation: Any command which is not supported throws an exception with the message “Operation Not Supported”."
    },
    {
        "id": 713,
        "Question": "Mahout provides ____________ libraries for common  and primitive Java collections.",
        "Options": [
            "a) Java",
            "b) Javascript",
            "c) Perl",
            "d) Python"
        ],
        "Answer": "Answer: a\nExplanation: Maths operations are focused on linear algebra and statistics."
    },
    {
        "id": 714,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Mahout is distributed under a commercially friendly Apache Software license",
            "b) Mahout is a library of scalable machine-learning algorithms, implemented on top of Apache Hadoop® and using the MapReduce paradigm",
            "c) Apache Mahout is a project of the Apache Software Foundation to produce free implementations of distributed or otherwise scalable machine learning algorithms",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: The goal of Mahout is to build a vibrant, responsive, diverse community to facilitate discussions not only on the project itself but also on potential use cases."
    },
    {
        "id": 715,
        "Question": "_________ does not restrict contributions to Hadoop based implementations.",
        "Options": [
            "a) Mahout",
            "b) Oozie",
            "c) Impala",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Mahout is distributed under a commercially friendly Apache Software license."
    },
    {
        "id": 716,
        "Question": "Mahout provides an implementation of a ______________ identification algorithm which scores collocations using log-likelihood ratio.",
        "Options": [
            "a) collocation",
            "b) compaction",
            "c) collection",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The log-likelihood score indicates the relative usefulness of a collocation with regards other term combinations in the text."
    },
    {
        "id": 717,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) ‘Taste’ collaborative-filtering recommender component of Mahout was originally a separate project and can run standalone without Hadoop",
            "b) Integration of Mahout with initiatives such as the Pregel-like Giraph are actively under discussion",
            "c) Calculating the LLR is very straightforward",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: There are a couple ways to run the llr-based collocation algorithm in mahout."
    },
    {
        "id": 718,
        "Question": "The tokens are passed through a Lucene ____________ to produce NGrams of the desired length.",
        "Options": [
            "a) ShngleFil",
            "b) ShingleFilter",
            "c) SingleFilter",
            "d) Collfilter"
        ],
        "Answer": "Answer: b\nExplanation: The tools that the collocation identification algorithm are embedded within either consume tokenized text as input or provide the ability to specify an implementation of the Lucene Analyzer class perform tokenization in order to form ngrams."
    },
    {
        "id": 719,
        "Question": "The _________ collocation identifier is integrated into the process that is used to create vectors from sequence files of text keys and values.",
        "Options": [
            "a) lbr",
            "b) lcr",
            "c) llr",
            "d) lar"
        ],
        "Answer": "Answer: c\nExplanation: The –minLLR option can be used to control the cutoff that prevents collocations below the specified LLR score from being emitted."
    },
    {
        "id": 720,
        "Question": "____________ generates NGrams and counts frequencies for ngrams, head and tail subgrams.",
        "Options": [
            "a) CollocationDriver",
            "b) CollocDriver",
            "c) CarDriver",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Each call to the mapper passes in the full set of tokens for the corresponding document using a StringTuple."
    },
    {
        "id": 721,
        "Question": "A key of type ___________ is generated which is used later to join ngrams with their heads and tails in the reducer phase.",
        "Options": [
            "a) GramKey",
            "b) Primary",
            "c) Secondary",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The GramKey is a composite key made up of a string n-gram fragment as the primary key and a secondary key used for grouping and sorting in the reduce phase."
    },
    {
        "id": 722,
        "Question": "________ phase merges the counts for unique ngrams or ngram fragments across multiple documents.",
        "Options": [
            "a) CollocCombiner",
            "b) CollocReducer",
            "c) CollocMerger",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The combiner treats the entire GramKey as the key and as such, identical tuples from separate documents are passed into a single call to the combiner’s reduce method, their frequencies are summed and a single tuple is passed out via the collector."
    },
    {
        "id": 723,
        "Question": "Drill is designed from the ground up to support high-performance analysis on the ____________ data.",
        "Options": [
            "a) semi-structured",
            "b) structured",
            "c) unstructured",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Drill is an Apache open-source SQL query engine for Big Data exploration."
    },
    {
        "id": 724,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Drill provides plug-and-play integration with existing Apache Hive",
            "b) Developers can use the sandbox environment to get a feel for the power and capabilities of Apache Drill by performing various types of queries",
            "c) Drill is inspired by Google Dremel",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Apache Drill is an open source, low latency SQL query engine for Hadoop and NoSQL."
    },
    {
        "id": 725,
        "Question": "___________ includes Apache Drill as part of the Hadoop distribution.",
        "Options": [
            "a) Impala",
            "b) MapR",
            "c) Oozie",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The MapR Sandbox with Apache Drill is a fully functional single-node cluster that can be used to get an overview on Apache Drill in a Hadoop environment."
    },
    {
        "id": 726,
        "Question": "MapR __________ Solution Earns Highest Score in Gigaom Research Data Warehouse Interoperability Report.",
        "Options": [
            "a) SQL-on-Hadoop",
            "b) Hive-on-Hadoop",
            "c) Pig-on-Hadoop",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Drill is a pioneer in delivering self-service data exploration capabilities on data stored in multiple formats in files or NoSQL databases."
    },
    {
        "id": 727,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Hadoop is a prerequisite for Drill",
            "b) Drill tackles rapidly evolving application driven schemas and nested data structures",
            "c) Drill provides a single interface for structured and semi-structured data allowing you to readily query JSON files and HBase tables as easily as a relational table",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Hadoop is not a prerequisite for Drill and users can start ramping up with Drill by running SQL queries directly on the local file system."
    },
    {
        "id": 728,
        "Question": "Drill integrates with  BI tools using a standard __________ connector.",
        "Options": [
            "a) JDBC",
            "b) ODBC",
            "c) ODBC-JDBC",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Drill conforms to the stringent ANSI SQL standards ensuring compatibility with existing BI environments as well as Hive deployments."
    },
    {
        "id": 729,
        "Question": "Drill analyze semi-structured/nested data coming from _________ applications.",
        "Options": [
            "a) RDBMS",
            "b) NoSQL",
            "c) NewSQL",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Modern big data applications such as social, mobile, web and IoT deal with a larger number of users and larger amount of data than the traditional transactional applications."
    },
    {
        "id": 730,
        "Question": "Apache _________ provides direct queries on self-describing and semi-structured data in files.",
        "Options": [
            "a) Drill",
            "b) Mahout",
            "c) Oozie",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Users can explore live data on their own as it arrives versus spending weeks or months on data preparation, modeling, ETL and subsequent schema management."
    },
    {
        "id": 731,
        "Question": "Drill provides a __________ like internal data model to represent and process data.",
        "Options": [
            "a) XML",
            "b) JSON",
            "c) TIFF",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The flexibility of JSON data model allows Drill to query, without flattening, both simple and complex/nested data types as well as constantly changing application-driven schemas commonly seen with Hadoop/NoSQL applications."
    },
    {
        "id": 732,
        "Question": "Drill also provides intuitive extensions to SQL to work with _______ data types.",
        "Options": [
            "a) simple",
            "b) nested",
            "c) int",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Users can also plug-and-play with Hive environments to enable ad-hoc low latency queries on existing Hive tables and reuse Hive’s metadata, hundreds of file formats and UDFs out of the box."
    },
    {
        "id": 733,
        "Question": "The Apache Crunch Java library provides a framework for writing, testing, and running ___________ pipelines.",
        "Options": [
            "a) MapReduce",
            "b) Pig",
            "c) Hive",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Goal of Crunch is to make pipelines that are composed of many user-defined functions simple to write, easy to test, and efficient to run."
    },
    {
        "id": 734,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Scrunch’s Java API is centered around three interfaces that represent distributed datasets",
            "b) All of the other data transformation operations supported by the Crunch APIs are implemented in terms of three primitives",
            "c) A number of common Aggregator<V> implementations are provided in the Aggregators class",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: PGroupedTable provides a combine values operation that allows a commutative and associative Aggregator to be applied to the values of the PGroupedTable instance on both the map and reduce sides of the shuffle."
    },
    {
        "id": 735,
        "Question": "For Scala users, there is the __________ API, which is built on top of the Java APIs.",
        "Options": [
            "a) Prunch",
            "b) Scrunch",
            "c) Hivench",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: It includes a REPL (read-eval-print loop) for creating MapReduce pipelines."
    },
    {
        "id": 736,
        "Question": "The Crunch APIs are modeled after _________  which is the library that Google uses for building data pipelines on top of their own implementation of MapReduce.",
        "Options": [
            "a) FlagJava",
            "b) FlumeJava",
            "c) FlakeJava",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The Apache Crunch project develops and supports Java APIs that simplify the process of creating data pipelines on top of Apache Hadoop."
    },
    {
        "id": 737,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Crunch pipeline written by the development team sessionizes a set of user logs generates are then processed by a diverse collection of Pig scripts and Hive queries",
            "b) Crunch pipelines provide a thin veneer on top of MapReduce",
            "c) Developers have access to low-level MapReduce APIs",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Crunch is extremely fast, only slightly slower than a hand-tuned pipeline developed with the MapReduce APIs."
    },
    {
        "id": 738,
        "Question": "Crunch was designed for developers who understand __________ and want to use MapReduce effectively.",
        "Options": [
            "a) Java",
            "b) Python",
            "c) Scala",
            "d) Javascript"
        ],
        "Answer": "Answer: a\nExplanation: Crunch is often used in conjunction with Hive and Pig."
    },
    {
        "id": 739,
        "Question": "Hive, Pig, and Cascading all use a _________ data model.",
        "Options": [
            "a) value centric",
            "b) columnar",
            "c) tuple-centric",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Crunch allows developers considerable flexibility in how they represent their data, which makes Crunch the best pipeline platform for developers."
    },
    {
        "id": 740,
        "Question": "A __________ represents a distributed, immutable collection of elements of type T.",
        "Options": [
            "a) PCollect<T>",
            "b) PCollection<T>",
            "c) PCol<T>",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: PCollection<T> provides a method, parallelDo, that applies a DoFn to each element in the PCollection<T>."
    },
    {
        "id": 741,
        "Question": "___________ executes the pipeline as a series of MapReduce jobs.",
        "Options": [
            "a) SparkPipeline",
            "b) MRPipeline",
            "c) MemPipeline",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Every Crunch data pipeline is coordinated by an instance of the Pipeline interface."
    },
    {
        "id": 742,
        "Question": "__________ represent the logical computations of your Crunch pipelines.",
        "Options": [
            "a) DoFns",
            "b) DoFn",
            "c) ThreeFns",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: DoFns are designed to be easy to write, easy to test, and easy to deploy within the context of a MapReduce job."
    },
    {
        "id": 743,
        "Question": "PCollection, PTable, and PGroupedTable all support a __________ operation.",
        "Options": [
            "a) intersection",
            "b) union",
            "c) OR",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Union operation takes a series of distinct PCollections that all have the same data type and treats them as a single virtual PCollection."
    },
    {
        "id": 744,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) StreamPipeline executes the pipeline in-memory on the client",
            "b) MemPipeline executes the pipeline by converting it to a series of Spark pipelines",
            "c) MapReduce framework approach makes it easy for the framework to serialize data from the client to the cluster",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: SparkPipeline executes the pipeline by converting it to a series of Spark pipelines."
    },
    {
        "id": 745,
        "Question": "Crunch uses Java serialization to serialize the contents of all of the ______ in a pipeline definition.",
        "Options": [
            "a) Transient",
            "b) DoFns",
            "c) Configuration",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Dofus is a Flash based massively multiplayer online role-playing game (MMORPG) developed and published by Ankama Games."
    },
    {
        "id": 746,
        "Question": "Inline DoFn that splits a line up into words is an inner class ____________",
        "Options": [
            "a) Pipeline",
            "b) MyPipeline",
            "c) ReadPipeline",
            "d) WritePipe"
        ],
        "Answer": "Answer: b\nExplanation: Inner classes contain references to their parent outer classes, so unless MyPipeline implements the Serializable interface, the NotSerializableException will be thrown when Crunch tries to serialize the inner DoFn."
    },
    {
        "id": 747,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) DoFns also have a number of helper methods for working with Hadoop Counters, all named increment",
            "b) The Crunch APIs contain a number of useful subclasses of DoFn that handle common data processing scenarios and are easier to write and test",
            "c) FilterFn class defines a single abstract method",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Counters are an incredibly useful way of keeping track of the state of long-running data pipelines and detecting any exceptional conditions that occur during processing"
    },
    {
        "id": 748,
        "Question": "DoFns provide direct access to the __________ object that is used within a given Map or Reduce task via the getContext method.",
        "Options": [
            "a) TaskInputContext",
            "b) TaskInputOutputContext",
            "c) TaskOutputContext",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: There are also a number of helper methods for working with the objects associated with the TaskInputOutputContext"
    },
    {
        "id": 749,
        "Question": "The top-level ___________ package contains three of the most important specializations in Crunch.",
        "Options": [
            "a) org.apache.scrunch",
            "b) org.apache.crunch",
            "c) org.apache.kcrunch",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Each of these specialized DoFn implementations has associated methods on the PCollection, PTable, and PGroupedTable interfaces to support common data processing steps."
    },
    {
        "id": 750,
        "Question": "The Avros class also has a _____ method for creating PTypes for POJOs using Avro’s reflection-based serialization mechanism.",
        "Options": [
            "a) spot",
            "b) reflects",
            "c) gets",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: There are a couple of restrictions on the structure of the POJO."
    },
    {
        "id": 751,
        "Question": "The ______________ class defines a configuration parameter named LINES_PER_MAP that controls how the input file is split.",
        "Options": [
            "a) NLineInputFormat",
            "b) InputLineFormat",
            "c) LineInputFormat",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: We can set the value of parameter via the Source interface’s inputConf method."
    },
    {
        "id": 752,
        "Question": "The ________ class allows developers to exercise precise control over how data is partitioned, sorted, and grouped by the underlying execution engine.",
        "Options": [
            "a) Grouping",
            "b) GroupingOptions",
            "c) RowGrouping",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The GroupingOptions class is immutable."
    },
    {
        "id": 753,
        "Question": "Which of the following project is interface definition language for hadoop?",
        "Options": [
            "a) Oozie",
            "b) Mahout",
            "c) Thrift",
            "d) Impala"
        ],
        "Answer": "Answer: c\nExplanation: Thrift is an interface definition language and binary communication protocol that is used to define and create services for numerous languages."
    },
    {
        "id": 754,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Thrift is developed for scalable cross-language services development",
            "b) Thrift includes a complete stack for creating clients and servers",
            "c) The top part of the Thrift stack is generated code from the Thrift definition",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: The services generate from this file client and processor code."
    },
    {
        "id": 755,
        "Question": "__________ is used as a remote procedure call (RPC) framework for facebook.",
        "Options": [
            "a) Oozie",
            "b) Mahout",
            "c) Thrift",
            "d) Impala"
        ],
        "Answer": "Answer: c\nExplanation: In contrast to built-in types, created data structures are sent as a result in generated code."
    },
    {
        "id": 756,
        "Question": "Which of the following is a straightforward binary format?",
        "Options": [
            "a) TCompactProtocol",
            "b) TDenseProtocol",
            "c) TBinaryProtocol",
            "d) TSimpleJSONProtocol"
        ],
        "Answer": "Answer: c\nExplanation: TBinaryProtocol is not optimized for space efficiency."
    },
    {
        "id": 757,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) With Thrift, it is not possible to define a service and change the protocol and transport without recompiling the code",
            "b) Thrift includes server infrastructure to tie protocols and transports together, like blocking, non-blocking, and multi threaded servers",
            "c) Thrift supports a number of protocols for service definition",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: The underlying I/O part of the stack is differently implemented for different languages."
    },
    {
        "id": 758,
        "Question": "Which of the following is a more compact binary format?",
        "Options": [
            "a) TCompactProtocol",
            "b) TDenseProtocol",
            "c) TBinaryProtocol",
            "d) TSimpleJSONProtocol"
        ],
        "Answer": "Answer: a\nExplanation: TCompactProtocol is typically more efficient to process as well."
    },
    {
        "id": 759,
        "Question": "Which of the following format is similar to TCompactProtocol?",
        "Options": [
            "a) TCompactProtocol",
            "b) TDenseProtocol",
            "c) TBinaryProtocol",
            "d) TSimpleJSONProtocol"
        ],
        "Answer": "Answer: b\nExplanation: In TDenseProtocol, stripping off the meta information from what is transmitted."
    },
    {
        "id": 760,
        "Question": "________ is a write-only protocol that cannot be parsed by Thrift.",
        "Options": [
            "a) TCompactProtocol",
            "b) TDenseProtocol",
            "c) TBinaryProtocol",
            "d) TSimpleJSONProtocol"
        ],
        "Answer": "Answer: d\nExplanation: TSimpleJSONProtocol drops metadata using JSON. Suitable for parsing by scripting languages."
    },
    {
        "id": 761,
        "Question": "Which of the following Uses JSON for encoding of data?",
        "Options": [
            "a) TCompactProtocol",
            "b) TDenseProtocol",
            "c) TBinaryProtocol",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: TJSONProtocol is used JSON for encoding of data."
    },
    {
        "id": 762,
        "Question": "_____________ is a human-readable text format to aid in debugging.",
        "Options": [
            "a) TMemory",
            "b) TDebugProtocol",
            "c) TBinaryProtocol",
            "d) TSimpleJSONProtocol"
        ],
        "Answer": "Answer: b\nExplanation: TBinaryProtocol is faster to process than the text protocol but more difficult to debug."
    },
    {
        "id": 763,
        "Question": "_______ transport is required when using a non-blocking server.",
        "Options": [
            "a) TZlibTransport",
            "b) TFramedTransport",
            "c) TMemoryTransport",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: TFramedTransport sends data in frames, where each frame is preceded by length information."
    },
    {
        "id": 764,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) To create a Mahout service, one has to write Thrift files that describe it, generate the code in the destination language",
            "b) Thrift is written in Java",
            "c) Thrift is a lean and clean library",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The predefined serialization styles include: binary, HTTP-friendly and compact binary."
    },
    {
        "id": 765,
        "Question": "__________ uses memory for I/O in Thrift.",
        "Options": [
            "a) TZlibTransport",
            "b) TFramedTransport",
            "c) TMemoryTransport",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The Java implementation uses a simple ByteArrayOutputStream internally."
    },
    {
        "id": 766,
        "Question": "________ uses blocking socket I/O for transport.",
        "Options": [
            "a) TNonblockingServer",
            "b) TSimpleServer",
            "c) TSocket",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: TFramedTransport must be used with this server."
    },
    {
        "id": 767,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) There are no XML configuration files in Thrift",
            "b) Thrift gives cross-language serialization with lower overhead than alternatives such as SOAP due to use of binary format",
            "c) No framework to code is a feature of Thrift",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: There are no build dependencies or non-standard software. No mix of incompatible software licenses."
    },
    {
        "id": 768,
        "Question": "Which of the following is a multi-threaded server using non-blocking I/O?",
        "Options": [
            "a) TNonblockingServer",
            "b) TSimpleServer",
            "c) TSocket",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Java implementation uses NIO channels."
    },
    {
        "id": 769,
        "Question": "__________ is a single-threaded server using standard blocking I/O.",
        "Options": [
            "a) TNonblockingServer",
            "b) TSimpleServer",
            "c) TSocket",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: TSimpleServer is useful for testing."
    },
    {
        "id": 770,
        "Question": "Which of the following performs compression using zlib?",
        "Options": [
            "a) TZlibTransport",
            "b) TFramedTransport",
            "c) TMemoryTransport",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: TZlibTransport is used in conjunction with another transport. Not available in the Java implementation."
    },
    {
        "id": 771,
        "Question": "________ is a multi-threaded server using standard blocking I/O.",
        "Options": [
            "a) TNonblockingServer",
            "b) TThreadPoolServer",
            "c) TSimpleServer",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: TFramedTransport must be used with this server."
    },
    {
        "id": 772,
        "Question": "_____________ transport writes to a file.",
        "Options": [
            "a) TNonblockingServer",
            "b) TFileTransport",
            "c) TFramedTransport",
            "d) TMemoryTransport"
        ],
        "Answer": "Answer: b\nExplanation: TMemoryTransport uses memory for I/O."
    },
    {
        "id": 773,
        "Question": "__________ is a server based Bundle Engine that provides a higher-level oozie abstraction that will batch a set of coordinator applications.",
        "Options": [
            "a) Oozie v2",
            "b) Oozie v3",
            "c) Oozie v4",
            "d) Oozie v5"
        ],
        "Answer": "Answer: c\nExplanation: Oozie combines multiple jobs sequentially into one logical unit of work."
    },
    {
        "id": 774,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Oozie is a scalable, reliable and extensible system",
            "b) Oozie is a server-based Workflow Engine specialized in running workflow jobs",
            "c) Oozie Coordinator jobs are recurrent Oozie Workflow jobs triggered by time (frequency) and data availability",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Jobs include streaming as well as system specific jobs."
    },
    {
        "id": 775,
        "Question": "___________ is a Java Web application used to schedule Apache Hadoop jobs.",
        "Options": [
            "a) Impala",
            "b) Oozie",
            "c) Mahout",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Oozie is a workflow scheduler system to manage Hadoop jobs."
    },
    {
        "id": 776,
        "Question": "Oozie Workflow jobs are Directed ________ graphs  of actions.",
        "Options": [
            "a) Acyclical",
            "b) Cyclical",
            "c) Elliptical",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Oozie is a framework allowing to combine multiple Map/Reduce jobs into a logical unit of work."
    },
    {
        "id": 777,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Oozie v2 is a server based Coordinator Engine specialized in running workflows based on time and data triggers",
            "b) Oozie v1 is a server based Workflow Engine specialized in running workflow jobs with actions that execute Hadoop Map/Reduce and Pig jobs",
            "c) A Workflow application is DAG that coordinates the following types of actions",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Cycle in workflows are not supported."
    },
    {
        "id": 778,
        "Question": "Oozie v2 is a server based ___________ Engine specialized in running workflows based on time and data triggers.",
        "Options": [
            "a) Compactor",
            "b) Collector",
            "c) Coordinator",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Oozie v2 can continuously run workflows based on time and data."
    },
    {
        "id": 779,
        "Question": "Which of the following is one of the possible state for a workflow jobs?",
        "Options": [
            "a) PREP",
            "b) START",
            "c) RESUME",
            "d) END"
        ],
        "Answer": "Answer: a\nExplanation: Possible states for a workflow jobs are: PREP, RUNNING, SUSPENDED, SUCCEEDED, KILLED and FAILED."
    },
    {
        "id": 780,
        "Question": "Oozie can make _________ callback notifications on action start events and workflow end events.",
        "Options": [
            "a) TCP",
            "b) HTTP",
            "c) IP",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: In the case of an action start failure in a workflow job, depending on the type of failure, Oozie will attempt automatic retries, it will request a manual retry or it will fail the workflow job."
    },
    {
        "id": 781,
        "Question": "A workflow definition is a ______ with control flow nodes or action nodes.",
        "Options": [
            "a) CAG",
            "b) DAG",
            "c) BAG",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Oozie does not support cycles in workflow definitions, workflow definitions must be a strict DAG."
    },
    {
        "id": 782,
        "Question": "Which of the following workflow definition language is XML based?",
        "Options": [
            "a) hpDL",
            "b) hDL",
            "c) hiDL",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: hpDL stands for Hadoop Process Definition Language."
    },
    {
        "id": 783,
        "Question": "___________ nodes are the mechanism by which a workflow triggers the execution of a computation/processing task.",
        "Options": [
            "a) Server",
            "b) Client",
            "c) Mechanism",
            "d) Action"
        ],
        "Answer": "Answer: d\nExplanation: Oozie provides support for the following types of actions: Hadoop map-reduce, Hadoop file system, Pig, Java and Oozie sub-workflow."
    },
    {
        "id": 784,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Oozie is a Java Web-Application that runs in a Java servlet-container",
            "b) Oozie workflow is a collection of actions arranged in a control dependency DAG",
            "c) hPDL is a fairly compact language",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation:  Control nodes define the flow of execution and include beginning and end of a workflow (start, end and fail nodes) and mechanisms to control the workflow execution path."
    },
    {
        "id": 785,
        "Question": "Workflow with id __________ should be in SUCCEEDED/KILLED/FAILED.",
        "Options": [
            "a) wfId",
            "b) iUD",
            "c) iFD",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Workflow with id wfId should exist."
    },
    {
        "id": 786,
        "Question": "Nodes in the config _____________  must be completed successfully.",
        "Options": [
            "a) oozie.wid.rerun.skip.nodes",
            "b) oozie.wf.rerun.skip.nodes",
            "c) oozie.wf.run.skip.nodes",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: If no configuration is passed, existing coordinator/workflow configuration will be used."
    },
    {
        "id": 787,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Oozie provides a unique callback URL to the task, the task should invoke the given URL to notify its completion",
            "b) All computation/processing tasks triggered by an mechanism node are remote to Oozie",
            "c) Oozie workflows can be parameterized",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: All computation/processing tasks are executed by Hadoop Map/Reduce framework."
    },
    {
        "id": 788,
        "Question": "_____________ will skip the nodes given in the config with the same exit transition as before.",
        "Options": [
            "a) ActionMega handler",
            "b) Action handler",
            "c) Data handler",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Currently there is no way to remove an existing configuration but only override by passing a different value in the input configuration."
    },
    {
        "id": 789,
        "Question": "________ nodes that control the start and end of the workflow and workflow job execution path.",
        "Options": [
            "a) Action",
            "b) Control",
            "c) Data",
            "d) SubDomain"
        ],
        "Answer": "Answer: b\nExplanation: Workflow nodes are classified in control flow nodes and action nodes."
    },
    {
        "id": 790,
        "Question": "Node names and transitions must be conform to the following pattern =[a-zA-Z][\\-_a-zA-Z0-0]*=, of up to __________ characters long.",
        "Options": [
            "a) 10",
            "b) 15",
            "c) 20",
            "d) 25"
        ],
        "Answer": "Answer: c\nExplanation: Action nodes trigger the execution of a computation/processing task."
    },
    {
        "id": 791,
        "Question": "A workflow definition must have one ________ node.",
        "Options": [
            "a) start",
            "b) resume",
            "c) finish",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The start node is the entry point for a workflow job, it indicates the first workflow node the workflow job must transition to."
    },
    {
        "id": 792,
        "Question": "If one or more actions started by the workflow job are executing when the ________ node is reached, the actions will be killed.",
        "Options": [
            "a) kill",
            "b) start",
            "c) end",
            "d) finsih"
        ],
        "Answer": "Answer: a\nExplanation: A workflow definition may have zero or more kill nodes."
    },
    {
        "id": 793,
        "Question": "A ___________ node enables a workflow to make a selection on the execution path to follow.",
        "Options": [
            "a) fork",
            "b) decision",
            "c) start",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: All decision nodes must have a default element to avoid bringing the workflow into an error state if none of the predicates evaluates to true."
    },
    {
        "id": 794,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Predicates are JSP Expression Language (EL) expressions",
            "b) Predicates are evaluated in order or appearance until one of them evaluates to true and the corresponding transition is taken",
            "c) The name attribute in the decision node is the name of the decision node",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: The predicate ELs are evaluated in order until one returns true and the corresponding transition is taken."
    },
    {
        "id": 795,
        "Question": "Which of the following can be seen as a switch-case statement?",
        "Options": [
            "a) fork",
            "b) decision",
            "c) start",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: A decision node consists of a list of predicates-transition pairs plus a default transition."
    },
    {
        "id": 796,
        "Question": "All decision nodes must have a _____________ element to avoid bringing the workflow into an error state if none of the predicates evaluates to true.",
        "Options": [
            "a) name",
            "b) default",
            "c) server",
            "d) client"
        ],
        "Answer": "Answer: b\nExplanation: The default element indicates the transition to take if none of the predicates evaluates to true."
    },
    {
        "id": 797,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The fork and join nodes must be used in pairs",
            "b) The fork node assumes concurrent execution paths are children of the same fork node",
            "c) A join node waits until every concurrent execution path of a previous fork node arrives to it",
            "d) A fork node splits one path of execution into multiple concurrent paths of execution"
        ],
        "Answer": "Answer: b\nExplanation: The join node assumes concurrent execution paths are children of the same fork node."
    },
    {
        "id": 798,
        "Question": "The ___________ attribute in the join node is the name of the workflow join node.",
        "Options": [
            "a) name",
            "b) to",
            "c) down",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The to attribute in the join node indicates the name of the workflow node that will executed after all concurrent execution paths of the corresponding fork arrive to the join node."
    },
    {
        "id": 799,
        "Question": "If a computation/processing task -triggered by a workflow fails to complete successfully, its transitions to _____________",
        "Options": [
            "a) error",
            "b) ok",
            "c) true",
            "d) false"
        ],
        "Answer": "Answer: a\nExplanation: If a computation/processing task -triggered by a workflow completes successfully, it transitions to ok."
    },
    {
        "id": 800,
        "Question": "If the failure is of ___________ nature, Oozie will suspend the workflow job.",
        "Options": [
            "a) transient",
            "b) non-transient",
            "c) permanent",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: If the failure is an error and a retry will not resolve the problem, Oozie will perform the error transition for the action."
    },
    {
        "id": 801,
        "Question": "A _______________ action can be configured to perform file system cleanup and directory creation before starting the mapreduce job.",
        "Options": [
            "a) map",
            "b) reduce",
            "c) map-reduce",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The map-reduce action starts a Hadoop map/reduce job from a workflow."
    },
    {
        "id": 802,
        "Question": "___________ properties can be overridden by specifying them in the job-xml file or configuration element.",
        "Options": [
            "a) Pipe",
            "b) Decision",
            "c) Flag",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Pipes information can be specified in the pipes element."
    },
    {
        "id": 803,
        "Question": "A collection of various actions in a control dependency DAG is referred to as ________________",
        "Options": [
            "a) workflow",
            "b) dataflow",
            "c) clientflow",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Falcon provides the key services for data processing apps."
    },
    {
        "id": 804,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Large datasets are incentives for users to come to Hadoop",
            "b) Data management is a common concern to be offered as a service",
            "c) Understanding the life-time of a feed will allow for implicit validation of the processing rules",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Falcon decouples a data location and its properties from workflows."
    },
    {
        "id": 805,
        "Question": "The ability of Hadoop to efficiently process large volumes of data in parallel is called __________ processing.",
        "Options": [
            "a) batch",
            "b) stream",
            "c) time",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: There are also a number of use cases that require more “real-time” processing of data—processing the data as it arrives, rather than through batch processing."
    },
    {
        "id": 806,
        "Question": "__________ is used for simplified Data Management in Hadoop.",
        "Options": [
            "a) Falcon",
            "b) flume",
            "c) Impala",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Apache Falcon process orchestration and scheduling."
    },
    {
        "id": 807,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Falcon promotes Javascript Programming",
            "b) Falcon does not do any heavy lifting but delegates to tools with in the Hadoop ecosystem",
            "c) Falcon handles retry logic and late data processing. Records audit, lineage and metrics",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Falcon promotes Polyglot Programming."
    },
    {
        "id": 808,
        "Question": "Falcon provides ___________ workflow for copying data from source to target.",
        "Options": [
            "a) recurring",
            "b) investment",
            "c) data",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Falcon instruments workflows for dependencies, retry logic, Table/Partition registration, notifications, etc."
    },
    {
        "id": 809,
        "Question": "A recurring workflow is used for purging expired data on __________ cluster.",
        "Options": [
            "a) Primary",
            "b) Secondary",
            "c) BCP",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Falcon provides retention workflow for each cluster based on the defined policy."
    },
    {
        "id": 810,
        "Question": "Falcon provides the key services data processing applications need so Sophisticated________ can easily be added to Hadoop applications.",
        "Options": [
            "a) DAM",
            "b) DLM",
            "c) DCM",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Complex data processing logic is handled by Falcon instead of hard-coded in apps."
    },
    {
        "id": 811,
        "Question": "Falcon promotes decoupling of data set location from ___________ definition.",
        "Options": [
            "a) Oozie",
            "b) Impala",
            "c) Kafka",
            "d) Thrift"
        ],
        "Answer": "Answer: a\nExplanation: Falcon uses declarative processing with simple directives enabling rapid prototyping."
    },
    {
        "id": 812,
        "Question": "Falcon provides seamless integration with _____________",
        "Options": [
            "a) HCatalog",
            "b) metastore",
            "c) HBase",
            "d) Kafka"
        ],
        "Answer": "Answer: b\nExplanation: Falcon maintains the dependencies and relationships between entities."
    },
    {
        "id": 813,
        "Question": "Which of the following is project for Infrastructure Engineers and Data Scientists?",
        "Options": [
            "a) Impala",
            "b) BigTop",
            "c) Oozie",
            "d) Flume"
        ],
        "Answer": "Answer: b\nExplanation: Bigtop supports a wide range of components/projects, including, but not limited to, Hadoop, HBase and Spark."
    },
    {
        "id": 814,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Bigtop provides an integrated smoke testing framework, alongside a suite of over 10 test files",
            "b) Bigtop includes tools and a framework for testing at various levels",
            "c) Bigtop components supports only one Operating Systems",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Bigtop  is used for both initial deployments as well as upgrade scenarios for the entire data platform, not just the individual components."
    },
    {
        "id": 815,
        "Question": "Which of the following work is done by BigTop in Hadoop framework?",
        "Options": [
            "a) Packaging",
            "b) Smoke Testing",
            "c) Virtualization",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: BigTop is looking for comprehensive packaging, testing, and configuration of the leading open source big data components."
    },
    {
        "id": 816,
        "Question": "Which of the following operating system is not supported by BigTop?",
        "Options": [
            "a) Fedora",
            "b) Solaris",
            "c) Ubuntu",
            "d) SUSE"
        ],
        "Answer": "Answer: b\nExplanation: Bigtop components power the leading Hadoop distros and support many Operating Systems, including Debian/Ubuntu, CentOS, Fedora, SUSE and many others."
    },
    {
        "id": 817,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Bigtop-0.5.0 : Builds the 0.5.0 release",
            "b) Bigtop-trunk-HBase builds the HCatalog packages only",
            "c) There are also jobs for building virtual machine images",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Bigtop provides vagrant recipes, raw images, and (work-in-progress) docker recipes for deploying Hadoop from zero."
    },
    {
        "id": 818,
        "Question": "Apache Bigtop uses ___________ for continuous integration testing.",
        "Options": [
            "a) Jenkinstop",
            "b) Jerry",
            "c) Jenkins",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: There are 2 Jenkins servers running for the project."
    },
    {
        "id": 819,
        "Question": "The Apache Jenkins server runs the ______________ job whenever code is committed to the trunk branch.",
        "Options": [
            "a) “Bigtop-trunk”",
            "b) “Bigtop”",
            "c) “Big-trunk”",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Jenken Server in turn runs several test jobs."
    },
    {
        "id": 820,
        "Question": "The Bigtop Jenkins server runs daily jobs for the _______ and trunk branches.",
        "Options": [
            "a) 0.1",
            "b) 0.2",
            "c) 0.3",
            "d) 0.4"
        ],
        "Answer": "Answer: c\nExplanation: Each job has a configuration for each supported operating system. In each branch there is a job to build each component."
    },
    {
        "id": 821,
        "Question": "Which of the following builds an APT or YUM package repository?",
        "Options": [
            "a) Bigtop-trunk-packagetest",
            "b) Bigtop-trunk-repository",
            "c) Bigtop-VM-matrix",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Bigtop-trunk-packagetest runs the package tests."
    },
    {
        "id": 822,
        "Question": "___________ builds virtual machines of branches trunk and 0.3 for KVM, VMWare and VirtualBox.",
        "Options": [
            "a) Bigtop-trunk-packagetest",
            "b) Bigtop-trunk-repository",
            "c) Bigtop-VM-matrix",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Bigtop-trunk-repository builds an APT or YUM package repository."
    },
    {
        "id": 823,
        "Question": "__________ is a fully integrated, state-of-the-art analytic database architected specifically to leverage strengths of Hadoop.",
        "Options": [
            "a) Oozie",
            "b) Impala",
            "c) Lucene",
            "d) BigTop"
        ],
        "Answer": "Answer: b\nExplanation: Impala provides scalability and flexibility to hadoop."
    },
    {
        "id": 824,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) With Impala, more users, whether using SQL queries or BI applications, can interact with more data",
            "b) Technical support for Impala is not available via a Cloudera Enterprise subscription",
            "c) Impala is proprietary tool for Hadoop",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: It is possible through a single repository and metadata store from source through analysis."
    },
    {
        "id": 825,
        "Question": "Impala is an integrated part of a ____________ enterprise data hub.",
        "Options": [
            "a) MicroSoft",
            "b) IBM",
            "c) Cloudera",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Impala is open source (Apache License), so you can self-support in perpetuity if you wish."
    },
    {
        "id": 826,
        "Question": "For Apache __________ users, Impala utilizes the same metadata.",
        "Options": [
            "a) cTakes",
            "b) Hive",
            "c) Pig",
            "d) Oozie"
        ],
        "Answer": "Answer: b\nExplanation: You don’t have to worry about re-inventing the implementation wheel."
    },
    {
        "id": 827,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) For Apache Hive users, Impala utilizes the same metadata, ODBC driver, SQL syntax, and user interface as Hive",
            "b) Impala provides high latency and low concurrency",
            "c) Impala also scales linearly, even in multi tenant environments",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Impala provides low latency and high concurrency."
    },
    {
        "id": 828,
        "Question": "Impala is integrated with native Hadoop security and Kerberos for authentication via __________ module.",
        "Options": [
            "a) Sentinue",
            "b) Sentry",
            "c) Sentinar",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Via the Sentry module, you can ensure that the right users and applications are authorized for the right data."
    },
    {
        "id": 829,
        "Question": "Which of the following companies shipped Impala?",
        "Options": [
            "a) Amazon",
            "b) Oracle",
            "c) MapR",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Impala is shipped by Cloudera, MapR, Oracle, and Amazon."
    },
    {
        "id": 830,
        "Question": "____________ analytics is a work in progress with Impala.",
        "Options": [
            "a) Reproductive",
            "b) Exploratory",
            "c) Predictive",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Impala is the de facto standard for open source interactive business intelligence and data discovery. "
    },
    {
        "id": 831,
        "Question": "Which of the following features is not provided by Impala?",
        "Options": [
            "a) SQL functionality",
            "b) ACID",
            "c) Flexibility",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation:  Impala combines all of the benefits of other Hadoop frameworks, including flexibility, scalability, and cost-effectiveness, with the performance, usability, and SQL functionality necessary for an enterprise-grade analytic database."
    },
    {
        "id": 832,
        "Question": "Which of the following hadoop file formats is supported by Impala?",
        "Options": [
            "a) SequenceFile",
            "b) Avro",
            "c) RCFile",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Impala reads Hadoop file formats, including text, LZO, SequenceFile, Avro, RCFile, and Parquet."
    },
    {
        "id": 833,
        "Question": "____________ is a distributed real-time computation system for processing large volumes of high-velocity data.",
        "Options": [
            "a) Kafka",
            "b) Storm",
            "c) Lucene",
            "d) BigTop"
        ],
        "Answer": "Answer: b\nExplanation: Storm on YARN is powerful for scenarios requiring real-time analytics, machine learning and continuous monitoring of operations."
    },
    {
        "id": 834,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) A Storm topology consumes streams of data and processes those streams in arbitrarily complex ways",
            "b) Apache Storm is a free and open source distributed real-time computation system",
            "c) Storm integrates with the queueing and database technologies you already use",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Storm has many use cases: real-time analytics, online machine learning, continuous computation, distributed RPC, ETL, and more."
    },
    {
        "id": 835,
        "Question": "Storm integrates with __________ via Apache Slider.",
        "Options": [
            "a) Scheduler",
            "b) YARN",
            "c) Compaction",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Impala is open source (Apache License), so you can self-support in perpetuity if you wish."
    },
    {
        "id": 836,
        "Question": "For Apache __________ users, Storm utilizes the same ODBC interface.",
        "Options": [
            "a) cTakes",
            "b) Hive",
            "c) Pig",
            "d) Oozie"
        ],
        "Answer": "Answer: b\nExplanation: You don’t have to worry about re-inventing the implementation wheel."
    },
    {
        "id": 837,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Storm is difficult and can be used with only Java",
            "b) Storm is fast: a benchmark clocked it at over a million tuples processed per second per node",
            "c) Storm is scalable, fault-tolerant, guarantees your data will be processed",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Storm is simple, can be used with any programming language."
    },
    {
        "id": 838,
        "Question": "Storm is benchmarked as processing one million _______ byte messages per second per node.",
        "Options": [
            "a) 10",
            "b) 50",
            "c) 100",
            "d) 200"
        ],
        "Answer": "Answer: c\nExplanation: Storm is a distributed real-time computation system. "
    },
    {
        "id": 839,
        "Question": "Apache Storm added the open source, stream data processing to _________ Data Platform.",
        "Options": [
            "a) Cloudera",
            "b) Hortonworks",
            "c) Local Cloudera",
            "d) MapR"
        ],
        "Answer": "Answer: b\nExplanation: The Storm community is working to improve capabilities related to three important themes: business continuity, operations and developer productivity."
    },
    {
        "id": 840,
        "Question": "How many types of nodes are present in Storm cluster?",
        "Options": [
            "a) 1",
            "b) 2",
            "c) 3",
            "d) 4"
        ],
        "Answer": "Answer: c\nExplanation: A storm cluster has three sets of nodes."
    },
    {
        "id": 841,
        "Question": "__________  node distributes code across the cluster.",
        "Options": [
            "a) Zookeeper",
            "b) Nimbus",
            "c) Supervisor",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Nimbus node is master node, similar to the Hadoop JobTracker."
    },
    {
        "id": 842,
        "Question": "____________ communicates with Nimbus through Zookeeper, starts and stops workers according to signals from Nimbus.",
        "Options": [
            "a) Zookeeper",
            "b) Nimbus",
            "c) Supervisor",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: ZooKeeper nodes coordinate the Storm cluster."
    },
    {
        "id": 843,
        "Question": "Kafka is comparable to traditional messaging systems such as _____________",
        "Options": [
            "a) Impala",
            "b) ActiveMQ",
            "c) BigTop",
            "d) Zookeeper"
        ],
        "Answer": "Answer: b\nExplanation: Kafka works well as a replacement for a more traditional message broker."
    },
    {
        "id": 844,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds",
            "b) Activity tracking is often very high volume as many activity messages are generated for each user page view",
            "c) Kafka is often used for operational monitoring data",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation:  This involves aggregating statistics from distributed applications to produce centralized feeds of operational data."
    },
    {
        "id": 845,
        "Question": "Many people use Kafka as a replacement for a ___________ solution.",
        "Options": [
            "a) log aggregation",
            "b) compaction",
            "c) collection",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Log aggregation typically collects physical log files off servers and puts them in a central place."
    },
    {
        "id": 846,
        "Question": "_______________ is a style of application design where state changes are logged as a time-ordered sequence of records.",
        "Options": [
            "a) Event sourcing",
            "b) Commit Log",
            "c) Stream Processing",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Kafka’s support for very large stored log data makes it an excellent backend for an application built in this style."
    },
    {
        "id": 847,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Kafka can serve as a kind of external commit-log for a distributed system",
            "b) The log helps replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data",
            "c) Kafka comes with a command-line client that will take input from a file or from standard input and send it out as messages to the Kafka cluster",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: By default, each line will be sent as a separate message."
    },
    {
        "id": 848,
        "Question": "Kafka uses __________ so you need to first start a ZooKeeper server if you don’t already have one.",
        "Options": [
            "a) Impala",
            "b) ActiveMQ",
            "c) BigTop",
            "d) Zookeeper"
        ],
        "Answer": "Answer: d\nExplanation: You can use the convenience script packaged with Kafka to get a quick-and-dirty single-node ZooKeeper instance."
    },
    {
        "id": 849,
        "Question": "__________ is the node responsible for all reads and writes for the given partition.",
        "Options": [
            "a) replicas",
            "b) leader",
            "c) follower",
            "d) isr"
        ],
        "Answer": "Answer: b\nExplanation: Each node will be the leader for a randomly selected portion of the partitions."
    },
    {
        "id": 850,
        "Question": "__________ is the subset of the replicas list that is currently alive and caught up to the leader.",
        "Options": [
            "a) replicas",
            "b) leader",
            "c) follower",
            "d) isr"
        ],
        "Answer": "Answer: d\nExplanation: “isr” is the set of “in-sync” replicas."
    },
    {
        "id": 851,
        "Question": "Kafka uses key-value pairs in the ____________ file format for configuration.",
        "Options": [
            "a) RFC",
            "b) Avro",
            "c) Property",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: These key values can be supplied either from a file or programmatically."
    },
    {
        "id": 852,
        "Question": "__________ is the amount of time to keep a log segment before it is deleted.",
        "Options": [
            "a) log.cleaner.enable",
            "b) log.retention",
            "c) log.index.enable",
            "d) log.flush.interval.message"
        ],
        "Answer": "Answer: b\nExplanation: log.cleaner.enable is configuration must be set to true for log compaction to run."
    },
    {
        "id": 853,
        "Question": "__________ provides the functionality of a messaging system.",
        "Options": [
            "a) Oozie",
            "b) Kafka",
            "c) Lucene",
            "d) BigTop"
        ],
        "Answer": "Answer: b\nExplanation: Kafka is a distributed, partitioned, replicated commit log service."
    },
    {
        "id": 854,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) With Kafka, more users, whether using SQL queries or BI applications, can interact with more data",
            "b) A topic is a category or feed name to which messages are published",
            "c) For each topic, the Kafka cluster maintains a partitioned log",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Kafka is possible through a single repository and metadata store from source through analysis."
    },
    {
        "id": 855,
        "Question": "Kafka maintains feeds of messages in categories called __________",
        "Options": [
            "a) topics",
            "b) chunks",
            "c) domains",
            "d) messages"
        ],
        "Answer": "Answer: a\nExplanation: We’ll call processes that publish messages to Kafka topic producers."
    },
    {
        "id": 856,
        "Question": "Kafka is run as a cluster comprised of one or more servers each of which is called __________",
        "Options": [
            "a) cTakes",
            "b) broker",
            "c) test",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: We’ll call processes that subscribe to topics and process the feed of published messages consumers."
    },
    {
        "id": 857,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The Kafka cluster does not retain all published messages",
            "b) A single Kafka broker can handle hundreds of megabytes of reads and writes per second from thousands of clients",
            "c) Kafka is designed to allow a single cluster to serve as the central data backbone for a large organization",
            "d) Messages are persisted on disk and replicated within the cluster to prevent data loss"
        ],
        "Answer": "Answer: a\nExplanation: The Kafka cluster retains all published messages—whether or not they have been consumed—for a configurable period of time. "
    },
    {
        "id": 858,
        "Question": "Communication between the clients and the servers is done with a simple, high-performance, language-agnostic _________ protocol.",
        "Options": [
            "a) IP",
            "b) TCP",
            "c) SMTP",
            "d) ICMP"
        ],
        "Answer": "Answer: b\nExplanation: Java client is provided for Kafka, but clients are available in many languages."
    },
    {
        "id": 859,
        "Question": "The only metadata retained on a per-consumer basis is the position of the consumer in the log, called __________",
        "Options": [
            "a) offset",
            "b) partition",
            "c) chunks",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: offset is controlled by the consumer: normally a consumer will advance its offset linearly as it reads messages"
    },
    {
        "id": 860,
        "Question": "Each kafka partition has one server which acts as the _________",
        "Options": [
            "a) leaders",
            "b) followers",
            "c) staters",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Each partition is replicated across a configurable number of servers for fault tolerance."
    },
    {
        "id": 861,
        "Question": "_________ has stronger ordering guarantees than a traditional messaging system.",
        "Options": [
            "a) kafka",
            "b) Slider",
            "c) Suz",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: A traditional queue retains messages in-order on the server."
    },
    {
        "id": 862,
        "Question": "Kafka only provides a _________ order over messages within a partition.",
        "Options": [
            "a) partial",
            "b) total",
            "c) 30%",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Per-partition ordering combined with the ability to partition data by key is sufficient for most applications. "
    },
    {
        "id": 863,
        "Question": "Apache __________ is a data repository containing device information, images, and other relevant information for all sorts of mobile devices.",
        "Options": [
            "a) DirectMemory",
            "b) Directory",
            "c) DeviceMap",
            "d) Drill"
        ],
        "Answer": "Answer: c\nExplanation: Drill is a distributed system for interactive analysis of large-scale datasets. "
    },
    {
        "id": 864,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Drill is a build system based on Apache Ant and Apache Ivy",
            "b) DirectMemory’s main purpose is to act as a second-level cache",
            "c) Easyant is inspired by Google’s Dremel",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: DirectMemory is used to store large amounts of data without filling up the Java heap and thus avoiding long garbage collection cycles."
    },
    {
        "id": 865,
        "Question": "____________ is a secure and highly scalable micro sharing and micro-messaging platform.",
        "Options": [
            "a) ESME",
            "b) Directory",
            "c) Empire-db",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: ESME allows people to discover and meet one another and get controlled access to other sources of information, all in a business process context."
    },
    {
        "id": 866,
        "Question": "Which of the framework is used for building and consuming network services?",
        "Options": [
            "a) ESME",
            "b) DirectoryMap",
            "c) Empire-db",
            "d) Etch"
        ],
        "Answer": "Answer: d\nExplanation: Etch is a cross-platform, language- and transport-independent framework."
    },
    {
        "id": 867,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Felix is implementation of the OSGi R4 specification",
            "b) Falcon is a data processing and management solution",
            "c) Flex is application framework for building Flash-based applications",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Falcon is used for coordination of data pipelines, lifecycle management, and data discovery."
    },
    {
        "id": 868,
        "Question": "_____________ is an open source system for expressive, declarative, fast, and efficient data analysis.",
        "Options": [
            "a) Flume",
            "b) Flink",
            "c) Flex",
            "d) ESME"
        ],
        "Answer": "Answer: b\nExplanation: Stratosphere combines the scalability and programming flexibility of distributed MapReduce-like platforms with the efficiency, out-of-core execution."
    },
    {
        "id": 869,
        "Question": "________________ is complete FTP Server based on Mina I/O system.",
        "Options": [
            "a) Giraph",
            "b) Gereition",
            "c) FtpServer",
            "d) Oozie"
        ],
        "Answer": "Answer: c\nExplanation: Giraph is a large-scale, fault-tolerant, Bulk Synchronous Parallel (BSP)-based graph processing framework."
    },
    {
        "id": 870,
        "Question": "_____________ is a distributed computing framework based on BSP.",
        "Options": [
            "a) HCataMan",
            "b) HCatlaog",
            "c) Hama",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: BSP stands for Bulk Synchronous Parallel."
    },
    {
        "id": 871,
        "Question": "Apache __________ is a generic cluster management framework used to build distributed systems.",
        "Options": [
            "a) Helix",
            "b) Gereition",
            "c) FtpServer",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Helix provides automatic partition management, fault tolerance and elasticity."
    },
    {
        "id": 872,
        "Question": "The __________ data Mapper framework makes it easier to use a database with Java or .NET applications.",
        "Options": [
            "a) iBix",
            "b) Helix",
            "c) iBATIS",
            "d) iBAT"
        ],
        "Answer": "Answer: c\nExplanation: iBATIS couples objects with stored procedures or SQL statements using an XML descriptor."
    },
    {
        "id": 873,
        "Question": "Which of the following is java-based tool for tracking, resolving and managing project dependencies?",
        "Options": [
            "a) jclouds",
            "b) JDO",
            "c) ivy",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: jclouds is a cloud agnostic library that enables developers to access a variety of supported cloud providers using one API."
    },
    {
        "id": 874,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Jena is Java framework for building Semantic Web applications",
            "b) JSPWiki is Java-based wiki engine",
            "c) jUDDI is implementation of a Universal Description Discovery and Integration (UDDI) registry",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: jUDDI is a web service."
    },
    {
        "id": 875,
        "Question": "Which of the following are Content Management and publishing system based on Cocoon?",
        "Options": [
            "a) LibCloud",
            "b) Kafka",
            "c) Lenya",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Kafka is a distributed publish-subscribe system for processing large amounts of streaming data."
    },
    {
        "id": 876,
        "Question": "__________ is used for Logging for .NET framework.",
        "Options": [
            "a) log4net",
            "b) logphp",
            "c) Lucene.NET",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Lucene.NET is a source code, class-per-class, API-per-API and algorithmic port of the Java."
    },
    {
        "id": 877,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Lucy is loose port of the Lucene search engine library, written in C and targeted at dynamic language users",
            "b) Manifold Connector Framework consist of connectors for content repositories like Sharepoint, Documentum, etc",
            "c) Marmotta is an open implementation of a Linked Data Platform",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Marmotta is incubator developed by Fabian Christ."
    },
    {
        "id": 878,
        "Question": "__________ is a cluster manager that provides resource sharing and isolation across cluster applications.",
        "Options": [
            "a) Merlin",
            "b) Mesos",
            "c) Max",
            "d) Merge"
        ],
        "Answer": "Answer: b\nExplanation: Merlin eclipse plugin is merged with an existing eclipse plugin already at avalon."
    },
    {
        "id": 879,
        "Question": "Which of the following is a data access framework?",
        "Options": [
            "a) Merge",
            "b) Lucene.NET",
            "c) MetaModel",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: MetaModel provides a common interface for exploration and querying of different types of datastores."
    },
    {
        "id": 880,
        "Question": "__________ is a library to support unit testing of Hadoop MapReduce jobs.",
        "Options": [
            "a) Myfaces",
            "b) Muse",
            "c) modftp",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: MRUnit is a library to support unit testing of Hadoop MapReduce jobs."
    },
    {
        "id": 881,
        "Question": "Which of the following is a robust implementation of the OASIS WSDM?",
        "Options": [
            "a) Myfaces",
            "b) Muse",
            "c) modftp",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Muse uses Web Services (MuWS) specifications."
    },
    {
        "id": 882,
        "Question": "__________ is a framework for building Java Server application GUIs.",
        "Options": [
            "a) Myfaces",
            "b) Muse",
            "c) Flume",
            "d) BigTop"
        ],
        "Answer": "Answer: a\nExplanation: Myfaces is based on JavaServer Faces (certified implementation of JSR-127)."
    },
    {
        "id": 883,
        "Question": "Which of the following is a Web search software?",
        "Options": [
            "a) Imphala",
            "b) Nutch",
            "c) Oozie",
            "d) Manmgy"
        ],
        "Answer": "Answer: b\nExplanation: Oozie is server-based workflow scheduling and coordination system to manage data processing jobs for Apache Hadoop."
    },
    {
        "id": 884,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) OFBiz stands for “The Open For Business Project”",
            "b) Od stands for “Orchestration Director Engine”",
            "c) OCG is Object-Graph Notation Language implementation in Java",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The Open For Business Project (OFBiz) is an open-source enterprise automation software project."
    },
    {
        "id": 885,
        "Question": "___________ defines an open application programming interface for common cloud application services.",
        "Options": [
            "a) Bigred",
            "b) Nuvem",
            "c) Oozie",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Nuvem allowed applications to be easily ported across the most popular cloud platforms."
    },
    {
        "id": 886,
        "Question": "__________ is OData implementation in Java.",
        "Options": [
            "a) Bigred",
            "b) Nuvem",
            "c) Olingo",
            "d) Onami"
        ],
        "Answer": "Answer: d\nExplanation: Apache Onami aims to create a community focused on the development and maintenance of a set of Google Guice extensions."
    },
    {
        "id": 887,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) OpenOffice.org is comprised of six personal productivity applications",
            "b) Open Climate Workbench is tool for scalable comparison of remote sensing observations",
            "c) OpenNLP is a machine learning based toolkit for the processing of natural language text",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Open Climate Workbench is used to observe climate model outputs, regionally and globally."
    },
    {
        "id": 888,
        "Question": "___________ is an open source SQL query engine for Apache HBase.",
        "Options": [
            "a) Pig",
            "b) Phoenix",
            "c) Pivot",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Pig is a platform for analyzing large datasets."
    },
    {
        "id": 889,
        "Question": "___________ provides multiple language implementations of the Advanced Message Queuing Protocol (AMQP).",
        "Options": [
            "a) RTA",
            "b) Qpid",
            "c) RAT",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: RAT became part of new Apache Creadur TLP."
    },
    {
        "id": 890,
        "Question": "___________ is A WEb And SOcial Mashup Engine.",
        "Options": [
            "a) ServiceMix",
            "b) Samza",
            "c) Rave",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: Samza is a stream processing system for running continuous computation on infinite streams of data."
    },
    {
        "id": 891,
        "Question": "The ___________ project will create an ESB and component suite based on the Java Business Interface (JBI) standard – JSR 208.",
        "Options": [
            "a) ServiceMix",
            "b) Samza",
            "c) Rave",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: ServiceMix project is Geronimo developed by James."
    },
    {
        "id": 892,
        "Question": "Which of the following is a spatial information system?",
        "Options": [
            "a) Sling",
            "b) Solr",
            "c) SIS",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The Spatial Information System (SIS) Project is a toolkit that spatial information system builders or users can leverage to build applications containing location context."
    },
    {
        "id": 893,
        "Question": "Stratos will be a polyglot _________ framework.",
        "Options": [
            "a) Daas",
            "b) PaaS",
            "c) Saas",
            "d) Raas"
        ],
        "Answer": "Answer: b\nExplanation: PaaS provides developers a cloud-based environment for developing, testing, and running scalable applications."
    },
    {
        "id": 894,
        "Question": "Which of the following supports random-writable and advance-able sparse bitsets?",
        "Options": [
            "a) Stratos",
            "b) Kafka",
            "c) Sqoop",
            "d) Lucene"
        ],
        "Answer": "Answer: d\nExplanation: Lucene.Net is a port of the Lucene search engine library, written in C# and targeted at .NET runtime users."
    },
    {
        "id": 895,
        "Question": "____________ is an open-source version control system.",
        "Options": [
            "a) Stratos",
            "b) Kafka",
            "c) Sqoop",
            "d) Subversion"
        ],
        "Answer": "Answer: d\nExplanation: Subversion contains lot of features for hadoop."
    },
    {
        "id": 896,
        "Question": "___________ is a distributed data warehouse system for Hadoop.",
        "Options": [
            "a) Stratos",
            "b) Tajo",
            "c) Sqoop",
            "d) Lucene"
        ],
        "Answer": "Answer: b\nExplanation: Sqoop is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases."
    },
    {
        "id": 897,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) TusCAN ia Service Component Architecture implementation",
            "b) Tob is a JSF based framework for web-applications",
            "c) Traffic is a scalable and extensible HTTP proxy server and cache",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Tuscany is used for service Component Architecture implementation."
    },
    {
        "id": 898,
        "Question": "___________ is a distributed, fault-tolerant, and high-performance realtime computation system.",
        "Options": [
            "a) Knife",
            "b) Storm",
            "c) Sqoop",
            "d) Lucene"
        ],
        "Answer": "Answer: b\nExplanation: Storm provides strong guarantees on the processing of data."
    },
    {
        "id": 899,
        "Question": "Which of the following is a standard compliant XML Query processor?",
        "Options": [
            "a) Whirr",
            "b) VXQuery",
            "c) Knife",
            "d) Lens"
        ],
        "Answer": "Answer: b\nExplanation: Whirr provides code for running a variety of software services on cloud infrastructure."
    },
    {
        "id": 900,
        "Question": "Apache _________ is a project that enables development and consumption of REST style web services.",
        "Options": [
            "a) Wives",
            "b) Wink",
            "c) Wig",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The core server runtime is based on the JAX-RS (JSR 311) standard."
    },
    {
        "id": 901,
        "Question": "__________ is a log collection and correlation software with reporting and alarming functionalities.",
        "Options": [
            "a) Lucene",
            "b) ALOIS",
            "c) Imphal",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: This Project activity is transferred to another Incubator project – ODE."
    },
    {
        "id": 902,
        "Question": "__________ is a non-blocking, asynchronous, event driven high performance web framework.",
        "Options": [
            "a) AWS",
            "b) AWF",
            "c) AWT",
            "d) ASW"
        ],
        "Answer": "Answer: b\nExplanation: AWF originally known as Deft, renamed to AWF on 2012-02-15."
    },
    {
        "id": 903,
        "Question": "___________ is the world’s most complete, tested, and popular distribution of Apache Hadoop and related projects.",
        "Options": [
            "a) MDH",
            "b) CDH",
            "c) ADH",
            "d) BDH"
        ],
        "Answer": "Answer: b\nExplanation: Cloudera’s open-source Apache Hadoop distribution, CDH (Cloudera Distribution Including Apache Hadoop), targets enterprise-class deployments of that technology."
    },
    {
        "id": 904,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Cloudera is also a sponsor of the Apache Software Foundation",
            "b) CDH is 100% Apache-licensed open source and is the only Hadoop solution to offer unified batch processing, interactive SQL, and interactive search, and role-based access controls",
            "c) More enterprises have downloaded CDH than all other such distributions combined",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Cloudera says that more than 50% of its engineering output is donated upstream to the various Apache-licensed open source projects."
    },
    {
        "id": 905,
        "Question": "Cloudera ___________ includes CDH and an annual subscription license (per node) to Cloudera Manager and technical support.",
        "Options": [
            "a) Enterprise",
            "b) Express",
            "c) Standard",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: CDH includes the core elements of Apache Hadoop plus several additional key open source projects."
    },
    {
        "id": 906,
        "Question": "Cloudera Express includes CDH and a version of Cloudera ___________ lacking enterprise features such as rolling upgrades and backup/disaster recovery.",
        "Options": [
            "a) Enterprise",
            "b) Express",
            "c) Standard",
            "d) Manager"
        ],
        "Answer": "Answer: d\nExplanation: All versions may be downloaded from Cloudera’s website."
    },
    {
        "id": 907,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) CDH contains the main, core elements of Hadoop",
            "b) In October 2012, Cloudera announced the Cloudera Impala project",
            "c) CDH may be downloaded from Cloudera’s website at no charge",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: CDH may be downloaded from Cloudera’s website with no technical support nor Cloudera Manager."
    },
    {
        "id": 908,
        "Question": "Cloudera Enterprise comes in ___________ edition.",
        "Options": [
            "a) One",
            "b) Two",
            "c) Three",
            "d) Four"
        ],
        "Answer": "Answer: c\nExplanation: Cloudera Enterprise comes in three editions: Basic, Flex, and Data Hub."
    },
    {
        "id": 909,
        "Question": "__________ is a online NoSQL developed by Cloudera.",
        "Options": [
            "a) HCatalog",
            "b) Hbase",
            "c) Imphala",
            "d) Oozie"
        ],
        "Answer": "Answer: b\nExplanation: HBase is a distributed key value store."
    },
    {
        "id": 910,
        "Question": "_______ is an open source set of libraries, tools, examples, and documentation engineered.",
        "Options": [
            "a) Kite",
            "b) Kize",
            "c) Ookie",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Kite is used to simplify the most common tasks when building applications on top of Hadoop."
    },
    {
        "id": 911,
        "Question": "To configure short-circuit local reads, you will need to enable ____________ on local Hadoop.",
        "Options": [
            "a) librayhadoop",
            "b) libhadoop",
            "c) libhad",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Short-circuit reads make use of a UNIX domain socket."
    },
    {
        "id": 912,
        "Question": "CDH process and control sensitive data and facilitate __________",
        "Options": [
            "a) multi-tenancy",
            "b) flexibilty",
            "c) scalability",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Cloudera Express offers the fastest and easiest way to getting your Hadoop cluster up and running and exploring your first use cases."
    },
    {
        "id": 913,
        "Question": "Microsoft and Hortonworks joined their forces to make Hadoop available on ___________ for on-premise deployments.",
        "Options": [
            "a) Windows 7",
            "b) Windows Server",
            "c) Windows 8",
            "d) Ubuntu"
        ],
        "Answer": "Answer: b\nExplanation: Win32 is supported as a development platform."
    },
    {
        "id": 914,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes",
            "b) GNU/Linux is supported as a development and production platform",
            "c) Distributed operation has not been well tested on Win32, so it is not supported as a production platform",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Microsoft and Hortonworks joined their forces to make Hadoop available on Windows Azure to support big data in the cloud."
    },
    {
        "id": 915,
        "Question": "Hadoop ___________ is a utility to support running external map and reduce jobs.",
        "Options": [
            "a) Orchestration",
            "b) Streaming",
            "c) Collection",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: These external jobs can be written in various programming languages such as Python or Ruby."
    },
    {
        "id": 916,
        "Question": "In Hadoop _____________ go to the Hadoop distribution directory for HDInsight.",
        "Options": [
            "a) Shell",
            "b) Command Line",
            "c) Compaction",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: In order to run Hadoop command line from Windows cmd prompt, you need to login to the HDInsight head node using Remote Desktop."
    },
    {
        "id": 917,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The other flavor of HDInsight interactive console is based on JavaScript",
            "b) Microsoft and Hortonworks have re-implemented the key binaries as executables",
            "c) The distribution consists of Hadoop 1.1.0, Pig-0.9.3, Hive 0.9.0, Mahout 0.5 and Sqoop 1.4.2",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: JavaScript commands are converted to Pig statements."
    },
    {
        "id": 918,
        "Question": "Microsoft Azure HDInsight comes with __________ types of interactive console.",
        "Options": [
            "a) two",
            "b) three",
            "c) four",
            "d) five"
        ],
        "Answer": "Answer: a\nExplanation: One is the standard Hadoop Hive console, the other one is unique in Hadoop world, it is based on JavaScript."
    },
    {
        "id": 919,
        "Question": "The key _________ command – which is traditionally a bash script – is also re-implemented as hadoop.cmd.",
        "Options": [
            "a) start",
            "b) hadoop",
            "c) had",
            "d) hadstrat"
        ],
        "Answer": "Answer: b\nExplanation: HDInsight is the framework for the Microsoft Azure cloud implementation of Hadoop."
    },
    {
        "id": 920,
        "Question": "Which of the following individual components are included on HDInsight clusters?",
        "Options": [
            "a) Hive",
            "b) Pig",
            "c) Oozie",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: HDInsight provides several configurations for specific workloads, or you can customize clusters using Script Actions."
    },
    {
        "id": 921,
        "Question": "Microsoft .NET Library for Avro provides data serialization for the Microsoft ___________ environment.",
        "Options": [
            "a) .NET",
            "b) Hadoop",
            "c) Ubuntu",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The Microsoft .NET Library for Avro implements the Apache Avro compact binary data interchange format for serialization for the Microsoft .NET environment."
    },
    {
        "id": 922,
        "Question": "Which of the following benefit is not a feature of HDInsight?",
        "Options": [
            "a) High availability",
            "b) High reliability",
            "c) High cost",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: HDInsight clusters are much easier to create than manually configuring Hadoop clusters."
    },
    {
        "id": 923,
        "Question": "Amazon EMR also allows you to run multiple versions concurrently, allowing you to control your ___________ version upgrade.",
        "Options": [
            "a) Pig",
            "b) Windows Server",
            "c) Hive",
            "d) Ubuntu"
        ],
        "Answer": "Answer: c\nExplanation: Amazon EMR supports several versions of Hive, which you can install on any running cluster."
    },
    {
        "id": 924,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Amazon Elastic MapReduce (Amazon EMR) provides support for Apache Hive",
            "b) Pig extends the SQL paradigm by including serialization formats and the ability to invoke mapper and reducer scripts",
            "c) The Amazon Hive default input format is text",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: With Hive 0.13.1 on Amazon EMR, certain options introduced in previous versions of Hive on EMR have been removed in favor of greater parity with Apache Hive. For example, the -x option was removed."
    },
    {
        "id": 925,
        "Question": "The Amazon EMR default input format for Hive is __________",
        "Options": [
            "a) org.apache.hadoop.hive.ql.io.CombineHiveInputFormat",
            "b) org.apache.hadoop.hive.ql.iont.CombineHiveInputFormat",
            "c) org.apache.hadoop.hive.ql.io.CombineFormat",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: You can specify the hive.base.inputformat option in Hive to select a different file format,"
    },
    {
        "id": 926,
        "Question": "Hadoop clusters running on Amazon EMR use ______ instances as virtual Linux servers for the master and slave nodes.",
        "Options": [
            "a) EC2",
            "b) EC3",
            "c) EC4",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Amazon EMR has made enhancements to Hadoop and other open-source applications to work seamlessly with AWS."
    },
    {
        "id": 927,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Apache Hive saves Hive log files to /tmp/{user.name}/ in a file named hive.log",
            "b) Amazon EMR saves Hive logs to /mnt/var/log/apps/",
            "c) In order to support concurrent versions of Hive, the version of Hive you run determines the log file name",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: If you have many GZip files in your Hive cluster, you can optimize performance by passing multiple files to each mapper."
    },
    {
        "id": 928,
        "Question": "Amazon EMR uses Hadoop processing combined with several __________  products.",
        "Options": [
            "a) AWS",
            "b) ASQ",
            "c) AMR",
            "d) AWES"
        ],
        "Answer": "Answer: a\nExplanation: Amazon Elastic MapReduce (Amazon EMR) is a web service that makes it easy to process large amounts of data efficiently."
    },
    {
        "id": 929,
        "Question": "___________ is an RPC framework that defines a compact binary serialization format used to persist data structures for later analysis.",
        "Options": [
            "a) Pig",
            "b) Hive",
            "c) Thrift",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Amazon EMR does not support Hive Authorization."
    },
    {
        "id": 930,
        "Question": "Impala on Amazon EMR requires _________ running Hadoop 2.x or greater.",
        "Options": [
            "a) AMS",
            "b) AMI",
            "c) AWR",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Impala is an open source tool in the Hadoop ecosystem for interactive, ad hoc querying using SQL syntax."
    },
    {
        "id": 931,
        "Question": "Impala executes SQL queries using a _________ engine.",
        "Options": [
            "a) MAP",
            "b) MPP",
            "c) MPA",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Impala avoids Hive’s overhead from creating MapReduce jobs, giving it faster query times than Hive."
    },
    {
        "id": 932,
        "Question": "Amazon EMR clusters can read and process Amazon _________ streams directly.",
        "Options": [
            "a) Kinet",
            "b) kinematics",
            "c) Kinesis",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: The Amazon EMR connector for Amazon Kinesis uses the DynamoDB database as its backing for checkpointing metadata."
    },
    {
        "id": 933,
        "Question": "The Amazon ____________ is a Web-based service that allows business subscribers to run application programs in the Amazon.com computing environment.",
        "Options": [
            "a) EC3",
            "b) EC4",
            "c) EMR",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Use Amazon EC2 for scalable computing capacity in the AWS cloud so you can develop and deploy applications without hardware constraints."
    },
    {
        "id": 934,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Amazon Web Services offers reliable, scalable, and inexpensive cloud computing services",
            "b) MongoDB runs well on Amazon EC2",
            "c) To deploy MongoDB on EC2 you can either set up a new instance manually",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: MongoDB on EC2 can be deployed easily by using sharded cluster management."
    },
    {
        "id": 935,
        "Question": "Amazon ___________ is a Web service that provides real-time monitoring to Amazon’s EC2 customers.",
        "Options": [
            "a) AmWatch",
            "b) CloudWatch",
            "c) IamWatch",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: The current AMIs for all CoreOS channels and EC2 regions are updated frequently."
    },
    {
        "id": 936,
        "Question": "Amazon ___________ provides developers the tools to build failure resilient applications and isolate themselves from common failure scenarios.",
        "Options": [
            "a) EC2",
            "b) EC3",
            "c) EC4",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Amazon EC2 changes the economics of computing by allowing you to pay only for capacity that you actually use."
    },
    {
        "id": 937,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud",
            "b) Amazon EC2 is designed to make web-scale cloud computing easier for developers.",
            "c) Amazon EC2’s simple web service interface allows you to obtain and configure capacity with minimal friction.",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: Amazon EC2 reduces the time required to obtain and boot new server instances to minutes."
    },
    {
        "id": 938,
        "Question": "Amazon EC2 provides virtual computing environments, known as __________",
        "Options": [
            "a) chunks",
            "b) instances",
            "c) messages",
            "d) none of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Using Amazon EC2 eliminates your need to invest in hardware up front"
    },
    {
        "id": 939,
        "Question": "Amazon ___________ is well suited to transfer bulk amount of data.",
        "Options": [
            "a) EC2",
            "b) EC3",
            "c) EC4",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic."
    },
    {
        "id": 940,
        "Question": "The EC2 can serve as a practically unlimited set of ___________ machines.",
        "Options": [
            "a) virtual",
            "b) real",
            "c) distributed",
            "d) all of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: To use the EC2, a subscriber creates an Amazon Machine Image (AMI) containing the operating system, application programs and configuration settings."
    },
    {
        "id": 941,
        "Question": "EC2 capacity can be increased or decreased in real time from as few as one to more than ___________ virtual machines simultaneously.",
        "Options": [
            "a) 1000",
            "b) 2000",
            "c) 3000",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: Billing takes place according to the computing and network resources consumed."
    },
    {
        "id": 942,
        "Question": "AMI is uploaded to the Amazon _______ and registered with Amazon EC2, creating a so-called AMI identifier (AMI ID).",
        "Options": [
            "a) S2",
            "b) S3",
            "c) S4",
            "d) S5"
        ],
        "Answer": "Answer: a\nExplanation: Amazon S3 stands for Amazon Simple Storage Service."
    },
    {
        "id": 943,
        "Question": "The IBM _____________ Platform provides all the foundational building blocks of trusted information, including data integration, data warehousing, master data management, big data and information governance.",
        "Options": [
            "a) InfoStream",
            "b) InfoSphere",
            "c) InfoSurface",
            "d) InfoData"
        ],
        "Answer": "Answer: a\nExplanation: InfoStream platform provides an enterprise-class foundation for information-intensive projects, providing the performance, scalability, reliability and acceleration needed to simplify difficult challenges and deliver trusted information to your business faster."
    },
    {
        "id": 944,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) IBM InfoSphere DataStage is an ETL tool",
            "b) IBM InfoSphere DataStage is a part of the IBM Information Platforms Solutions suite and IBM InfoSphere",
            "c) InfoSphere uses a graphical notation to construct data integration solutions",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: d\nExplanation: InfoSphere DataStage is a powerful data integration tool."
    },
    {
        "id": 945,
        "Question": "InfoSphere DataStage has __________ levels of Parallelism.",
        "Options": [
            "a) 1",
            "b) 2",
            "c) 3",
            "d) 4"
        ],
        "Answer": "Answer: c\nExplanation: InfoSphere DataStage also facilitates extended metadata management and enterprise connectivity."
    },
    {
        "id": 946,
        "Question": "InfoSphere DataStage uses a client/server design where jobs are created and administered via a ________ client against a central repository on a server.",
        "Options": [
            "a) Ubuntu",
            "b) Windows",
            "c) Debian",
            "d) Solaris"
        ],
        "Answer": "Answer: b\nExplanation: The IBM InfoSphere DataStage is capable of integrating data on demand across multiple and high volumes of data sources and target applications using a high-performance parallel framework."
    },
    {
        "id": 947,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) InfoSphere DataStage also facilitates extended metadata management and enterprise connectivity",
            "b) Real-Time Integration pack can turn server or parallel jobs into SOA services",
            "c) In 2012 the suite was renamed to InfoSphere Information Server and the product was renamed to InfoSphere DataStage",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: c\nExplanation: In 2006 the product was released as part of the IBM Information Server under the Information Management family but was still known as WebSphere DataStage."
    },
    {
        "id": 948,
        "Question": "__________ is a name given to the version of DataStage that had a parallel processing architecture and parallel ETL jobs.",
        "Options": [
            "a) Enterprise Edition",
            "b) Server Edition",
            "c) MVS Edition",
            "d) TX"
        ],
        "Answer": "Answer: a\nExplanation: DataStage 5 added Sequence Jobs and DataStage 6 added Parallel Jobs via Enterprise Edition."
    },
    {
        "id": 949,
        "Question": "___________ is used for processing complex transactions and messages.",
        "Options": [
            "a) PX",
            "b) Server Edition",
            "c) MVS Edition",
            "d) TX"
        ],
        "Answer": "Answer: d\nExplanation: MVS Edition developed on a Windows or Unix/Linux platform and transferred to the mainframe as compiled mainframe jobs."
    },
    {
        "id": 950,
        "Question": "InfoSphere ___________ provides you with the ability to flexibly meet your unique information integration requirements.",
        "Options": [
            "a) Data Server",
            "b) Information Server",
            "c) Info Server",
            "d) All of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: IBM InfoSphere Information Server is a market-leading data integration platform which includes a family of products that enable you to understand, cleanse, monitor, transform, and deliver data."
    },
    {
        "id": 951,
        "Question": "DataStage originated at __________ a company that developed two notable products: UniVerse database and the DataStage ETL tool.",
        "Options": [
            "a) VMark",
            "b) Vzen",
            "c) Hatez",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: a\nExplanation: The first VMark ETL prototype was built by Lee Scheffler in the first half of 1996."
    },
    {
        "id": 952,
        "Question": "DataStage RTI is real time integration pack for __________",
        "Options": [
            "a) STD",
            "b) ISD",
            "c) EXD",
            "d) None of the mentioned"
        ],
        "Answer": "Answer: b\nExplanation: ISD stands for Information Services Director."
    }
]